/ See VANDERPLAS
/ See GERON
/ See MY ML
/ See MY PYTHON
/ See LOGISTICREGRESSION=OWN ALGORITHM

/ VANDERPLAS

/ (79)

In [1]: import numpy as np

In [56]: np.random.RandomState?
...
In [50]: rand=np.random.RandomState(42)

n [55]: rand.randint(10,size=10)
Out[55]: array([0, 9, 2, 6, 3, 8, 2, 4, 2, 6])
/ dus van 0 t/m 9	,

/ we kunnen net zo goed np.random use ipv np.random.RandomState(42)	,

/i, 

/ er zijn 2 vragen	, 

/ wat is np.random.RandomState() ivm np.random	?
/ wat is verschil np.random.rand en np.random.random	?

/ np.random.RandomState kan meer, 	maar wij kunen np.random use	,
/ np.random.random is arg een tuple	,

/ lees,
https://stackoverflow.com/questions/47231852/np-random-rand-vs-np-random-random

First note that numpy.random.random is actually an alias for numpy.random.random_sample. I'll use the latter in the following. (See this question and answer for more aliases.)

Both functions generate samples from the uniform distribution on [0, 1). The only difference is in how the arguments are handled. With numpy.random.rand, the length of each dimension of the output array is a separate argument. With numpy.random.random_sample, the shape argument is a single tuple.

For example, to create an array of samples with shape (3, 5), you can write
sample = np.random.rand(3, 5)
or
sample = np.random.random_sample((3, 5))

The Python stdlib module "random" also contains a Mersenne Twister
pseudo-random number generator with a number of methods that are similar
to the ones available in `RandomState`. `RandomState`, besides being
NumPy-aware, has the advantage that it provides a much larger number
of probability distributions to choose from.

/ Dus je kunt use	, 
rand=np.random.RandomState(42)
/ of,
rand2=np.random

In [70]: rand.rand(5)
Out[70]: array([0.31171108, 0.52006802, 0.54671028, 0.18485446, 0.96958463])
In [71]: rand2.rand(5)
Out[71]: array([0.37216378, 0.57503978, 0.89170664, 0.78540636, 0.41280859])

/ maar,
In [72]: rand.random(10)
AttributeError: 'mtrand.RandomState' object has no attribute 'random'

In [73]: rand2.random(10)
Out[73]: 
array([0.43016259, 0.50016717, 0.19611866, 0.15956935, 0.55851395,
       0.9850149 , 0.38095386, 0.77806371, 0.55303435, 0.74382784])

/ je zou altijd np.random (=rand2) kunnen gebruiken	, 

/ Wat is het verschil,
In [75]: rand2.random(10)
Out[75]: 
array([0.84989639, 0.30169224, 0.45256591, 0.60041092, 0.00564067,
       0.12172757, 0.25734236, 0.76184963, 0.80985867, 0.99555487])
In [76]: rand2.rand(10)
Out[76]: 
array([0.95845232, 0.20018847, 0.18249688, 0.98379025, 0.50036593,
       0.64649962, 0.89065535, 0.77494879, 0.22977625, 0.05152478])

/ hierboven staat dat eig	,
In [77]: rand2.random((10))
Out[77]: 
array([0.25805734, 0.08009396, 0.16077197, 0.73844499, 0.93320537,
       0.39854573, 0.4232493 , 0.09888422, 0.3778649 , 0.89745537])
/ TODO

/ Einde i	,

/ 79	.

/ we use np.random ipv. np.random.RandState(42)	,

In [82]: x=np.random.randint(100,size=10)

In [83]: x
Out[83]: array([ 7,  9, 22, 41, 34, 42, 40, 64, 14, 27])

/ 81	,

In [96]: %matplotlib
Using matplotlib backend: Qt5Agg
In [97]: import matplotlib.pyplot as plt
In [98]: import seaborn
In [99]: seaborn.set()

/i, 
In [104]: type(X)
Out[104]: numpy.ndarray
In [105]: X.shape
Out[105]: (100, 2)

In [102]: type(X[:,0])
Out[102]: numpy.ndarray
In [103]: X[:,0].shape
Out[103]: (100,)

/ Einde i	,

In [106]: plt.scatter(X[:,0],X[:,1])

/ je kunt nog een paar keer	, je ziet ze met andere kleuren	,

In [107]: X=np.random.multivariate_normal(m,conv,100)
In [108]: plt.scatter(X[:,0],X[:,1])

In [109]: X=np.random.multivariate_normal(m,conv,100)
Out[110]: <matplotlib.collections.PathCollection at 0x7f9a23ac6198>

/ waarom geldt ongeveer dat X[:,1]=2*X[:0] ?
/ TODO 

/i, 

/ de conv matrix moet pos (semi) def	,
/ is als = transpose(A)*A

In [126]: A=np.array([1,2,0,1]).reshape(2,2)

In [127]: A
Out[127]: 
array([[1, 2],
       [0, 1]])

In [128]: np.dot(A.transpose(),A)
Out[128]: 
array([[1, 2],
       [2, 5]])

In [129]: A=np.array([1,4,0,1]).reshape(2,2)

In [130]: np.dot(A.transpose(),A)
Out[130]: 
array([[ 1,  4],
       [ 4, 17]])

/ Einde i	,

In [132]: A=np.array([1,2,0,1]).reshape(2,2)
In [134]: conv=np.dot(A.transpose(),A)
In [138]: m=[0,0]							# is geen np.array	, maar een list	,
In [136]: X=np.random.multivariate_normal(m,conv,100)

/ Einde VANDERPLAS

/ GERON

/ 7	. 

/ hier zijn alle playbooks	,
[eric@almond handson-ml]$ pwd
/home/eric/Devel/python/geron/handson-ml

/ geef daarom hier,
[eric@almond handson-ml]$ jupyter notebook

/ ga in de browser,
http://localhost:8888/tree
/ kies notebook, we zien in tab ernaast	,
http://localhost:8888/notebooks/04_training_linear_models.ipynb


/ we zien de tekst over log regr,

/ Einde GERON

/ MY ML

/ 7	 .

/ lees	,
[eric@almond my]$ less log_reg_1_class_1_feature_newton.py

[eric@almond my]$ pwd
/home/eric/Devel/python/my

[eric@almond my]$ ipython

In [1]: import numpy as np
In [2]: from sklearn import datasets
In [7]: iris=datasets.load_iris()
In [6]: iris.keys()
Out[6]: dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])

In [34]: iris.data.shape
Out[34]: (150, 4)
In [35]: iris.target.shape
Out[35]: (150,)

In [46]: iris.feature_names
Out[46]: 
['sepal length (cm)',
 'sepal width (cm)',
 'petal length (cm)',
 'petal width (cm)']
In [47]: iris.target_names
Out[47]: array(['setosa', 'versicolor', 'virginica'], dtype='<U10')


/ de 1ste 2 rijen,
In [41]:  iris.data[:2]
Out[41]: 
array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2]])

In [42]: iris.target[:2]
Out[42]: array([0, 0])

/ dus de 1ste 2 records zijn van type 0=setosa	,

/ alleen de laaste kolom	, dus  petal_width	,
In [51]: iris.data[:,[3]]
Out[51]: 
array([[0.2],
       [0.2],
       [0.2],
...
/ toon de eerste 2, 
In [57]: iris.data[:,[3]][:2]
Out[57]: 
array([[0.2],
       [0.2]])
/ toon de laatste 2,
In [56]: iris.data[:,[3]][-2:]
Out[56]: 
array([[2.3],
       [1.8]])





/ 13	. 

/ lees,
[eric@almond my]$ pwd
/home/eric/Devel/python/my

[eric@almond my]$ cp log_reg_1_class_1_feature_newton.py log_reg_1_class_1_feature_newton_iris_202102.py 

$ vi log_reg_1_class_1_feature_newton_iris_202102.py

In [1]: import numpy as np
In [2]: from sklearn import datasets


In [368]: iris=datasets.load_iris()   

n [367]: iris.data
Out[367]: 
array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
...
n [368]: iris.target
Out[368]: 
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])


/ phi is 2de kolom Phi	,
In [228]: phi=iris.data[:,3]                      
In [228]: phi.shape
Out[228]: (150,)
In [229]: Phi=np.c_[np.ones(len(phi),dtype='int'),phi]
In [230]: Phi.shape
Out[230]: (150, 2)
In [235]: w=np.zeros(2)
In [237]: w.shape
Out[237]: (2,)
In [239]: t=(iris['target']==2).astype(int)
In [241]: t.shape
Out[241]: (150,)
In [242]: from scipy.special import expit
In [243]: p1=expit(Phi.dot(w))
In [244]: p1.shape
Out[244]: (150,)
In [255]: grad=Phi.T @ (p1-t)
In [256]: grad.shape
Out[256]: (2,)
In [247]: R=np.diag(p1*(1-p1))
In [248]: R.shape
Out[248]: (150, 150)
In [257]: H=Phi.T @ R @ Phi
In [259]: H.shape
Out[259]: (2, 2)
In [260]: w=w-np.linalg.inv(H) @ grad
/ of	,
In [260]: w=w-la.lu_solve(la.lu_factor(H),grad)

/ 13	. 

/ FINAL 	,

/ class 2 ogv feature 3	,

[eric@almond my]$ cat  log_reg_1_class_1_feature_newton_iris_202102.py

import numpy as np
from sklearn import datasets
import scipy as sp
import scipy.linalg as la

iris=datasets.load_iris()

phi=iris['data'][:,3]    # (150,)
Phi=np.c_[np.ones(len(phi),dtype='int'),phi] # (150,2)
w=np.zeros(2) # (2,)
t=(iris['target']==2).astype(int) # (150,)
p1=sp.special.expit(Phi@w)     # p1|phi # (150,)
grad=Phi.T @ (p1-t) # (2,)

for i in range(0,10):
    p1=sp.special.expit(Phi@w)     # p1|phi  (150,)
    grad=Phi.T @ (p1-t) # (2,)
    R=np.diag(p1*(1-p1))    # (150,150)
    H=Phi.T @ R @ Phi
    w=w-la.lu_solve(la.lu_factor(H),grad)
    display(w)

import matplotlib.pyplot as plt
import seaborn
seaborn.set()

plt.figure()
plt.plot(phi[t==0],t[t==0],'gs')
plt.plot(phi[t==1],t[t==1],'b^')

grid=np.linspace(min(phi),max(phi),101)
Phi_grid=np.c_[np.ones(101,dtype='int'),grid]
t_grid=sp.special.expit(Phi_grid @ w)
plt.plot(grid,t_grid)

plt.show()

[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_iris_202102.py 
array([-2.95221619,  1.9067432 ])
array([-5.41595373,  3.37328912])
array([-9.04645818,  5.53966354])
array([-13.70212828,   8.36784257])
array([-18.04411003,  11.03842708])
array([-20.54124041,  12.58379068])
array([-21.10321628,  12.9334759 ])
array([-21.12560604,  12.94748582])
array([-21.12563996,  12.94750716])
array([-21.12563996,  12.94750716])

/ 13	. 

/ LOGISTICREGRESSION=OWN ALGORITHM

/ class 2	, feature 3
[eric@almond my]$ ipython  log_reg_1_class_1_feature_iris_geron_202102.py 2 3
array([-21.12560598])
array([[12.94748578]])

[eric@almond my]$ cat log_reg_1_class_1_feature_iris_geron_202102.py

import sys
import numpy as np
from sklearn import datasets
from sklearn.linear_model import LogisticRegression

iris=datasets.load_iris()

phi=iris.data[:,[int(sys.argv[2])]]							# (150,1)
t = (iris.target==int(sys.argv[1])).astype(int)	 # (150,)
lg=LogisticRegression(solver="liblinear",C=10**10) 
lg.fit(phi,t)

display(lg.intercept_)
display(lg.coef_)

/ of interactive	,

from sklearn import datasets 
In [523]: from sklearn.linear_model import LogisticRegression                                

iris=datasets.load_iris()
In [523]: X=iris.data[:,[3]]   						# (150,1)                                                               
In [523]: y=(iris.target==2).astype(int)  # (150,)                                          


In [520]: lg=LogisticRegression(solver='liblinear',C=10**10)
In [521]: lg.fit(X,y)
Out[521]: 
LogisticRegression(C=10000000000, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=100,
          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)

In [522]: lg.intercept_,lg.coef_
Out[522]: (array([-21.12560598]), array([[12.94748578]]))

[eric@almond my]$ pwd
/home/eric/Devel/python/my
[eric@almond my]$ ls -ltr
...
-rw-rw-r--. 1 eric eric  952 Feb 20 20:19 log_reg_1_class_1_feature_newton_iris_202102.py
-rw-rw-r--. 1 eric eric  730 Feb 21 19:30 log_reg_1_class_1_feature_newton_eigen_voorbeelden_202102.py
-rw-rw-r--. 1 eric eric 1491 Feb 23 21:20 log_reg_1_class_1_feature_iris_geron_202102.py


/ Einde FINAL

/ FINAL

/ class 2 ogv feature 3	,
[eric@almond my]$ ipython log_reg_1_class_1_feature_iris_geron_202102.py 2 3
array([-21.12560598])
array([[12.94748578]])
/ class 2 ogv feature 2	,
[eric@almond my]$ ipython log_reg_1_class_1_feature_iris_geron_202102.py 2 2
array([-43.78073255])
array([[9.00196394]])
/ class 2 ogv feature 1	,
[eric@almond my]$ ipython log_reg_1_class_1_feature_iris_geron_202102.py 2 1
array([1.32333756])
array([[-0.66482798]])
[eric@almond my]$ ipython log_reg_1_class_1_feature_iris_geron_202102.py 2 0
array([-16.31956316])
array([[2.59202035]])
[eric@almond my]$ ipython log_reg_1_class_1_feature_iris_geron_202102.py 1 3
array([-1.10535792])
array([[0.33481593]])
[eric@almond my]$ ipython log_reg_1_class_1_feature_iris_geron_202102.py 1 2
array([-1.68856759])
array([[0.25557469]])
[eric@almond my]$ ipython log_reg_1_class_1_feature_iris_geron_202102.py 1 1
array([8.88625873])
array([[-3.22331457]])
[eric@almond my]$ ipython log_reg_1_class_1_feature_iris_geron_202102.py 1 0
array([-1.8863686])
array([[0.2034156]])
[eric@almond my]$ ipython log_reg_1_class_1_feature_iris_geron_202102.py 0 3
array([25.96540174])
array([[-33.64024542]])

[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_iris_202102.py 1 3
array([-1.01843134,  0.2934633 ])
array([-1.10399591,  0.33409926])
array([-1.10535792,  0.33481593])
array([-1.10535826,  0.33481611])
array([-1.10535826,  0.33481611])
array([-1.10535826,  0.33481611])
array([-1.10535826,  0.33481611])
array([-1.10535826,  0.33481611])
array([-1.10535826,  0.33481611])
array([-1.10535826,  0.33481611])
[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_iris_202102.py 1 3
array([-1.01843134,  0.2934633 ])
array([-1.10399591,  0.33409926])
array([-1.10535792,  0.33481593])
array([-1.10535826,  0.33481611])
array([-1.10535826,  0.33481611])
array([-1.10535826,  0.33481611])
array([-1.10535826,  0.33481611])
array([-1.10535826,  0.33481611])
array([-1.10535826,  0.33481611])
array([-1.10535826,  0.33481611])

[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_iris_202102.py 1 2
array([-1.47912289,  0.21615543])
array([-1.68094347,  0.25410858])
array([-1.68856759,  0.25557469])
array([-1.68857837,  0.25557676])
array([-1.68857837,  0.25557676])
array([-1.68857837,  0.25557676])
array([-1.68857837,  0.25557676])
array([-1.68857837,  0.25557676])
array([-1.68857837,  0.25557676])
array([-1.68857837,  0.25557676])

[eric@almond my]$ cat log_reg_1_class_1_feature_newton_iris_202102.py
import sys
import numpy as np
from sklearn import datasets
# import sklearn as sk
import scipy as sp
import scipy.linalg as la

# iris=sk.datasets.load_iris() # does not work TODO
iris=datasets.load_iris()

phi=iris['data'][:,int(sys.argv[2])]    # (150,)
Phi=np.c_[np.ones(len(phi),dtype='int'),phi] # (150,2)
w=np.zeros(2) # (2,)
t=(iris['target']==int(sys.argv[1])).astype(int) # (150,)
p1=sp.special.expit(Phi@w)     # p1|phi # (150,)
grad=Phi.T @ (p1-t) # (2,)

for i in range(0,10):
    p1=sp.special.expit(Phi@w)     # p1|phi  (150,)
    grad=Phi.T @ (p1-t) # (2,)
    R=np.diag(p1*(1-p1))    # (150,150)
    H=Phi.T @ R @ Phi
    w=w-la.lu_solve(la.lu_factor(H),grad)
    display(w)

import matplotlib.pyplot as plt
import seaborn
seaborn.set()

plt.figure()
plt.plot(phi[t==0],t[t==0],'gs')
plt.plot(phi[t==1],t[t==1],'b^')

grid=np.linspace(min(phi),max(phi),101)
Phi_grid=np.c_[np.ones(101,dtype='int'),grid]
t_grid=sp.special.expit(Phi_grid @ w)
plt.plot(grid,t_grid)

plt.show()

/ Einde FINAL

/ FINAL

/ 1 class ogv 2 features	,

[eric@almond my]$ cp log_reg_1_class_1_feature_iris_geron_202102.py log_reg_1_class_2_features_iris_geron_202102.py

/ class 2 ogv feature 2,3
[eric@almond my]$ ipython log_reg_1_class_2_features_iris_geron_202102.py 2 2 3 
array([-45.26062435])
array([[ 5.7528683 , 10.44455633]])

[eric@almond my]$ cat log_reg_1_class_2_features_iris_geron_202102.py
import sys
import numpy as np
from sklearn import datasets
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
import seaborn

iris=datasets.load_iris()

phi=iris.data[:,[int(sys.argv[2]),int(sys.argv[3])]]			# <<<
t = (iris.target==int(sys.argv[1])).astype(int)
lg=LogisticRegression(solver="liblinear",C=10**10) 

lg.fit(phi,t)

display(lg.intercept_)
display(lg.coef_)

/ 13	. 

[eric@almond my]$ cp log_reg_1_class_1_feature_newton_iris_202102.py log_reg_1_class_2_features_newton_iris_202102.py

[eric@almond my]$ cat log_reg_1_class_2_features_newton_iris_202102.py
import sys
import numpy as np
from sklearn import datasets
# import sklearn as sk
import scipy as sp
import scipy.linalg as la

# iris=sk.datasets.load_iris() # does not work TODO
iris=datasets.load_iris()

phi=iris['data'][:,[int(sys.argv[2]),int(sys.argv[3])]]    # (150,2)
Phi=np.c_[np.ones(len(phi),dtype='int'),phi] # (150,3)
w=np.zeros(3) # (3,)
t=(iris['target']==int(sys.argv[1])).astype(int) # (150,)
p1=sp.special.expit(Phi@w)     # p1|phi # (150,)
grad=Phi.T @ (p1-t) # (3,)

for i in range(0,10):
    p1=sp.special.expit(Phi@w)     # p1|phi  (150,)
    grad=Phi.T @ (p1-t) # (3,)
    R=np.diag(p1*(1-p1))    # (150,150)
    H=Phi.T @ R @ Phi
    w=w-la.lu_solve(la.lu_factor(H),grad)
    display(w)

[eric@almond my]$ ipython log_reg_1_class_2_features_newton_iris_202102.py 2 2 3
array([-2.64206527, -0.28439284,  2.53976906])
array([-5.21160351, -0.13845717,  3.66413443])
array([-9.73272434,  0.46875474,  4.56796196])
array([-16.77710881,   1.5201714 ,   5.68636136])
array([-25.41318186,   2.83525866,   7.02607745])
array([-34.71567155,   4.22896712,   8.55612783])
array([-42.00359367,   5.28752914,   9.84965916])
array([-44.93355387,   5.7061491 ,  10.38530258])
array([-45.2685563 ,   5.75398881,  10.44602654])
array([-45.27234329,   5.75453225,  10.44669981])

/ Klopt	,

/ Einde FINAL

/ FINAL

/ 4 features, 1 class	,

/ het arg is alleen de class	,

[eric@almond my]$ ipython log_reg_1_class_4_features_newton_iris_202102.py 2
array([-4.74178586, -0.17639365,  0.79293022,  0.01696003,  2.18617083])
array([-7.65381609, -0.34685703,  1.14673432,  0.27343687,  3.22908725])
array([-9.24456123, -0.62559279,  0.71716471,  0.91031363,  4.08679559])
array([-10.30107232,  -1.11873883,  -1.00840542,   2.32640258,
         5.34612322])
array([-14.08319984,  -1.59714009,  -2.20181703,   3.81731771,
         7.04317257])
array([-19.55140272,  -2.03655545,  -3.19916405,   5.31654289,
         9.20982049])
array([-26.42071823,  -2.34555249,  -4.26926678,   6.77553364,
        11.94216001])
array([-34.15841224,  -2.46092702,  -5.46642484,   8.1121248 ,
        15.04523182])
array([-40.34708283,  -2.46700914,  -6.37748773,   9.07597058,
        17.45178156])
array([-42.47594337,  -2.4649336 ,  -6.66152129,   9.40361465,
        18.23124169])

[eric@almond my]$ ipython log_reg_1_class_4_features_newton_iris_202102.py 1
array([ 4.25190769, -0.08617855, -1.76282223,  0.87408548, -1.93276518])
array([ 6.56881264, -0.21118307, -2.53163175,  1.20429215, -2.54309988])
array([ 7.27312282, -0.25009901, -2.76278409,  1.29334683, -2.69463101])
array([ 7.32269986, -0.2527318 , -2.77931212,  1.29927968, -2.70423046])
array([ 7.32292704, -0.25274345, -2.77938917,  1.29930595, -2.70427087])
array([ 7.32292705, -0.25274345, -2.77938918,  1.29930595, -2.70427087])
array([ 7.32292705, -0.25274345, -2.77938918,  1.29930595, -2.70427087])
array([ 7.32292705, -0.25274345, -2.77938918,  1.29930595, -2.70427087])
array([ 7.32292705, -0.25274345, -2.77938918,  1.29930595, -2.70427087])
array([ 7.32292705, -0.25274345, -2.77938918,  1.29930595, -2.70427087])

[eric@almond my]$ ipython log_reg_1_class_4_features_newton_iris_202102.py 0
array([-1.51012184,  0.2625722 ,  0.96989201, -0.8910455 , -0.25340565])
array([-2.16891298,  0.34811397,  1.62694371, -1.51216664, -0.24905748])
array([-2.69843449,  0.44395163,  2.23097412, -2.14169962, -0.26894348])
array([-3.24226357,  0.58122257,  2.79050805, -2.75144633, -0.4728069 ])
array([-3.82642141,  0.78094121,  3.28328233, -3.31894617, -0.96641189])
array([-4.44371247,  1.06203581,  3.68608972, -3.85876241, -1.74914524])
array([-5.12502249,  1.43385265,  4.00539326, -4.41178719, -2.72752261])
array([-5.89823525,  1.89095295,  4.2647414 , -5.01175344, -3.80226753])
array([-6.75035569,  2.41571074,  4.48690014, -5.67306832, -4.90861677])
array([-7.63901841,  2.98541869,  4.68892965, -6.39706983, -6.00845404])

[eric@almond my]$ ipython log_reg_1_class_4_features_iris_geron_202102.py 2
array([-42.46853369])
array([[-2.46312182, -6.66512187,  9.40005677, 18.23704717]])
[eric@almond my]$ ipython log_reg_1_class_4_features_iris_geron_202102.py 1
array([7.32072397])
array([[-0.25239142, -2.77909537,  1.29890498, -2.70358802]])
[eric@almond my]$ ipython log_reg_1_class_4_features_iris_geron_202102.py 0
array([0.9011384])
array([[ 1.51526198,  4.92414959, -7.80941819, -3.81889566]])

/ we zien dat class 0 helemaal niet klopt,
/ TODO

/ Einde FINAL

/ FINAL

/ class 0

[eric@almond my]$ ipython log_reg_1_class_1_feature_iris_geron_202102.py 0 0
array([27.82505917])
array([[-5.17505474]])
[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_iris_202102.py 0 0
array([ 8.91128693, -1.63912497])
array([15.72512211, -2.90541812])
array([22.12900054, -4.10482265])
array([26.432491  , -4.91329749])
array([27.74028948, -5.15911792])
array([27.82816437, -5.17563106])
array([27.82852139, -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
/ okay	,
[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_iris_202102.py 0 0
array([ 8.91128693, -1.63912497])
array([15.72512211, -2.90541812])
array([22.12900054, -4.10482265])
array([26.432491  , -4.91329749])
array([27.74028948, -5.15911792])
array([27.82816437, -5.17563106])
array([27.82852139, -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
array([27.8285214 , -5.17569813])
/ okay	,


[eric@almond my]$ ipython log_reg_1_class_2_features_iris_geron_202102.py 0 2 3
array([24.05565958])
array([[-6.67454643, -9.82307197]])
[eric@almond my]$ ipython log_reg_1_class_2_features_newton_iris_202102.py 0 2 3
array([ 3.06217693, -1.00087776,  0.02763259])
array([ 5.09783354, -1.81936807,  0.37691497])
array([ 6.96313852, -2.51389932,  0.36646666])
array([ 8.57781031, -2.88906075, -0.60381399])
array([10.17504528, -3.07351266, -2.33328867])
array([12.05662731, -3.37945185, -4.02475719])
array([14.21146714, -3.83501714, -5.54245832])
array([16.58293616, -4.40070486, -6.95002171])
array([19.12953648, -5.04310032, -8.31030283])
array([21.81940571, -5.73678936, -9.66992458])
/ redelijk	,
[eric@almond my]$ ipython log_reg_1_class_2_features_newton_iris_202102.py 0 2 3
array([ 3.06217693, -1.00087776,  0.02763259])
array([ 5.09783354, -1.81936807,  0.37691497])
array([ 6.96313852, -2.51389932,  0.36646666])
array([ 8.57781031, -2.88906075, -0.60381399])
array([10.17504528, -3.07351266, -2.33328867])
array([12.05662731, -3.37945185, -4.02475719])
array([14.21146714, -3.83501714, -5.54245832])
array([16.58293616, -4.40070486, -6.95002171])
array([19.12953648, -5.04310032, -8.31030283])
array([21.81940571, -5.73678936, -9.66992458])
array([ 24.62580747,  -6.46307232, -11.05860594])
array([ 27.52555668,  -7.20922129, -12.49043747])
array([ 30.49881329,  -7.9674343 , -13.96751314])
array([ 33.52923103,  -8.73344616, -15.48477056])
array([ 36.60390895,  -9.50517343, -17.03427843])
array([ 39.71301729, -10.28168833, -18.6080121 ])
array([ 42.84922949, -11.0625848 , -20.19915355])
array([ 46.00713321, -11.84764966, -21.80242316])
array([ 49.18272458, -12.6367189 , -23.41394327])
array([ 52.37301646, -13.42962776, -25.03094989])
array([ 55.57575169, -14.22620105, -26.65150345])
array([ 58.78919876, -15.02625758, -28.27425105])
array([ 62.01200731, -15.82961767, -29.89824639])
array([ 65.24310652, -16.636109  , -31.52282232])
array([ 68.48163272, -17.44557057, -33.14750128])
array([ 71.72687612, -18.25785687, -34.77192581])
array([ 74.97824201, -19.07284098, -36.39580855])
array([ 78.23522799, -19.89040196, -38.01894344])
array([ 81.49739863, -20.71045089, -39.64110504])
array([ 84.7643771 , -21.5328895 , -41.26215633])
array([ 88.0358041 , -22.35769795, -42.88176008])
array([ 91.31141991, -23.18466682, -44.50021432])
array([ 94.59085785, -24.01393654, -46.11672915])
array([ 97.874037  , -24.84527765, -47.73183185])
array([101.16027606, -25.67927045, -49.34321451])
array([104.45012943, -26.51432195, -50.95581889])
array([107.74185541, -27.3539001 , -52.55831262])
array([111.03656981, -28.19728933, -54.15371232])
array([114.33707563, -29.03107137, -55.78181552])
array([117.68198571, -29.71975092, -57.85352592])
array([121.02688571, -30.40855589, -59.92488392])
array([124.37177779, -31.09747036, -61.99593509])
array([123.37177779, -31.09747036, -61.99593509])
array([126.71666447, -31.78648052, -64.06671949])
array([125.71666447, -31.78648052, -64.06671949])
array([129.0615439 , -32.47557367, -66.13727015])
array([128.0615439 , -32.47557367, -66.13727015])
array([131.40641669, -33.16473913, -68.20761692])
array([130.40641669, -33.16473913, -68.20761692])
array([133.75128368, -33.8539676 , -70.27778599])
/ helemaal verkeerd,	


[eric@almond my]$ ipython log_reg_1_class_2_features_iris_geron_202102.py 0 0 1
array([80.6272241])
array([[-31.61987832,  28.31499853]])
[eric@almond my]$ ipython log_reg_1_class_2_features_newton_iris_202102.py 0 0 1
array([ 1.1723718 , -1.50833671,  2.28378379])
array([ 2.87323278, -2.70910229,  3.83782039])
array([ 5.82383027, -4.12785049,  5.35446739])
array([10.5793925 , -5.93112164,  6.942185  ])
array([17.07997853, -8.25137327,  8.84788891])
array([ 25.59650632, -11.29663105,  11.33778311])
array([ 37.1111016 , -15.44304898,  14.7321623 ])
array([ 52.28933004, -20.97539974,  19.30173826])
array([ 70.17600067, -27.59497793,  24.85001005])
array([ 89.6675178 , -34.86617108,  30.98810799])
/ redelijk	,

[eric@almond my]$ ipython log_reg_1_class_2_features_newton_iris_202102.py 0 0 1
array([ 1.1723718 , -1.50833671,  2.28378379])
array([ 2.87323278, -2.70910229,  3.83782039])
array([ 5.82383027, -4.12785049,  5.35446739])
array([10.5793925 , -5.93112164,  6.942185  ])
array([17.07997853, -8.25137327,  8.84788891])
array([ 25.59650632, -11.29663105,  11.33778311])
array([ 37.1111016 , -15.44304898,  14.7321623 ])
array([ 52.28933004, -20.97539974,  19.30173826])
array([ 70.17600067, -27.59497793,  24.85001005])
array([ 89.6675178 , -34.86617108,  30.98810799])
array([110.32791991, -42.57911105,  37.49456855])
array([131.89964514, -50.6100114 ,  44.23896734])
array([154.12494796, -58.85452327,  51.12671621])
array([176.76712212, -67.22927372,  58.09441401])
array([199.64857602, -75.67778955,  65.10613621])
array([222.65723857, -84.16673642,  72.14365884])
array([245.73046692, -92.67806498,  79.19792362])
array([ 268.83573012, -101.20240843,   86.2642373 ])
array([ 291.95679795, -109.73495529,   93.3399881 ])
array([ 315.08573766, -118.27319331,  100.42358575])
array([ 338.21871189, -126.81575191,  107.51395722])
array([ 361.35387303, -135.36182562,  114.61030112])
array([ 384.49033108, -143.9108913 ,  121.71196677])
array([ 407.62765335, -152.46257039,  128.81839456])
array([ 430.76562345, -161.01656245,  135.92908688])
array([ 453.90412615, -169.57261296,  143.04359342])
array([ 477.04309282, -178.13049832,  150.16150477])
array([ 500.18247606, -186.69001782,  157.2824474 ])
array([ 523.32223627, -195.25098955,  164.40608126])
array([ 546.46233963, -203.81325118,  171.53210214])
array([ 569.60275461, -212.3766428 ,  178.66020978])
array([ 592.74345477, -220.94106295,  185.79021605])
array([ 615.88441396, -229.50638399,  192.92188167])
array([ 639.02560081, -238.07247986,  200.05497242])
array([ 662.16701914, -246.63935148,  207.1894886 ])
array([ 685.30845131, -255.20629862,  214.3241471 ])
array([ 708.45032843, -263.77465445,  221.46138396])
array([ 731.59318556, -272.34608301,  228.6042411 ])
array([ 754.73604269, -280.91751158,  235.74709824])
array([ 777.87889983, -289.48894015,  242.88995538])
array([ 801.02175697, -298.06036872,  250.03281253])
array([ 824.05017206, -307.77621779,  259.46451066])
array([ 847.06310503, -317.64688818,  269.20585144])
array([ 846.11002199, -317.66642863,  269.22538394])
array([ 869.11358443, -327.63080426,  279.1541352 ])
array([ 867.23736343, -327.26603015,  278.78966634])
array([ 890.20428267, -337.59683774,  289.45128152])
array([ 889.20428267, -337.59683774,  289.45128152])
array([ 912.25109491, -347.12871532,  298.51503668])
array([ 911.25109491, -347.12871532,  298.51503668])
/ dit gaat helemaal verkeerd	, 
/ TODO

/ gaat alleen verkeerd bij class 0	,

[eric@almond my]$ ipython log_reg_1_class_2_features_newton_iris_202102.py 1 2 3
array([-2.42011165,  1.2852706 , -2.56740165])
array([-2.83346688,  1.53022448, -3.0725512 ])
array([-2.8587132 ,  1.54639795, -3.10696093])
array([-2.85880689,  1.54646368, -3.10710627])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])
array([-2.85880689,  1.54646368, -3.10710628])

[eric@almond my]$ ipython log_reg_1_class_2_features_newton_iris_202102.py 2 2 3
array([-2.64206527, -0.28439284,  2.53976906])
array([-5.21160351, -0.13845717,  3.66413443])
array([-9.73272434,  0.46875474,  4.56796196])
array([-16.77710881,   1.5201714 ,   5.68636136])
array([-25.41318186,   2.83525866,   7.02607745])
array([-34.71567155,   4.22896712,   8.55612783])
array([-42.00359367,   5.28752914,   9.84965916])
array([-44.93355387,   5.7061491 ,  10.38530258])
array([-45.2685563 ,   5.75398881,  10.44602654])
array([-45.27234329,   5.75453225,  10.44669981])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])
array([-45.27234377,   5.75453232,  10.44669989])

/ Einde FINAL

/ MULTINOMIAL

$ ls -ltr
-rw-rw-r--. 1 eric eric 1431 Feb 24 22:14 log_reg_3_class_4_features_iris_geron_202102.py
/ heeft geen args	,
-rw-rw-r--. 1 eric eric 1440 Feb 24 22:24 log_reg_3_class_1_features_iris_geron_202102.py
/ heeft de feature als arg	,

/ 13	 .

[eric@almond my]$ ipython log_reg_3_class_1_features_iris_geron_202102.py 3
array([ 32.25741399,  -5.56591178, -26.69150222])
array([[-36.81302781],
       [ 11.93277549],
       [ 24.88025232]])
lg=LogisticRegression(solver="liblinear",C=10**10) 


[eric@almond my]$ ipython log_reg_1_class_1_feature_iris_geron_202102.py 0 3
array([25.96540174])
array([[-33.64024542]])
[eric@almond my]$ ipython log_reg_1_class_1_feature_iris_geron_202102.py 1 3
array([-1.10535792])
array([[0.33481593]])
[eric@almond my]$ ipython log_reg_1_class_1_feature_iris_geron_202102.py 2 3
array([-21.12560598])
array([[12.94748578]])
lg=LogisticRegression(multi_class="multinomial",solver="newton-cg",C=10**10) 

/ 13	. 

[eric@almond my]$ ipython log_reg_3_class_4_features_iris_geron_202102.py
array([  2.32925138,  20.15434543, -22.48359681])
array([[  4.8858444 ,   7.84777582, -12.84179439,  -6.6637919 ],
       [ -1.2103132 ,  -0.58344044,   1.70619436,  -5.81118631],
       [ -3.6755312 ,  -7.26433538,  11.13560003,  12.47497821]])
[eric@almond my]$ 
[eric@almond my]$  ipython log_reg_1_class_4_features_iris_geron_202102.py 0
array([0.9011384])
array([[ 1.51526198,  4.92414959, -7.80941819, -3.81889566]])
[eric@almond my]$  ipython log_reg_1_class_4_features_iris_geron_202102.py 1
array([7.32072397])
array([[-0.25239142, -2.77909537,  1.29890498, -2.70358802]])
[eric@almond my]$  ipython log_reg_1_class_4_features_iris_geron_202102.py 2
array([-42.46853369])
array([[-2.46312182, -6.66512187,  9.40005677, 18.23704717]])




/ Einde MULTINOMIAL

/ MULTINOMIAL





/ Einde MULTINOMIAL




/ 13	. 

In [534]: y=(iris.target==1).astype(int)

In [535]: lg.fit(X,y)
Out[535]: 
LogisticRegression(C=10000000000, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=100,
          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)

In [536]: lg.intercept_,lg.coef_
Out[536]: (array([-1.10535792]), array([[0.33481593]]))

/ 13	. 

In [537]: y=(iris.target==0).astype(int)

In [538]: lg=LogisticRegression(solver='liblinear',C=10**10)

In [539]: lg.fit(X,y)
Out[539]: 
LogisticRegression(C=10000000000, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=100,
          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)

In [540]: lg.intercept_,lg.coef_
Out[540]: (array([25.96540174]), array([[-33.64024542]]))

/ 13	. 

In [541]: lg=LogisticRegression(multi_class='multinomial',solver='newton-cg',C=10**10,random_state=0)
In [542]: lg.fit(X,iris.target)
In [543]: lg.intercept_,lg.coef_

/ we zien andere getallen	,

/ of	,
In [542]: lg.fit(iris.data,iris.target)



/ 7	. 

[eric@almond my]$ cp log_reg_1_class_1_feature_newton_iris_202102.py log_reg_1_class_1_feature_newton_eigen_voorbeelden_202102.py

/ H wordt singulier	,


/ 7	. 

/ vlg sklearn	,

[eric@almond my]$ cp log_reg_1_class_1_feature_newton_iris_202102.py  log_reg_1_class_1_feature_iris_geron_202102.py

/ 13	. 

/ lees,
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression

>>> from sklearn.datasets import load_iris
>>> from sklearn.linear_model import LogisticRegression
>>> X, y = load_iris(return_X_y=True)
>>> clf = LogisticRegression(random_state=0).fit(X, y)
>>> clf.predict(X[:2, :])
array([0, 0])
>>> clf.predict_proba(X[:2, :])
array([[9.8...e-01, 1.8...e-02, 1.4...e-08],
       [9.7...e-01, 2.8...e-02, ...e-08]])
>>> clf.score(X, y)
0.97...

In [448]: X
Out[448]: 
array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
...
In [449]: y
Out[449]: 
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
In [450]: X.shape
Out[450]: (150, 4)

In [451]: y.shape
Out[451]: (150,)

In [454]: clf = LogisticRegression(random_state=0).fit(X, y)

In [455]: clf
Out[455]: 
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)

/ TODO

/ 13	. 

[eric@almond my]$ ipython log_reg_multiclass_202103.py 
Use: og_reg_multiclass_20102.py <features> <#classes> <#iteraties>
[eric@almond my]$ ipython log_reg_multiclass_202103.py  1 3 10
...
[eric@almond my]$ ipython log_reg_multiclass_202103.py  1,2 3 10
...

/ 13	. 

[eric@almond my]$ mv log_reg_multiclass_202103.py log_reg_multiclass_iris_202103.py
[eric@almond my]$ cp log_reg_multiclass_iris_202103.py log_reg_multiclass_self_202103.py

/ 13	. 

In [1076]: np.exp(Phi@W)/( np.ones(NS) @ np.exp(Phi@W) )
Out[1076]: 
array([[0.5       , 0.01798621, 0.5       ],
       [0.5       , 0.98201379, 0.5       ]])

[eric@almond my]$ ipython log_reg_multiclass_self_202103.py 1
T:

[[1 0]
 [0 1]]
W:

[[1. 0.]
 [0. 1.]]
PcGs:

[[0.5        0.01798621]							/ ERR	,
 [0.5        0.98201379]]
W:

[[ 2.          0.50915782]
 [-0.5         1.25457891]]

/ 13	.

[eric@almond my]$ git status
	modified:   args.py
	modified:   log_reg_multiclass_20102.py
	deleted:    log_reg_multiclass_202103.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	fun.py
	log_reg_multiclass_iris_202103.py
	log_reg_multiclass_self_202103.py

/ we hadden eerder	,
 1050  mv log_reg_multiclass_202103.py log_reg_multiclass_iris_202103.py
 1051  cp log_reg_multiclass_iris_202103.py log_reg_multiclass_self_202103.py


[eric@almond my]$ git rm log_reg_multiclass_202103.py
[eric@almond my]$ git add log_reg_multiclass_iris_202103.py
[eric@almond my]$ git add log_reg_multiclass_self_202103.py
[eric@almond my]$  git status
	renamed:    log_reg_multiclass_202103.py -> log_reg_multiclass_iris_202103.py
	new file:   log_reg_multiclass_self_202103.py

/ 13	. 


In [1102]: A
Out[1102]: 
array([[2, 4],
       [3, 6]])

In [1107]: v
Out[1107]: 
array([[1],
       [2]])

In [1104]: A*v
Out[1104]: 
array([[ 2,  4],
       [ 6, 12]])

In [1105]: A*v.T
Out[1105]: 
array([[ 2,  8],
       [ 3, 12]])

In [1110]: A/v
Out[1110]: 
array([[2. , 4. ],
       [1.5, 3. ]])

In [1111]: A/v.T
Out[1111]: 
array([[2., 2.],
       [3., 3.]])

/ Deel A door de som van de rij	, 

In [1117]: A/(A@np.ones((2,1)))
Out[1117]: 
array([[0.33333333, 0.66666667],
       [0.33333333, 0.66666667]])

/ 13	 .

/ v@v.T, maar dan 4 keer in een matrix, dus, 

(v@v.T v@v.T)
(v@v.T v@v.T)

In [1128]: np.kron(np.ones((2,2)),v @ v.T)
Out[1128]: 
array([[1., 2., 1., 2.],
       [2., 4., 2., 4.],
       [1., 2., 1., 2.],
       [2., 4., 2., 4.]])

/ 13	. 

/ maar nu met een diagonaalmatrix ertussen,

(v@R@v.T v@R@v.T)
(v@R@v.T v@R@v.T)

/ 1313	. 

/ zo kan het NIET,

In [1129]: R=np.diag([3,5])
In [1130]: R
Out[1130]: 
array([[3, 0],
       [0, 5]])

In [1131]: Itm=np.kron(np.ones((2,2)),R)
In [1132]: Itm
Out[1132]: 
array([[3., 0., 3., 0.],
       [0., 5., 0., 5.],
       [3., 0., 3., 0.],
       [0., 5., 0., 5.]])

In [1133]: v@ Itm @ v.T
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1133-954f9446d591> in <module>()
----> 1 v@ Itm @ v.T

ValueError: shapes (2,1) and (4,4) not aligned: 1 (dim 1) != 4 (dim 0)

/ 1313	. 

In [1151]: v
Out[1151]: 
array([[1],
       [2]])

In [1152]: v @ np.array([3]).reshape(-1,1) @ v.T
Out[1152]: 
array([[ 3,  6],
       [ 6, 12]])

/ NB. we moeten van (3) een 1x1 matrix maken	,

In [1144]: v @ np.array([3]) 
Out[1144]: array([3, 6])
/ en is geen staande vector meer, en gaat vervolgens met @ v.T fout	,

/ 1313	. 

In [1184]: A
Out[1184]: 
array([[2, 4],
       [3, 6]])

In [1185]: np.kron(np.ones((3,3)),A)
Out[1185]: 
array([[2., 4., 2., 4., 2., 4.],
       [3., 6., 3., 6., 3., 6.],
       [2., 4., 2., 4., 2., 4.],
       [3., 6., 3., 6., 3., 6.],
       [2., 4., 2., 4., 2., 4.],
       [3., 6., 3., 6., 3., 6.]])

In [1191]: np.kron(np.ones((3,3)),v@v.T)
Out[1191]: 
array([[1., 2., 1., 2., 1., 2.],
       [2., 4., 2., 4., 2., 4.],
       [1., 2., 1., 2., 1., 2.],
       [2., 4., 2., 4., 2., 4.],
       [1., 2., 1., 2., 1., 2.],
       [2., 4., 2., 4., 2., 4.]])

/ 1313	. 

In [1166]: A
Out[1166]: 
array([[2, 4],
       [3, 6]])

In [1167]: A.shape
Out[1167]: (2, 2)

In [1168]: type(A.shape)
Out[1168]: tuple

In [1169]: 3*A.shape
Out[1169]: (2, 2, 2, 2, 2, 2)

/ Maar we willen (6,6)	,

/ 1313	. 

/ lees,
https://stackoverflow.com/questions/1781970/multiplying-a-tuple-by-a-scalar

In [1181]: tuple([3*e for e in A.shape])
Out[1181]: (6, 6)
/ of,
In [1177]: [3*e for e in A.shape]
Out[1177]: [6, 6]

/ want np.ones vindt een tuple, of list, of numpy array goed,

/ 1313	. 

/ als je een col van A als diagonaal wilt hebben van een matrix, gebruik dan col als vector, niet als matrix, 

In [1192]: A
Out[1192]: 
array([[2, 4],
       [3, 6]])

In [1194]: A[:,0]
Out[1194]: array([2, 3])

In [1195]: A[:,[0]]
Out[1195]: 
array([[2],
       [3]])

In [1199]: A[:,0]*A[:,0]
Out[1199]: array([4, 9])

/ dit willen we	,
In [1198]: np.diag(A[:,0]*A[:,0])
Out[1198]: 
array([[4, 0],
       [0, 9]])

In [1196]: A[:,[0]]*A[:,[0]]
Out[1196]: 
array([[4],
       [9]])

/ dit willen we niet	,
In [1197]: np.diag(A[:,[0]]*A[:,[0]])
Out[1197]: array([4])

/ 1313	. 

In [1221]: A
Out[1221]: 
array([[2, 4],
       [3, 6]])

In [1222]: np.vstack([np.diag(A[:,j]) for j in range(A.shape[1])])
Out[1222]: 
array([[2, 0],
       [0, 3],
       [4, 0],
       [0, 6]])

In [1226]: Itm @ Itm.T
Out[1226]: 
array([[ 4,  0,  8,  0],
       [ 0,  9,  0, 18],
       [ 8,  0, 16,  0],
       [ 0, 18,  0, 36]])


/ 1313	. 

/ verschillen np.array, np.block, np.stack, np.vstack, np.hstack	,

In [1227]: A
Out[1227]: 
array([[2, 4],
       [3, 6]])

/ verschil np.array, np.block	,

In [1232]: np.array([np.diag(A[:,j]) for j in range(2)])
Out[1232]: 
array([[[2, 0],
        [0, 3]],

       [[4, 0],
        [0, 6]]])

In [1233]: np.array([np.diag(A[:,j]) for j in range(2)]).shape
Out[1233]: (2, 2, 2)

In [1234]: np.block([np.diag(A[:,j]) for j in range(2)])
Out[1234]: 
array([[2, 0, 4, 0],
       [0, 3, 0, 6]])

In [1235]: np.block([np.diag(A[:,j]) for j in range(2)]).shape
Out[1235]: (2, 4)

/ np.stack = np.array	,

In [1238]: np.stack([np.diag(A[:,j]) for j in range(2)])
Out[1238]: 
array([[[2, 0],
        [0, 3]],

       [[4, 0],
        [0, 6]]])

/ np.hstack = np.block	,

In [1240]: np.hstack([np.diag(A[:,j]) for j in range(2)])
Out[1240]: 
array([[2, 0, 4, 0],
       [0, 3, 0, 6]])

In [1243]: np.vstack([np.diag(A[:,j]) for j in range(A.shape[1])])
Out[1243]: 
array([[2, 0],
       [0, 3],
       [4, 0],
       [0, 6]])

In [1244]: np.vstack([np.diag(A[:,j]) for j in range(A.shape[1])]).shape
Out[1244]: (4, 2)

/ 1313	. 

/ Bishop (4.110) (210)

/ (c/s) NSxNC 
/ getallen kloppen niet: som der rijen moet 1 zijn: p(0|s)+p(1|s)+p(2|s)=1 als NC=3	, 
In [1285]: A
Out[1285]: 
array([[2, 4],
       [3, 6]])

/ NS=2, NC=2


In [1259]: Itm0=np.block([np.diag(A[:,j]) for j in range(A.shape[1])])
In [1271]: Itm0
Out[1271]: 
array([[2, 0, 4, 0],
       [0, 3, 0, 6]])

In [1276]: Itm1=Itm0.T @ -Itm0

In [1277]: Itm1
Out[1277]: 
array([[ -4,   0,  -8,   0],
       [  0,  -9,   0, -18],
       [ -8,   0, -16,   0],
       [  0, -18,   0, -36]])


In [1267]: [A[:,j] for j in range(A.shape[1])]
Out[1267]: [array([2, 3]), array([4, 6])]

In [1268]: np.block([A[:,j] for j in range(A.shape[1])])
Out[1268]: array([2, 3, 4, 6])

In [1269]: Itm2=np.diag(np.block([A[:,j] for j in range(A.shape[1])]))
In [1275]: Itm2
Out[1269]: 
array([[2, 0, 0, 0],
       [0, 3, 0, 0],
       [0, 0, 4, 0],
       [0, 0, 0, 6]])

In [1278]: Itm=Itm2+Itm1

In [1284]: Itm
Out[1284]: 
array([[ -2,   0,  -8,   0],
       [  0,  -6,   0, -18],
       [ -8,   0, -12,   0],
       [  0, -18,   0, -30]])

/ 131313	. 

/ NS=2
/ NF=1

/ Phi = NSxNF+1

In [1301]: Phi=np.array([1,-2,1,2]).reshape(-1,2)

In [1302]: Phi
Out[1302]: 
array([[ 1, -2],
       [ 1,  2]])

/ we moeten Phi en Phi.T in de diagonaal zetten van NCxNC	,

In [1311]: Phi_NC_diag=np.kron(np.eye(NC),Phi)
In [1304]: Phi_NC_diag
Out[1304]: 
array([[ 1., -2.,  0., -0.],
       [ 1.,  2.,  0.,  0.],
       [ 0., -0.,  1., -2.],
       [ 0.,  0.,  1.,  2.]])

In [1305]: Phi_NC_diag.T @ Itm @ Phi_NC_diag
Out[1305]: 
array([[  -8.,   -8.,  -26.,  -20.],
       [  -8.,  -32.,  -20., -104.],
       [ -26.,  -20.,  -42.,  -36.],
       [ -20., -104.,  -36., -168.]])

/ Bishop (4.110) (p.210)

/ 1313	 .

[eric@almond my]$ git status
On branch master
Your branch is up-to-date with 'origin/master'.
Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

	modified:   args.py
	modified:   log_reg_multiclass_20102.py
	deleted:    log_reg_multiclass_self_202103.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	fun.py
	log_reg_multiclass_iris_20210312.py
	log_reg_multiclass_iris_20210313.py
	log_reg_multiclass_self_20210313.py

[eric@almond my]$ git add log_reg_multiclass_iris_20210312.py
[eric@almond my]$ git add log_reg_multiclass_iris_20210313.py
[eric@almond my]$ git add log_reg_multiclass_self_20210313.py
[eric@almond my]$ git rm log_reg_multiclass_self_202103.py
rm 'log_reg_multiclass_self_202103.py'

[eric@almond my]$ git status
On branch master
Your branch is up-to-date with 'origin/master'.
Changes to be committed:
  (use "git reset HEAD <file>..." to unstage)

	renamed:    log_reg_multiclass_self_202103.py -> log_reg_multiclass_iris_20210312.py
	new file:   log_reg_multiclass_iris_20210313.py
	new file:   log_reg_multiclass_self_20210313.py




/ 1313	. 

[eric@almond my]$ ipython log_reg_multiclass_iris_20210313.py 0,1,2,3  2
[eric@almond my]$ ipython log_reg_multiclass_iris_20210313.py 3 3 2




/ 1313	. 

/ stel dat R zo berekent moet worden	,

In [1255]: R=Itm @ Itm.T
In [1256]: R
Out[1256]: 
array([[ 4,  0,  8,  0],
       [ 0,  9,  0, 18],
       [ 8,  0, 16,  0],
       [ 0, 18,  0, 36]])

/ hoe dan Phi.T @ <block in R> @ Phi	?

/ je kunt (Phi.T Phi.T) @ R (Phi
														 Phi)

/ 1313	. 

/ list comprehension

/ 13	. 

/ aanroep	,
[eric@almond my]$ ipython log_reg_multiclass_self_20210313.py
Use: log_reg_multiclass_self_202103.py <features> <classes> <#iteraties>
[eric@almond my]$ ipython log_reg_multiclass_self_20210313.py -- -2,2 0,1 1
/ of	,
[eric@almond my]$ ipython log_reg_multiclass_self_20210313.py -- -3,-2,-1,0,1,2,3 0,0,0,1,1,1,1 1

[eric@almond my]$ ipython log_reg_multiclass_iris_20210314.py 
Use: log_reg_multiclass_20102.py <features> <#iteraties>

/ 13	 .

[eric@almond my]$ ipython log_reg_multiclass_self_20210313.py -- -2,2 0,1 1

/ Hessian @ v = grad : 

(8/81 0 -8/81 0		= 2/9
 0   2/9 0   -2/9 = 8/9
(-8/81 0 8/81 0   = -2/9
 0   -2/9 0   2/9 = -8/9)

/ lu:

(8/81 0 -8/81 0		= 2/9
 0   2/9 0   -2/9 = 8/9
(0    0  0    0   = 0
(0    0  0    0   = 0

/ opl: 

<(1,0,1,0).T,(0,1,0,1).T>+(9/4,4,0,0)

/ lu van python komt hier niet uit	,

/ zie blaadje B16

/ TODO

/ 7	. 

[eric@almond my]$ ipython log_reg_1_class_features_newton_self_202102.py -- -2,2 0,1 2
/ en in licplise,
log_reg_1_class_features_self_geron.py	,
/=
[eric@almond my]$ ipython log_reg_1_class_features_self_geron_20210315.py -- -2,2 0,1 
intercept_
[0.]
coef_
[[5.60143377]]



/ in liclipse	, 

	lg=LogisticRegression(solver="newton-cg",C=10**10) 
/ dan is 
multi_class='ovr'
	fit(...)
/s,
    def fit(self, X, y, sample_weight=None):

        self.classes_ = np.unique(y)
[0,1]

        fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
                               backend=backend)(
            path_func(X, y, pos_class=class_, Cs=[self.C],										/ pos_class=1
                      fit_intercept=self.fit_intercept, tol=self.tol,
                      verbose=self.verbose, solver=self.solver,
                      multi_class=self.multi_class, max_iter=self.max_iter,
                      class_weight=self.class_weight, check_input=False,
                      random_state=self.random_state, coef=warm_start_coef_,
                      penalty=self.penalty,
                      max_squared_sum=max_squared_sum,
                      sample_weight=sample_weight)
            for class_, warm_start_coef_ in zip(classes_, warm_start_coef))			 / classes_=[1], TODO, class_=1
/s,
    def __call__(self):
        return [func(*args, **kwargs) for func, args, kwargs in self.items]
args=<class 'tuple'>: (array([[-2.], [ 2.]]), array([0, 1]))
kwargs=dict: {'pos_class': 1, 'Cs': [10000000000], 'fit_intercept': True, 'tol': 0.0001, 'verbose': 0, 'solver': 'newton-cg', 'multi_class': 'ovr', 'max_iter': 100, 'class_weight': None, 'check_input': False, 'random_state': None, 
 'coef': None, 'penalty': 'l2', 'max_squared_sum': None, 'sample_weight': None}
/s,
def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
                          scoring=None, fit_intercept=False,
                          max_iter=100, tol=1e-4, class_weight=None,
                          verbose=0, solver='lbfgs', penalty='l2',
                          dual=False, intercept_scaling=1.,
                          multi_class='ovr', random_state=None,
                          max_squared_sum=None, sample_weight=None):
    """Computes scores across logistic_regression_path

    pos_class : int, None
        The class with respect to which we perform a one-vs-all fit.
        If None, then it is assumed that the given problem is binary.


    # For doing a ovr, we need to mask the labels first. for the
    # multinomial case this is not necessary.
    if multi_class == 'ovr':
        w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)
n_features=1
fit_intercept=True
w0=[0,0]
        mask_classes = np.array([-1, 1])
        mask = (y == pos_class)
/ TODO
[False,True]
        y_bin = np.ones(y.shape, dtype=X.dtype)
[1,1]
        y_bin[~mask] = -1.
y_bin=[-1,1]

    else:
        target = y_bin

        elif solver == 'newton-cg':
            func = _logistic_loss
            grad = lambda x, *args: _logistic_loss_and_grad(x, *args)[1]
            hess = _logistic_grad_hess

    for i, C in enumerate(Cs):
        if solver == 'lbfgs':
BroydenFletcherGoldfarbShanno algorithm
/n,
        elif solver == 'newton-cg':
/j,
            args = (X, target, 1. / C, sample_weight)
<class 'tuple'>: (array([[-2.],[ 2.]]), array([-1.,  1.]), 1e-10, array([1., 1.]))
/ dus class 0 is -1 geworden	,
/ TODO
            w0, n_iter_i = newton_cg(hess, func, grad, w0, args=args,
                                     maxiter=max_iter, tol=tol)
/s,
def newton_cg(grad_hess, func, grad, x0, args=(), tol=1e-4,
              maxiter=100, maxinner=200, line_search=True, warn=True):
    if line_search:
/j,
        old_fval = func(x0, *args)
x0=ndarray: [0. 0.]	 # w
/s,
def _logistic_loss(w, X, y, alpha, sample_weight=None):
/ Bishop (4.90) (p. 206)
/ we gaan uitrekenen p(0|-2) en p(1|2)	, = sigma(w.dot(1,-2)), sigma(w.dot(1,2))
    w, c, yz = _intercept_dot(w, X, y)	 # inproduct met de intercept erbij, dus met (1,-2).dot(w) en (1,2).dot(w)
w=[0,0]
X=[[-2],
	[2]]
y=[-1,1]
/s,
def _intercept_dot(w, X, y):
    """Computes y * np.dot(X, w).
    It takes into consideration if the intercept should be fit or not.

    if w.size == X.shape[1] + 1:	/ X.shape[1]=1	, X is zonder intercept	, daarom +1 voor intercept	,
        c = w[-1]	/ de intercept	, staat bij hun achteraan	,	= w0
0.
        w = w[:-1]	/ niet de intercept	,	= w1
0.
    z = safe_sparse_dot(X, w) + c
[0,0]	/ klopt	, 

/ wij doen altijd 
w.T.dot(1,-2).T = w0-2w1		, phi1=(1,-2).T
w.T.dot(1,2).T = w0+2w1			, phi2=(1,2).T
/ zij doen:
(-2,2).T.dot(w1)+w0=-2w1+w0, 2w1+w0
/ dus z= beide w.T.dot(phi1)

/ TODO check,	

    yz = y * z
y=[-1,1]
/ de error fct hieronder is log(p(0|-2) + log(p(1|2)	=log(1-p(1|-2))+log(p(1|2)=log(1-sigma(w.T.dot(1,-2)))+log(sigma(w.T.dot(1,2)))=log(sigma(-w.T.dot(1,-2)))+log(sigma(w.T.dot(1,2))) : 1-sigma(x)=sigma(-x)	. Daarom verm. ze met y.
/ loss fct = - error fct	,

/ dus je hebt -(w0-2w1), (w0+2w1), en dan sgm(-(w0-2w1)), sgm(w0+2w1) = p(0|-2), p(1|2)
/ want sgm(-x)=1-sgm(x)
/t,
def _logistic_loss(w, X, y, alpha, sample_weight=None):
    w, c, yz = _intercept_dot(w, X, y)
/d,
    # Logistic loss is the negative of the log of the logistic function.
    out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)
/ Bishop (4.90)
/s,
def log_logistic(X, out=None):
/ X=[-0,0]	, omdat we 2 samples hebben	,
/ je berekent w.T.dot((1,-2)) , en dat is de 1ste 0	, en w.T.dot((1,2)), en dat is de 2de 0, 	
    """Compute the log of the logistic function, ``log(1 / (1 + e ** -x))``.

    This implementation is numerically stable because it splits positive and
    negative values::

        -log(1 + exp(-x_i))     if x_i > 0
        x_i - log(1 + exp(x_i)) if x_i <= 0

    For the ordinary logistic function, use ``scipy.special.expit``.

/ klopt	, we willen berekenen de loss fct, dus log(p(0|-2) + log(p(1|2)	, later nemen we de - en krijgen de cost fct	,
/ hier in log_logistic berekenen we log(p(1|xi)). 
/ wat hier staat met xi>0 of xi<=0 klopt	, 
log(1/(1+e^-x))=-log(1+e^-x)
log(1/(1+e^-x))=log(e^x/e^x+1)=x-log(e^x+1)

/t,
def _logistic_loss(w, X, y, alpha, sample_weight=None):
    out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)
/d,
float64: 1.3862943611198906
/ klopt	, 

/t,
ef newton_cg(grad_hess, func, grad, x0, args=(), tol=1e-4,
              maxiter=100, maxinner=200, line_search=True, warn=True):
    if line_search:
        old_fval = func(x0, *args)
/d,
        old_old_fval = None

    while k < maxiter:
        # Compute a search direction pk by applying the CG method to
        #  del2 f(xk) p = - fgrad f(xk) starting from 0.
        fgrad, fhess_p = grad_hess(xk, *args)
/s,
def _logistic_grad_hess(w, X, y, alpha, sample_weight=None):

    n_samples, n_features = X.shape
2,1
X=[[-2]
	[2]]
X.shape=(2,1)
    grad = np.empty_like(w)											/ (2,)
    fit_intercept = grad.shape[0] > n_features 	
/j, grad.shape=(2,)	, 2>1
    w, c, yz = _intercept_dot(w, X, y)
yz=[-0,0]	, want y=[-1,1]			/ loss=-log(1-sgm(w.T@(1,-2).T)-log(sgm(w.T@(1,2).T)
															/ = -log(sgm(-(w.T@(1,-2).T)))-log(sgm(w.T@(1,2).T)	 / vandaar de -
    z = expit(yz)	/ neem sgm	,
[.5,.5]
    z0 = sample_weight * (z - 1) * y

/ zij doen, 
z=(sgm(-(w0-2w1)),sgm(w0+2w1)).T=(1-sgm(w0-2w1),sgm(w0+2w1)).T		= (p(0|(1,-2).T,p(1|(1,2)).T).T
/ z-1=(-sgm(w0-2w1),sgm(w0+2w1)-1).T, (z-1)*y=(sgm(w0-2w1),sgm(w0+2w1)-1).T= y-t Bishop(4.96), 
	/ waarin y=(p(1|(1,-2).T),p(1|(1,2).T)

/ klopt	,
(.5,-.5)

    grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w

/ wij doen Phi.T*(y-t) = 
( 1 1) * (.5	= (0
(-2 2)	 -.5)		-2)

/ zij berekenen grad[:1]=(-2,2).dot((.5,-.5).T)=-2
grad=[-2,0]
    if fit_intercept:
        grad[-1] = z0.sum()
0
/ Klopt	,
/ doen wij ook : (1,1).dot(.5, -.5).T = som nemen	,
/ zij zetten de intercept onderaan; wij altijd bovenaan	,

    d = sample_weight * z * (1 - z)
/ [1/4,1/4]
/ bij ons R Bishop (4.97)	,
    if sparse.issparse(X):
/n,
    else:
/j,
        # Precompute as much as possible
        dX = d[:, np.newaxis] * X

/i,
In [1551]: d[:,np.newaxis]
Out[1551]: 
array([[0.25],
       [0.25]])

In [1552]: d[:,np.newaxis].shape
Out[1552]: (2, 1)
/ Einde i	,

X=[[-2.]
	[ 2.]]

dX=[[-0.5]
 [ 0.5]]

    if fit_intercept:
        # Calculate the double derivative with respect to intercept
        # In the case of sparse matrices this returns a matrix object.
        dd_intercept = np.squeeze(np.array(dX.sum(axis=0)))

In [1558]: dX.sum(axis=0)
Out[1558]: array([0.])
In [1570]: np.array(dX.sum(axis=0))	 / doet niets in ons geval,
Out[1570]: array([0.])
In [1571]: np.squeeze(np.array(dX.sum(axis=0)))
Out[1571]: array(0.)

    def Hs(s):
        ret = np.empty_like(s)
        ret[:n_features] = X.T.dot(dX.dot(s[:n_features]))		/ X.T.dot(dX) = hessian	, zonder de intercept	,
        ret[:n_features] += alpha * s[:n_features]

        # For the fit intercept case.
        if fit_intercept:
            ret[:n_features] += s[-1] * dd_intercept
            ret[-1] = dd_intercept.dot(s[:n_features])
            ret[-1] += d.sum() * s[-1]
        return ret

    return grad, Hs

/i, 

/ we schrijven de intercept bovenaan,

X'.T@d@X'@grad = 1 1  @ a 0 @ 1 -2 @ x
							  -2 2		0 b   1  2   y 

/ zij schrijven de intercept beneden	,

X'.T@d@X'@grad = -2 2	@	a 0 @  -2 1 @ y 
 								  1 1   0 b     2 1   x

/ we schrijven X' , want bij hun is X=(-2,2).T, dus maar 1 col van X'	,

/ denk in block matrices,	je verm. bijv,
-2 @ -2 1 ipv 1 @ 1 -2 met a ertussen	,  dus a blijft ertussen	,
1						 -2

/ en aan -2 1 @ y zie je dat je de volgorde -2 1 en 2 1 zo moet	, 
          2 1   x

/ werk uit:

(4a+4b		-2a+2b)@(y
(-2a+2b   a+b		)  x)

/ 1	.

ret[:n_features] = X.T.dot(dX.dot(s[:n_features])) voorlopig	,

/ bij hun is X=(-2,2).T	, en d=(a b) , en dX=d@X=(-2a,2b), dus X.T@d@X=4a+4b en dat wordt @ y	,

/ 2	. 

            ret[:n_features] += s[-1] * dd_intercept

/ dd_intercept=dX.sum(axis=0)	, = -2a +2b	, maar die staat ook rechtsboven	, dus uiteindelijk, 
	ret[:n_features]= (4a+4b)y+(-2a+2b)x

/ 3	.

            ret[-1] = dd_intercept.dot(s[:n_features])

/ intercept= (-2a+2b)y voorlopig	,

            ret[-1] += d.sum() * s[-1]

/ intercept= (-2a+2b)y +(a+b)x uiteindelijk	, 
 
/ Einde i,

/t,
def newton_cg(grad_hess, func, grad, x0, args=(), tol=1e-4,
              maxiter=100, maxinner=200, line_search=True, warn=True):
        fgrad, fhess_p = grad_hess(xk, *args)
/d,
        absgrad = np.abs(fgrad)
(2,0).T
        if np.max(absgrad) < tol:
            break

        maggrad = np.sum(absgrad)
        eta = min([0.5, np.sqrt(maggrad)])
        termcond = eta * maggrad

        xsupi = _cg(fhess_p, fgrad, maxiter=maxinner, tol=termcond)

/ we willen oplossen: Hessian@x=grad

fgrad=ndarray: [-2.  0.]	/ intercept_ achteraan	,
/s,
def _cg(fhess_p, fgrad, maxiter, tol):
    """
    Solve iteratively the linear system 'fhess_p . xsupi = fgrad'
    with a conjugate gradient descent.


    xsupi = np.zeros(len(fgrad), dtype=fgrad.dtype)
[0,0] 	/ w	,
    ri = fgrad	/ (0,-2).T bij ons, (-2,0).T bij hun	,   
    psupi = -ri 			/ bij ons (0,-2).T, bij hun (2,0).T
    i = 0
    dri0 = np.dot(ri, ri)	/ 4 bij beide	,

    while i <= maxiter:
        if np.sum(np.abs(ri)) <= tol:
/n,
            break
        Ap = fhess_p(psupi)	 	/ bij ons (0,-4).T, bij hun (4,0).T

        curv = np.dot(psupi, Ap)	/ 8 bij beide	,
				...
        alphai = dri0 / curv	 / .5 bij beide,	 
        xsupi += alphai * psupi 	/ 0+(0,-1).T bij ons, 0+(1,0).T bij hun	,
        ri = ri + alphai * Ap		/ (0,-2)-.5*(0,-4)=0	, bij hun (-2,0).T+.5*(4,0)=0	, er staat +alphai, bij ons -	,
        dri1 = np.dot(ri, ri)
        betai = dri1 / dri0
        psupi = -ri + betai * psupi
        i = i + 1
        dri0 = dri1          # update np.dot(ri,ri) for next time.

/ volgende,

    while i <= maxiter:
        if np.sum(np.abs(ri)) <= tol:
/ j,
            break

    return xsupi / bij ons (0,-1).T, bij hun (1,0)	,

/t,
def newton_cg(grad_hess, func, grad, x0, args=(), tol=1e-4,
              maxiter=100, maxinner=200, line_search=True, warn=True):
        # Inner loop: solve the Newton update by conjugate gradient, to
        # avoid inverting the Hessian
        xsupi = _cg(fhess_p, fgrad, maxiter=maxinner, tol=termcond)
/d,
/ bij ons (0,-1).T, bij hun (1,0)	,











/ Einde MY ML 

/ GERON ML 

[eric@almond my]$ grep LogisticRegression *.py
log_reg_1_class_1_feature_iris_geron_202102.py:from sklearn.linear_model import LogisticRegression
log_reg_1_class_1_feature_iris_geron_202102.py:#lg=LogisticRegression()
log_reg_1_class_1_feature_iris_geron_202102.py:lg=LogisticRegression(solver="liblinear",C=10**10) 
log_reg_1_class_1_feature_iris_geron_202102.py:#lg=LogisticRegression(C=10**10)
log_reg_1_class_1_feature_iris_geron_202102.py:#lg=LogisticRegression(C=10)
log_reg_1_class_2_features_iris_geron_202102.py:from sklearn.linear_model import LogisticRegression
log_reg_1_class_2_features_iris_geron_202102.py:lg=LogisticRegression(solver="liblinear",C=10**10) 
log_reg_1_class_4_features_iris_geron_202102.py:from sklearn.linear_model import LogisticRegression
log_reg_1_class_4_features_iris_geron_202102.py:lg=LogisticRegression(solver="liblinear",C=10**10) 
log_reg_3_class_1_features_iris_geron_202102.py:from sklearn.linear_model import LogisticRegression
log_reg_3_class_1_features_iris_geron_202102.py:lg=LogisticRegression(multi_class="multinomial",solver="newton-cg",C=10**10) 
log_reg_3_class_4_features_iris_geron_202102.py:from sklearn.linear_model import LogisticRegression
log_reg_3_class_4_features_iris_geron_202102.py:lg=LogisticRegression(multi_class="multinomial",solver="newton-cg",C=10**10) 
log_reg_3_classes_2_features_newton_geron.py:from sklearn.linear_model import LogisticRegression
log_reg_3_classes_2_features_newton_geron.py:#lg=LogisticRegression(C=10**10)
log_reg_3_classes_2_features_newton_geron.py:lg=LogisticRegression(multi_class="multinomial",solver="lbfgs",C=10) # C=10 !
log_reg_3_classes_2_features_newton_geron.py:#lg=LogisticRegression(C=10)
log_reg_3_classes_2_features_sklearn.py:from sklearn.linear_model import LogisticRegression
log_reg_3_classes_2_features_sklearn.py:lg=LogisticRegression(C=10**10)
log_reg_sklearn.py:from sklearn.linear_model import LogisticRegression
log_reg_sklearn.py:log_reg = LogisticRegression(solver="liblinear", C=10**

/ 1313	. 

[eric@almond my]$ ipython log_reg_1_class_1_feature_iris_geron_202102.py 3 2
array([-21.12560598])
array([[12.94748578]])

/ de 1ste kan weg,	 en de 2de moet anders heten	,
log_reg_1_class_2_features_iris_geron_202102.py
log_reg_1_class_2_features_iris_geron_202102.py

[eric@almond my]$ ipython log_reg_1_class_2_features_iris_geron_202102.py 3 2
array([-21.12560598])
array([[12.94748578]])

[eric@almond my]$ ipython log_reg_1_class_2_features_iris_geron_202102.py 3 2
array([-21.12560598])
array([[12.94748578]])
/ maar	, 
[eric@almond my]$ ipython log_reg_1_class_2_features_iris_geron_202102.py 0,1,2,3 2
array([-42.46853369])
array([[-2.46312182, -6.66512187,  9.40005677, 18.23704717]])
/ TODO




/ 1313	. 

/ 1ste kan weg, 2de hernoemen	,	
log_reg_3_class_1_features_iris_geron_202102.py:
log_reg_3_class_4_features_iris_geron_202102.py

[eric@almond my]$ ipython log_reg_3_class_4_features_iris_geron_202102.py 3
array([ 32.25741399,  -5.56591178, -26.69150222])
array([[-36.81302781],
       [ 11.93277549],
       [ 24.88025232]])

[eric@almond my]$ ipython log_reg_3_class_4_features_iris_geron_202102.py 0,1,2,3
array([  2.32925138,  20.15434543, -22.48359681])	 / class 0,1,2
array([[  4.8858444 ,   7.84777582, -12.84179439,  -6.6637919 ],	/ class 0
       [ -1.2103132 ,  -0.58344044,   1.70619436,  -5.81118631],	 / class 1
       [ -3.6755312 ,  -7.26433538,  11.13560003,  12.47497821]])	 / class 2
feature 	0						1						2							3


/ Deze doet het goed, vergl met 1 class hierboven: feature 3, class 2	, 
/ je moet alle features kiezen	,

/ 13	. 

/ self algoritm on iris en self data
-rw-rw-r--. 1 eric eric 4751 Mar 14 00:26 log_reg_multiclass_iris_20210313.py
-rw-rw-r--. 1 eric eric 5144 Mar 14 21:53 log_reg_multiclass_iris_20210314.py		<<<
-rw-rw-r--. 1 eric eric 5055 Mar 14 22:17 log_reg_multiclass_self_20210313.py		<<<

/ sklearn on iris
-rw-rw-r--. 1 eric eric 1633 Mar 15 22:16 log_reg_1_class_2_features_iris_geron_202102.py		<<<  1 class LogisticRegression 
-rw-rw-r--. 1 eric eric 1648 Mar 15 22:20 log_reg_3_class_4_features_iris_geron_202102.py		<<<	 multinomial LogisticRegression
-rw-rw-r--. 1 eric eric 1674 Mar 15 21:51 log_reg_1_class_1_feature_iris_geron_202102.py	 
-rw-rw-r--. 1 eric eric 1584 Mar 15 21:54 log_reg_3_class_1_features_iris_geron_202102.py
-rw-rw-r--. 1 eric eric 1661 Mar 15 22:17 log_reg_3_classes_2_features_newton_geron.py

/ 13	. 

/ sklearn 1 nomial op self voorbeeld,	

[eric@almond my]$ cp log_reg_1_class_2_features_iris_geron_202102.py log_reg_1_class_features_self_geron_20210315.py

$ vi log_reg_1_class_features_self_geron_20210315.py

phi=np.array(sys.argv[1].split(',')).astype(int).reshape(-1,1)	/ moet 2-dim array	,
t=np.array(sys.argv[2].split(',')).astype(int)
lg=LogisticRegression(solver="liblinear",C=10**10)
lg.fit(phi,t)
display(lg.intercept_)
display(lg.coef_)

[eric@almond my]$ ipython log_reg_1_class_features_self_geron_20210315.py -- -2,2 0,1
array([0.])
array([[5.60143377]])

/ 13	 .

$ vi log_reg_multi_class_features_self_geron_20210315.py

phi=np.array(sys.argv[1].split(',')).astype(int).reshape(-1,1)
t=np.array(sys.argv[2].split(',')).astype(int)
lg=LogisticRegression(multi_class="multinomial",solver="newton-cg",C=10**10)
lg.fit(phi,t)
display(lg.intercept_)
display(lg.coef_)

[eric@almond my]$ ipython log_reg_multi_class_features_self_geron_20210315.py -- -2,2 0,1
array([-8.57543388e-13])
array([[2.80071759]])

/ 13	. 

[eric@almond my]$ ls ~/Devel/Eclipse/python/workspace/logisticregression/src/root/nested/
example.py  __init__.py  log_reg_sklearn2.py  log_reg_sklearn.py

/ in liclipse	, right click log_reg_sklearn2.py, 
/ run as , python run, 
/ of	,
/ debug as,  








/ Einde GERON ML 








/ Einde DG

/ MY PYTHON

/ 13	

In [17]: a=np.ones((3,3))
In [19]: a
Out[19]: 
array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]])
/ het ziet eruit als een array van rows, met iedere row een array	,
In [18]: a.dtype
Out[18]: dtype('float64')
In [20]: a.shape
Out[20]: (3, 3)

In [21]: a[0]
Out[21]: array([1., 1., 1.])
In [22]: type(a[0])
Out[22]: numpy.ndarray
In [23]: a[0].shape
Out[23]: (3,)


In [29]: a=np.arange(1,10)
In [30]: a.shape
Out[30]: (9,)
In [31]: a=a.reshape(3,3)
In [32]: a.shape
Out[32]: (3, 3)

/ 13	. 

/ transpose vector	,

In [58]: a=np.array([1,2,3])
In [74]: a.shape
Out[74]: (3,)

/ 1313	. 

In [77]: c=a.reshape(3,1)

In [78]: c.shape
Out[78]: (3, 1)

In [79]: c
Out[79]: 
array([[1],
       [2],
       [3]])

/ of	,

In [91]: d=np.c_[a]
In [92]: d.shape
Out[92]: (3, 1)
In [93]: d
Out[93]: 
array([[1],
       [2],
       [3]])

/ let op	,

In [132]: np.r_[a].shape
Out[132]: (3,)
/ en niet (3,1)	,


/ 1313	. 

In [59]: b=np.array([4,5,6])
In [86]: c=np.c_[a,b]
In [87]: c
Out[87]: 
array([[1, 4],
       [2, 5],
       [3, 6]])
In [88]: c.shape
Out[88]: (3, 2)

/ lees,
https://stackoverflow.com/questions/22053050/difference-between-numpy-array-shape-r-1-and-r

/ 1313	. 

In [105]: a2=c[:,0]

In [106]: a2.shape
Out[106]: (3,)
/ en niet (3,1)

In [107]: a2
Out[107]: array([1, 2, 3])

In [111]: b2=c[0,:]

In [112]: b2
Out[112]: array([1, 4])

In [113]: b2.shape
Out[113]: (2,)
/ en niet (1,2)

/ 1313	. 

/ maar je kunt gewoon met vectoren links en rechts verm	,

In [116]: a2
Out[116]: array([1, 2, 3])
/ in column space	,

In [118]: c
Out[118]: 
array([[1, 4],
       [2, 5],
       [3, 6]])

In [119]: b2
Out[119]: array([1, 4])
/ in row space	, 

In [120]: x=c.dot(b2)
In [121]: x.shape
Out[121]: (3,)
In [122]: x
Out[122]: array([17, 22, 27])

In [124]: y=a2.dot(c)
In [125]: y.shape
Out[125]: (2,)
In [126]: y
Out[126]: array([14, 32])

/ als je a2 neerlegt, is het antwoord ook een liggende ...
In [135]: a2.reshape(1,3).dot(c)
Out[135]: array([[14, 32]])
In [136]: a2.reshape(1,3).dot(c).shape
Out[136]: (1, 2)

n [138]: c.dot(b2.reshape(2,1))
Out[138]: 
array([[17],
       [22],
       [27]])
In [139]: c.dot(b2.reshape(2,1)).shape
Out[139]: (3, 1)

/ 1313	. 

/ truc,

In [152]: pw=iris.data[:,3]
In [154]: pw.shape
Out[154]: (150,)
 
In [157]: pw2=iris.data[:,[3]]
In [158]: pw2.shape
Out[158]: (150, 1)

/ maar voor np.ones,	 np.c_ maakt het niet uit,

In [169]: o=np.ones(pw.shape,dtype=int)
In [170]: o.shape
Out[170]: (150, 1)

In [164]: o2=np.ones(pw2.shape,dtype=int)
In [165]: o2.shape
Out[165]: (150, 1)

/ maar voor np.c_ kun je ook nemen:
In [174]: o[:,0].shape
Out[174]: (150,)

/ 2 vectors 	,
In [177]: np.c_[o[:,0],pw].shape
Out[177]: (150, 2)
/ 1 matrix, 1 vector,	
In [178]: np.c_[o,pw].shape
Out[178]: (150, 2)
/ 2 matrices	,
In [167]: np.c_[o2,pw2].shape
Out[167]: (150, 2)

/ 1313	. 

/ zet vector rechtop	,

/ met reshape(-1,1)

/ 1313	. 

n [191]: iris['target']==2
Out[191]: 
array([False, False, False, False,...
In [192]: (iris['target']==2).shape
Out[192]: (150,)
In [193]: (iris['target']==2)[:2]
Out[193]: array([False, False])

/ 1313	. 

/ we zien hetzelfde bij np.exp: als arg=vector, is het antwoord het ook	; als arg is 'n matrix is het antwoor het ook	,

In [198]: c.dot(np.array([1,4]))
Out[198]: array([17, 22, 27])

In [199]: np.exp(c.dot(np.array([1,4])))
Out[199]: array([2.41549528e+07, 3.58491285e+09, 5.32048241e+11])

In [201]: np.exp(c.dot(np.array([1,4])).reshape(-1,1))
Out[201]: 
array([[2.41549528e+07],
       [3.58491285e+09],
       [5.32048241e+11]])

/ 1313	. 

/ ravel,

/ is terug naar vector	,

In [218]: a.shape
Out[218]: (3,)
In [214]: a.reshape(-1,1).shape
Out[214]: (3, 1)
In [215]: a.reshape(-1,1).ravel().shape
Out[215]: (3,)
In [219]: a.reshape(-1,1).shape
Out[219]: (3, 1)
In [217]: a.reshape(1,-1).ravel().shape
Out[217]: (3,)

/ 13	. 

In [409]: p1.shape
Out[409]: (150,)
In [410]:  (p1*p1).shape
Out[410]: (150,)
/ puntsgewijs verm	,

In [405]: p1@(1-p1)
Out[405]: 37.5
In [407]: p1.T@(1-p1)
Out[407]: 37.5
/ .T maakt bij vectoren niet uit	,


/ 7	. 

/ imports	,

/ 13	 

/ in interactive ipython	,

In [1]: import numpy as np
In [2]: from sklearn import datasets
/ je import een deel van sklearn, sklearn zelf kent deze ipython niet	,
In [390]: from scipy.special import expit

/ wat ook kan is	,
from sklearn import datasets as ds

In [397]: sklearn....
/ kan niet, zoals we boven zagen	,	
In [398]: import sklearn as sk
/ deze ipython kent nu heel sklearn	,
In [399]: sk.datasets.load_iris()
/ kan wel	,
In [397]: sklearn....
/ kan nog steeds niet	,



/ we hoeven dus niet te doen,
from ... import numpy

In [392]: import scipy as sp
In [395]: sp.special.expit(0)
Out[395]: 0.5

In [393]: import sp.special as sl
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-393-9fdaba6119b9> in <module>()
----> 1 import sp.special as sl

In [394]: import scipy.special as sl
In [396]: sl.expit(0)
Out[396]: 0.5

/ 13	. 

/ lees,
https://docs.scipy.org/doc/scipy/reference/index.html

/ scipy heeft submodules,
scipy.special
scipy.linalg
...

/ lees,
https://scikit-learn.org/stable/modules/classes.html?highlight=datasets#module-sklearn.datasets

/ ze noemen datasets ook een module	,

/ we zien dat	, 

from sklearn import datasets
iris=datasets.load_iris()
/ OK	,

import sklearn as sk
iris=sk.datasets.load_iris() # does not work TODO
/ ERR, werkt niet	, 
/ TODO

import scipy as sp
import scipy.linalg as la
    p1=sp.special.expit(Phi@w)     # p1|phi  (150,)
    w=w-la.lu_solve(la.lu_factor(H),grad)
/ OK	,


/ 7	. 

In [435]: phi.shape
Out[435]: (150,)
In [430]: phi_=phi.reshape(150,1)
In [431]: t_=t.reshape(150,1)
In [433]: phi_[t_==0].shape
Out[433]: (100,)
In [434]: phi_[(t_==0).ravel()].shape
Out[434]: (100, 1)
/ TODO
 
/ 13	. 

/ hier werkte sk.datasets niet	, 
/ maar sp.special wel	,
-rw-rw-r--. 1 eric eric  952 Feb 20 20:19 log_reg_1_class_1_feature_newton_iris_202102.py
import numpy as np
from sklearn import datasets
# import sklearn as sk
import scipy as sp
import scipy.linalg as la

# iris=sk.datasets.load_iris() # does not work TODO
iris=datasets.load_iris()

phi=iris['data'][:,3]    # (150,)
Phi=np.c_[np.ones(len(phi),dtype='int'),phi] # (150,2)
w=np.zeros(2) # (2,)
t=(iris['target']==2).astype(int) # (150,)
p1=sp.special.expit(Phi@w)     # p1|phi # (150,)

/ hier werkt sp.special niet	,
[eric@almond my]$ cat log_reg_1_class_1_feature_newton_eigen_voorbeelden_202102.py
mport numpy as np
from scipy import special 
import scipy.linalg as la

phi=np.array([-1,1])     # (2,)
t=np.array([0,1]) # (2,)

Phi=np.c_[np.ones(len(phi),dtype='int'),phi] # (2,2)
w=np.zeros(2) # (2,)
for i in range(0,100):
    sgm=special.expit(Phi@w)     # p(1|phi)=sgm  (2,)

/ 13	. 

/ args to python script	,

[eric@almond my]$ cat args.py 
import sys
display(sys.argv[0])
display(int(sys.argv[1]))
display(int(sys.argv[2]))
[eric@almond my]$ ipython args.py 13 7
'/home/eric/Devel/python/my/args.py'
13
7

/ 7	 .

/ randint	,

Parameters
----------
low : int
    Lowest (signed) integer to be drawn from the distribution (unless
    ``high=None``, in which case this parameter is one above the
    *highest* such integer).




/ Einde MY PYTHON

/ MY 

/ 7	. 

/ 13	. 

In [33]:  M
Out[33]: 
array([[1, 2],
       [3, 4]])


In [31]: np.kron(M,np.eye(2))
Out[31]: 
array([[1., 0., 2., 0.],
       [0., 1., 0., 2.],
       [3., 0., 4., 0.],
       [0., 3., 0., 4.]])

In [32]: np.kron(np.eye(2),M)
Out[32]: 
array([[1., 2., 0., 0.],
       [3., 4., 0., 0.],
       [0., 0., 1., 2.],
       [0., 0., 3., 4.]])

/ 13	. 

In [33]:  M
Out[33]: 
array([[1, 2],
       [3, 4]])

In [57]: np.kron(np.array([1,1]),np.diag(M[:,0]))
Out[57]: 
array([[1, 0, 1, 0],
       [0, 3, 0, 3]])

In [58]: np.kron(np.array([1,1]),np.diag(M[:,1]))
Out[58]: 
array([[2, 0, 2, 0],
       [0, 4, 0, 4]])

In [59]: K=np.r_[np.kron(np.array([1,1]),np.diag(M[:,0])),np.kron(np.array([1,1]),np.diag(M[:,1]))]
Out[59]: 
array([[1, 0, 1, 0],
       [0, 3, 0, 3],
       [2, 0, 2, 0],
       [0, 4, 0, 4]])

In [68]: K.T
Out[68]: 
array([[1, 0, 2, 0],
       [0, 3, 0, 4],
       [1, 0, 2, 0],
       [0, 3, 0, 4]])

/ 13	. 

In [96]: v=np.array([1,2,3])

In [98]: v@v.T
Out[98]: 14

/ dus dat gaat niet goed,	

In [105]: v=v.reshape(3,1)

In [106]: v
Out[106]: 
array([[1],
       [2],
       [3]])

In [108]: v@v.T
Out[108]: 
array([[1, 2, 3],
       [2, 4, 6],
       [3, 6, 9]])

/ nu wel	,

/ 13	. 

In [111]: a=array([1, 2, 3])

In [112]: v=a.reshape(3,1)

In [113]: a.shape
Out[113]: (3,)

In [114]: v.shape
Out[114]: (3, 1)

In [115]: np.kron(a,a)
Out[115]: array([1, 2, 3, 2, 4, 6, 3, 6, 9])

In [116]: np.kron(v,v)
Out[116]: 
array([[1],
       [2],
       [3],
       [2],
       [4],
       [6],
       [3],
       [6],
       [9]])

In [117]: np.kron(v,v.T)
Out[117]: 
array([[1, 2, 3],
       [2, 4, 6],
       [3, 6, 9]])

In [120]: v@v.T
Out[120]: 
array([[1, 2, 3],
       [2, 4, 6],
       [3, 6, 9]])

/ in dit geval np.kron = @	, maar beneden niet	,

In [118]: np.kron(a,a.T)
Out[118]: array([1, 2, 3, 2, 4, 6, 3, 6, 9])

/ 13	. 

/ 1313	. 

In [138]: np.tensordot(v,v.T,0)
Out[138]: 
array([[[[1, 2, 3]]],


       [[[2, 4, 6]]],


       [[[3, 6, 9]]]])
In [139]: np.tensordot(v,v.T,0).shape
Out[139]: (3, 1, 1, 3)


/ 1313	 

In [127]: L=np.tensordot(v,v.T,1)
In [127]: L
Out[127]: 
array([[1, 2, 3],
       [2, 4, 6],
       [3, 6, 9]])

# summed over 
In [142]: np.tensordot(L,L.T,1)
Out[142]: 
array([[ 14,  28,  42],
       [ 28,  56,  84],
       [ 42,  84, 126]])

/ 1313	. 

In [145]: np.tensordot(L,L.T,2)
Out[145]: array(196)


/ 1313	. 

# wordt niet summed over	, 
In [140]: np.tensordot(L,L.T,0)
Out[140]: 
array([[[[ 1,  2,  3],
         [ 2,  4,  6],
         [ 3,  6,  9]],

        [[ 2,  4,  6],
         [ 4,  8, 12],
         [ 6, 12, 18]],

        [[ 3,  6,  9],
         [ 6, 12, 18],
         [ 9, 18, 27]]],


       [[[ 2,  4,  6],
         [ 4,  8, 12],
         [ 6, 12, 18]],

        [[ 4,  8, 12],
         [ 8, 16, 24],
         [12, 24, 36]],

        [[ 6, 12, 18],
         [12, 24, 36],
         [18, 36, 54]]],


       [[[ 3,  6,  9],
         [ 6, 12, 18],
         [ 9, 18, 27]],

        [[ 6, 12, 18],
         [12, 24, 36],
         [18, 36, 54]],

        [[ 9, 18, 27],
         [18, 36, 54],
         [27, 54, 81]]]])

In [141]: np.tensordot(L,L.T,0).shape
Out[141]: (3, 3, 3, 3)

/ 1313	. 

In [143]:  np.kron(L,L.T)
Out[143]: 
array([[ 1,  2,  3,  2,  4,  6,  3,  6,  9],
       [ 2,  4,  6,  4,  8, 12,  6, 12, 18],
       [ 3,  6,  9,  6, 12, 18,  9, 18, 27],
       [ 2,  4,  6,  4,  8, 12,  6, 12, 18],
       [ 4,  8, 12,  8, 16, 24, 12, 24, 36],
       [ 6, 12, 18, 12, 24, 36, 18, 36, 54],
       [ 3,  6,  9,  6, 12, 18,  9, 18, 27],
       [ 6, 12, 18, 12, 24, 36, 18, 36, 54],
       [ 9, 18, 27, 18, 36, 54, 27, 54, 81]])

In [144]: L@L.T
Out[144]: 
array([[ 14,  28,  42],
       [ 28,  56,  84],
       [ 42,  84, 126]])

/ 1313	. 

In [146]: np.tensordot(L,L.T,([1],[0]))
Out[146]: 
array([[ 14,  28,  42],
       [ 28,  56,  84],
       [ 42,  84, 126]])

In [147]: np.tensordot(L,L.T,([0],[1]))
Out[147]: 
array([[ 14,  28,  42],
       [ 28,  56,  84],
       [ 42,  84, 126]])

/ 1313	. 

In [150]: M=np.array([-1,4,5,-2,3,-4,-3,2,-1]).reshape(3,3)

In [151]: M
Out[151]: 
array([[-1,  4,  5],
       [-2,  3, -4],
       [-3,  2, -1]])

# de normale matrix verm	, (2,1) elem in product=2de rij links * 1ste kol rechts	,
In [152]: np.tensordot(M,M,([1],[0]))
Out[152]: 
array([[-22,  18, -26],
       [  8,  -7, -18],
       [  2,  -8, -22]])

# (2,1) elem in product = 2de kol links * 1ste rij rechts
In [153]: np.tensordot(M,M,([0],[1]))
Out[153]: 
array([[-22,   8,   2],
       [ 18,  -7,  -8],
       [-26, -18, -22]])

/ 13	. 

In [53]: M
Out[53]: 
array([[1, 2],
       [3, 4]])

In [57]: np.kron(np.array([1,1]),np.diag(M[:,0]))
Out[57]: 
array([[1, 0, 1, 0],
       [0, 3, 0, 3]])
In [58]: np.kron(np.array([1,1]),np.diag(M[:,1]))
Out[58]: 
array([[2, 0, 2, 0],
       [0, 4, 0, 4]])

In [66]: K=np.r_[np.kron(np.array([1,1]),np.diag(M[:,0])),np.kron(np.array([1,1]),np.diag(M[:,1]))]

In [67]: K
Out[67]: 
array([[1, 0, 1, 0],
       [0, 3, 0, 3],
       [2, 0, 2, 0],
       [0, 4, 0, 4]])

/ deze zochten we	,
In [156]: (np.eye(4)-K)*K.T
Out[156]: 
array([[  0.,   0.,  -2.,   0.],
       [  0.,  -6.,   0., -12.],
       [ -2.,   0.,  -2.,   0.],
       [  0., -12.,   0., -12.]])

/ 13	. 

In [178]: R=np.array([]).reshape(0,4)
In [179]: for i in np.arange(0,2):
     ...:     R=np.r_[R,np.kron(np.ones(2)),np.diag(M[:,i]))]
     ...:     
     ...:     
In [180]: R
Out[180]: 
array([[1., 0., 1., 0.],
       [0., 3., 0., 3.],
       [2., 0., 2., 0.],
       [0., 4., 0., 4.]])
w=w-la.lu_solve(la.lu_factor(H),grad)
In [192]: (np.eye(4)-R)*R.T
Out[192]: 
array([[  0.,   0.,  -2.,   0.],
       [  0.,  -6.,   0., -12.],
       [ -2.,   0.,  -2.,   0.],
       [  0., -12.,   0., -12.]])

/ 7	 .

/ args python script	,

/ als het een integer is	,
int(sys.argv[1]))

[eric@almond my]$ cat args.py 
import sys
if len(sys.argv)!=3:
    print("Give #features #classes\n" ,file=sys.stderr)
    sys.exit(1)

display(sys.argv[0])
display(int(sys.argv[1]))
display(int(sys.argv[2]))
print(sys.argv[2])
display(type(sys.argv))

[eric@almond my]$ ipython args.py 13 7
'/home/eric/Devel/python/my/args.py'
13
7
7
list

[eric@almond my]$ ipython args.py 13 7 7
Give #features #classes

---------------------------------------------------------------------------
SystemExit                                Traceback (most recent call last)
~/Devel/python/my/args.py in <module>()
      2 if len(sys.argv)!=3:
      3     print("Give #features #classes\n" ,file=sys.stderr)
----> 4     sys.exit(1)
      5 
      6 display(sys.argv[0])

SystemExit: 1

/ 13	. 

/ lees https://stackoverflow.com/questions/36926077/how-to-pass-an-array-to-python-through-command-line/36926259

/ lees array en een amount	,

[eric@almond my]$ cat args.py 
import sys
import numpy as np
if len(sys.argv)!=3:
    print("Give features #classes\n" ,file=sys.stderr)
    sys.exit(1)

features=np.array(sys.argv[1].split(',')).astype(int)
nclasses=int(sys.argv[2])

print(len(sys.argv))

print(sys.argv[0])
print(type(features))
print(features)
print(len(features))

print(type(nclasses))
print(int(nclasses))

display(type(sys.argv))

# 3 args: args.py	, 0,1,2,3	, 3
[eric@almond my]$ ipython args.py 0,1,2,3 3
3
/home/eric/Devel/python/my/args.py
<class 'numpy.ndarray'>
[0 1 2 3]
4
<class 'int'>
3
list

/ 13		,

A=np.random.randint(1,100,size=6).reshape(2,3)
w=np.zeros(((NF+1)*NC,1))
for i in np.arange(0,NC-1):
	e=np.zeros((NC,1))
	e[i]=1
	w=w+np.kron(e,A @ e)

/ 7	. 

/ reshape	,

/ zet kolommen in matrix onder elkaar -> 1 vector	,

/ order='C' lees in rijen	, 
/ order='F' lees in cols	,

In [943]: A
Out[943]: 
array([[54, 50, 51],
       [ 8, 32, 47]])

In [960]:  A.reshape(1,-1)
Out[960]: array([[54, 50, 51,  8, 32, 47]])

In [961]:  A.reshape(1,-1,order='c')
Out[961]: array([[54, 50, 51,  8, 32, 47]])

In [962]:  A.reshape(1,-1,order='f')
Out[962]: array([[54,  8, 50, 32, 51, 47]])

In [965]:  A.reshape(-1,1)
Out[965]: 
array([[54],
       [50],
       [51],
       [ 8],
       [32],
       [47]])

In [963]:  A.reshape(-1,1,order='f')
Out[963]: 
array([[54],
       [ 8],
       [50],
       [32],
       [51],
       [47]])

In [964]:  A.reshape(-1,1,order='c')
Out[964]: 
array([[54],
       [50],
       [51],
       [ 8],
       [32],
       [47]])

/ 13	.

In [969]: A
Out[969]: 
array([[54, 50, 51],
       [ 8, 32, 47]])


In [968]: A.T
Out[968]: 
array([[54,  8],
       [50, 32],
       [51, 47]])


In [966]:  A.reshape(-1,1,order='f')
Out[966]: 
array([[54],
       [ 8],
       [50],
       [32],
       [51],
       [47]])
/=
In [967]:  A.T.reshape(-1,1,order='c')
Out[967]: 
array([[54],
       [ 8],
       [50],
       [32],
       [51],
       [47]])

/ 13	. 

/ terug	, 

/ eerst naar vector	,
In [970]: v= A.reshape(-1,1,order='f')

In [971]: v
Out[971]: 
array([[54],
       [ 8],
       [50],
       [32],
       [51],
       [47]])

In [973]: v.reshape(2,-1,order='f')
Out[973]: 
array([[54, 50, 51],
       [ 8, 32, 47]])

/ 13	. 

In [979]: A
Out[979]: 
array([[54, 50, 51],
       [ 8, 32, 47]])

In [978]: A.ravel(order='f')
Out[978]: array([54,  8, 50, 32, 51, 47])

/ 13	. 

/ lees over permutation vector,
https://scicomp.stackexchange.com/questions/3229/permute-a-matrix-in-place-in-numpy
https://stackoverflow.com/questions/41210142/get-all-permutations-of-a-numpy-array
https://stackoverflow.com/questions/104420/how-to-generate-all-permutations-of-a-list?noredirect=1
https://stackoverflow.com/questions/27323448/numpy-array-to-permutation-matrix

https://scicomp.stackexchange.com/questions/3229/permute-a-matrix-in-place-in-numpy:
N memory overhead

for i in range(N):
    M[:,i] = M[p,i]
for i in range(N):
    M[i,:] = M[i,p]

N^2 memory overhead

M[:,:] = M[p,:]
M[:,:] = M[:,p]

/ 7	 

/ vergl 2 arrays,

/ lees,
https://stackoverflow.com/questions/10580676/comparing-two-numpy-arrays-for-equality-element-wise

(A==B).all()
Note: maybe you also want to test A and B shape, such as A.shape == B.shape

np.array_equal(A,B)  # test if same shape, same elements values
np.array_equiv(A,B)  # test if broadcastable shape, same elements values
np.allclose(A,B,...) # test if same shape, elements have close enough values


/ Einde MY

/ GD 

/ 7	. 

/ in lin_reg_self_20210405.py zit de Phi@Phi_plus opl, ...
/ TODO doc	, 

$ vi lin_reg_self_20210405.py

...
import sys
if len(sys.argv)!=5:
    print("Use: lin_reg_self_20210405.py <target> <alpha> <#iterations> <absolute tolerance>\n",file=sys.stderr)
    sys.exit(1)


Phi=np.array([[1,1,0],[0,1,1]]).T
w=np.zeros(Phi.shape[1])#;w[0]=1
t=np.array(sys.argv[1].split(',')).astype(int)#.reshape(-1,1) #(2,)
alpha=float(sys.argv[2])
niter=int(sys.argv[3])
atol=float(sys.argv[4])
cost_prev=1e8
for i in np.arange(0,niter):
    w=w-alpha*Phi.T @ (Phi @ w - t)
    cost=.5*np.square(la.norm(Phi @ w - t))
    print("w=",w," ,cost_prev=",cost_prev," cost=",cost," cost_prev-cost=",cost_prev-cost," i=",i)
    if(cost_prev<cost):
        print("verkeerde richting; stijgende cost\n")
        break
    if(cost_prev-cost<atol):
        print("klaar\n")

/ t=(1,2,1).T	, 
/ t ligt in Im(Phi)

/ t=(1,2,3).T	, 
/ t ligt niet in Im(Phi)
/ , w = Phi+ @ (1,2,3).T = 1/3*(1,7).T
/ Phi @ Phi+ @ (1,2,3).T= 1/3 * (1,8,7).T

/ check dat t-Phi@w loodrecht op ImPhi dus Phi.T @ (t - Phi @ w ) =0
/ TODO

/ oef met verschillende alpha	,

/ alpha=1 is te groot	,
[eric@almond my]$ ipython lin_reg_self_20210405.py 1,2,3 1 100 1e-8
w= [3. 5.]  ,cost_prev= 100000000.0  cost= 22.0  cost_prev-cost= 99999978.0  i= 0
w= [-5. -3.]  ,cost_prev= 22.0  cost= 85.99999999999999  cost_prev-cost= -63.999999999999986  i= 1
verkeerde richting; stijgende cost

/ alpha=.5 is 
[eric@almond my]$ ipython lin_reg_self_20210405.py 1,2,3 .5 100 1e-8
...
w= [0.33332825 2.33329773]  ,cost_prev= 0.6666666725650429  cost= 0.6666666681412607  cost_prev-cost= 4.4237821184012205e-09  i= 15
klaar


[eric@almond my]$ ipython lin_reg_self_20210405.py 1,2,3 .1 100 1e-8
...
w= [0.33352996 2.33313671]  ,cost_prev= 0.6666667143977739  cost= 0.6666667053288637  cost_prev-cost= 9.068910178378076e-09  i= 80
klaar

[eric@almond my]$ ipython lin_reg_self_20210405.py 1,2,1 .5 100 1e-8
...
w= [1.00003052 1.00003052]  ,cost_prev= 1.117587089538574e-08  cost= 2.793967723846435e-09  cost_prev-cost= 8.381903171539305e-09  i= 14
klaar
/ maakt qua aantal iteraties niet uit of t in Im (Phi) of niet	, 

/ docs lin_reg_self_20210405.py
/ TODO


/ 7	. 

[eric@almond my]$ cp log_reg_1_class_1_feature_newton_iris_202102.py log_reg_1_class_1_feature_gd_iris_20210406.py

[eric@almond my]$ cp log_reg_1_class_features_newton_self_202102.py log_reg_1_class_features_gd_self_20210406.py
/ ook	,
-rw-rw-r--. 1 eric eric 1002 Feb 24 21:46 log_reg_1_class_4_features_newton_iris_202102.py
-rw-rw-r--. 1 eric eric 1028 Feb 24 21:57 log_reg_1_class_2_features_newton_iris_202102.py


/ newton,
[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_iris_202102.py 3 2 10
cost= 103.9720770839918
grad= [ 25.  -11.4]
w= [-2.95221619  1.9067432 ]
cost= 48.33896142340537
grad= [ 8.97074108 -0.38681719]
w= [-5.41595373  3.37328912]
cost= 33.32783858145029
grad= [ 3.93730333 -0.14849381]
w= [-9.04645818  5.53966354]
cost= 23.6818712579049
grad= [ 1.34667894 -0.60577289]
w= [-13.70212828   8.36784257]
cost= 18.62183583463461
grad= [ 0.30106532 -0.53613458]
w= [-18.04411003  11.03842708]
cost= 16.97124946930131
grad= [ 0.03908603 -0.231231  ]
w= [-20.54124041  12.58379068]
cost= 16.71880884270708
grad= [-0.00456524 -0.05416769]
w= [-21.10321628  12.9334759 ]
cost= 16.710416392172014
grad= [-0.00072846 -0.0029108 ]
w= [-21.12560604  12.94748582]
cost= 16.710404144814625
grad= [-1.98233959e-06 -5.82996475e-06]
w= [-21.12563996  12.94750716]
cost= 16.71040414478605
grad= [-6.50768328e-12 -1.66400227e-11]
w= [-21.12563996  12.94750716]

/ we zien dat alpha=.11 okay, maar .12 ERR	,	

[eric@almond my]$ ipython log_reg_1_class_1_feature_gd_iris_20210406.py 3 2 .03  10000 1e-10
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
klaar
w= [-21.12420933  12.94662432] , i= 8152 , cost_prev= 16.710404193358727 , cost= 16.7104041932588

[eric@almond my]$ ipython log_reg_1_class_1_feature_gd_iris_20210406.py 3 2 .05  10000 1e-10
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
klaar
w= [-21.12453232  12.94682363] , i= 4981 , cost_prev= 16.71040417394218 , cost= 16.710404173842246

[eric@almond my]$ ipython log_reg_1_class_1_feature_gd_iris_20210406.py 3 2 .06  10000 1e-10
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
klaar
w= [-21.1246295  12.9468836] , i= 4136 , cost_prev= 16.710404169067253 , cost= 16.710404168967393


[eric@almond my]$ ipython log_reg_1_class_1_feature_gd_iris_20210406.py 3 2 .11  10000 1e-10
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
verkeerde richting; stijgende cost
klaar
w= [-21.12638341  12.94796594] , i= 2424 , cost_prev= 16.71040415797424 , cost= 16.71040415787491

[eric@almond my]$ ipython log_reg_1_class_1_feature_gd_iris_20210406.py 3 2 .12  10000 1e-10
...
verkeerde richting; stijgende cost

/ 7	.

/ kijk,
https://www.youtube.com/watch?v=-kwZhTPAhIQ
/ 17:17
/ backtracking line search	,

[eric@almond my]$ vi linsearchbacktrack.py 


def p(x):
    return -(x-.5)*(x-1.5)*(x-3)

def l(x):
    return -(x-1.5)*(x-3)-(x-.5)*(x-3)-(x-.5)*(x-1.5)

def s(x):
    return -2*(x-.5) -2*(x-1.5) -2*(x-3)

# newton om minimum te bepalen,
x0=0
n=0
while n<10:
    x1=x0-l(x0)/s(x0)
    x0=x1
    n=n+1
print("x0=",x0)

x=np.linspace(0,3,101)
plt.plot(x,p(x))
c=.2										# <<<
plt.plot(x,p(0)+c*l(0)*x)

d=3
g=.9
n=0
niter=100
while n<niter:
    t=g**n
    print(t,p(0+t*d),p(0)+c*l(0)*t*d)
    plt.plot(t*d,p(0)+c*l(0)*t*d,'gs')
    if p(0+t*d)<=p(0)+c*l(0)*t*d:
        break
    n=n+1
print("t=",t)
print("0+t*d=",0+t*d)

plt.show()

/ .show() moet onderaan, anders doet hij het alg niet	,

/ je kunt ook g=.8 nemen	, of in .5-.8	,

/ c=.2

/ de lijn snijdt de fct rechts van het minimum	, 
/ ons punt is het 1ste macht van g, die links van dat snijpunt ligt	,
/ en die ligt dus ook rechts van het min,

[eric@almond my]$ ipython linsearchbacktrack.py 
x0= 0.9401835094098877
1.0 -0.0 -1.8000000000000007
0.9 0.7919999999999997 -1.3950000000000005
0.81 1.023093 -1.0305000000000004
0.7290000000000001 0.9422417970000001 -0.7024500000000007
0.6561 0.7094019650130002 -0.40720500000000026
0.5904900000000001 0.4240467379444775 -0.14148450000000068
0.531441 0.14509373862602398 0.09766394999999983
0.4782969000000001 -0.09526832681838343 0.31289755499999927
t= 0.4782969000000001
0+t*d= 1.4348907000000002

/ c=.4

/ vrij goed!

[eric@almond my]$ ipython linsearchbacktrack.py 
x0= 0.9401835094098877
1.0 -0.0 -5.850000000000001
0.9 0.7919999999999997 -5.040000000000001
0.81 1.023093 -4.311000000000001
0.7290000000000001 0.9422417970000001 -3.6549000000000014
0.6561 0.7094019650130002 -3.0644100000000005
0.5904900000000001 0.4240467379444775 -2.5329690000000014
0.531441 0.14509373862602398 -2.0546721000000003
0.4782969000000001 -0.09526832681838343 -1.6242048900000015
0.4304672100000001 -0.282064115742273 -1.2367844010000013
0.3874204890000001 -0.411049212967121 -0.8881059609000008
0.3486784401000001 -0.4843502288802194 -0.5742953648100011
0.31381059609000006 -0.50758067932733 -0.29186582832900054
t= 0.31381059609000006
0+t*d= 0.9414317882700002

/ c=.6

/ gaat er voorbij	,
/ de gekantelde raaklijn loopt te steil, en snijdt de fct links van het minimum,
/ ons punt is het 1ste macht van g, die links van dat snijpunt ligt	,
/ en ligt ook links van het min,

[eric@almond my]$ ipython linsearchbacktrack.py 
x0= 0.9401835094098877
1.0 -0.0 -9.899999999999999
0.9 0.7919999999999997 -8.685
0.81 1.023093 -7.5915
0.7290000000000001 0.9422417970000001 -6.60735
0.6561 0.7094019650130002 -5.721615
0.5904900000000001 0.4240467379444775 -4.9244535
0.531441 0.14509373862602398 -4.20700815
0.4782969000000001 -0.09526832681838343 -3.5613073350000004
0.4304672100000001 -0.282064115742273 -2.980176601500001
0.3874204890000001 -0.411049212967121 -2.4571589413500003
0.3486784401000001 -0.4843502288802194 -1.9864430472150012
0.31381059609000006 -0.50758067932733 -1.5627987424935008
0.2824295364810001 -0.4879749629943449 -1.1815188682441509
0.2541865828329001 -0.43321794045726 -0.8383669814197354
0.2287679245496101 -0.35074403343565497 -0.5295302832777624
0.20589113209464907 -0.24734831887028172 -0.2515772549499862
0.18530201888518416 -0.12900067020401823 -0.001419529454987245
t= 0.18530201888518416
0+t*d= 0.5559060566555525

/ als c=.1, dan  is voor t=1 meteen	,
    if p(0+t*d)<=p(0)+c*t*d*l(0)

/ 7	. 

[eric@almond my]$ cp log_reg_1_class_1_feature_gd_iris_20210406.py log_reg_1_class_1_feature_gd_backtrack_iris_20210406.py

/ maar 100 gd iteraties, om het begin te laten zien	,
[eric@almond my]$ ipython log_reg_1_class_1_feature_gd_backtrack_iris_20210406.py 3 2 100 1e-8 .4 .9 100
n_bt= 0
s= 1.0
w= [-0.90986726  0.41489947]
n_bt= 2
s= 0.81
w= [-1.26459016  1.14309701]
n_bt= 7
s= 0.4782969000000001
w= [-1.68570696  0.91631835]
n_bt= 7
s= 0.4782969000000001
w= [-1.83824504  1.36963949]
n_bt= 9
s= 0.3874204890000001
w= [-2.19863063  1.22745349]
n_bt= 10
s= 0.3486784401000001
...

/ maximaal 10000 gd iteraties,
[eric@almond my]$ ipython log_reg_1_class_1_feature_gd_backtrack_iris_20210406.py 3 2 10000 1e-8 .4 .9 100

n_bt= 94
s= 4.997995805289306e-05
w= [-21.11824421  12.94296364]
n_bt= 93
s= 5.553328672543673e-05
w= [-21.11829819  12.9429506 ]
klaar
w= [-21.11829819  12.9429506 ] , i= 1922 , cost_prev= 16.71040543995145 , cost= 16.710405429958488

/ Einde GD

/ NEWTON

[eric@almond my]$ cp log_reg_1_class_1_feature_newton_iris_202102.py log_reg_1_class_1_feature_newton_backtrack_iris_202102.py

/ we zien bij newton dat s=1 altijd	, 

/ je ziet dat d van links naar rechts gaat rondom het min	,
	--->
<--
	->
 <-
/ en w gaat 1 richting op	 
/ TODO

[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_backtrack_iris_202102.py 3 2 100 1e-8 .4 .9 100
w= [0. 0.]
grad= [ 25.  -11.4]
grad= [ 25.  -11.4]
d= [ 2.95221619 -1.9067432 ]
break
n_bt= 0
s= 1.0
w= [-2.95221619  1.9067432 ]
grad= [ 8.97074108 -0.38681719]
grad= [ 8.97074108 -0.38681719]
d= [ 2.46373755 -1.46654592]
break
n_bt= 0
s= 1.0
w= [-5.41595373  3.37328912]
grad= [ 3.93730333 -0.14849381]
grad= [ 3.93730333 -0.14849381]
d= [ 3.63050444 -2.16637442]
break
n_bt= 0
s= 1.0
w= [-9.04645818  5.53966354]
grad= [ 1.34667894 -0.60577289]
grad= [ 1.34667894 -0.60577289]
d= [ 4.6556701  -2.82817902]
break
n_bt= 0
s= 1.0
w= [-13.70212828   8.36784257]
grad= [ 0.30106532 -0.53613458]
grad= [ 0.30106532 -0.53613458]
d= [ 4.34198174 -2.67058451]
break
n_bt= 0
s= 1.0
w= [-18.04411003  11.03842708]
grad= [ 0.03908603 -0.231231  ]
grad= [ 0.03908603 -0.231231  ]
d= [ 2.49713039 -1.5453636 ]
break
n_bt= 0
s= 1.0
w= [-20.54124041  12.58379068]
grad= [-0.00456524 -0.05416769]
grad= [-0.00456524 -0.05416769]
d= [ 0.56197587 -0.34968523]
break
n_bt= 0
s= 1.0
w= [-21.10321628  12.9334759 ]
grad= [-0.00072846 -0.0029108 ]
grad= [-0.00072846 -0.0029108 ]
d= [ 0.02238976 -0.01400991]
break
n_bt= 0
s= 1.0
w= [-21.12560604  12.94748582]
grad= [-1.98233959e-06 -5.82996475e-06]
grad= [-1.98233959e-06 -5.82996475e-06]
d= [ 3.39225916e-05 -2.13408485e-05]
break
n_bt= 0
s= 1.0
klaar
w= [-21.12563996  12.94750716] , i= 8 , cost_prev= 16.710404144814625 , cost= 16.71040414478605


[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_backtrack_iris_202102.py 1 2 100 1e-8 .4 .9 100  
w= [0. 0.]
grad= [25.   80.35]
grad= [25.   80.35]
d= [-1.07769123  0.57117154]
break
n_bt= 0
s= 1.0
w= [ 1.07769123 -0.57117154]
grad= [1.21638118 4.21209334]
grad= [1.21638118 4.21209334]
d= [-0.23977809  0.09150389]
break
n_bt= 0
s= 1.0
w= [ 1.31746932 -0.66267543]
grad= [0.02019702 0.07314543]
grad= [0.02019702 0.07314543]
d= [-0.0058669   0.00215207]
break
n_bt= 0
s= 1.0
w= [ 1.32333622 -0.66482751]
grad= [8.76873797e-06 3.20340852e-05]
grad= [8.76873797e-06 3.20340852e-05]
d= [-2.69573457e-06  9.83737916e-07]
break
n_bt= 0
s= 1.0
klaar
w= [ 1.32333892 -0.66482849] , i= 3 , cost_prev= 94.16658045045467 , cost= 94.16658045045074

/ 13	. 

[eric@almond my]$ cp log_reg_1_class_1_feature_newton_backtrack_iris_202102.py log_reg_1_class_1_feature_newton_backtrack_self_202102.py

/ eerder,
[eric@almond my]$ ipython log_reg_1_class_features_self_geron_20210315.py -- -2,2 0,1
[LibLinear]intercept_
[0.]
coef_
[[5.60143377]]

[eric@almond my]$ ipython log_reg_1_class_features_self_geron_20210315.py -- -1,0,1 0,1,1
[LibLinear]intercept_
[10.62752168]
coef_
[[21.77784361]]

/////////////////////////////////////////
[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_backtrack_self_202102.py -- -1,0,1 0,1,1 100 1e-8 .4 .9 100
w= [12.07132684 24.66545232] , i= 15 , cost_prev= 9.113861197352576e-06 , cost= 9.113089817422816e-06

[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_backtrack_self_202102.py -- -2,2 0,1 100 1e-8 .4 .9 100
w= [1.12148518e-16 1.01014474e+01] , i= 18 , cost_prev= 9.147844280792127e-09 , cost= 3.365303927249901e-09

////////////////////////////////////
/ redelijk	,

/ we zien hieronder dat op een gegeven moment, 
w= [10.6275953  21.77800071]
/ dit is vrijwel wat geron geeft	,
/ maar het alg gaat door,	
/ dan zien we 	,
/ TODO



[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_backtrack_self_202102.py -- -1,0,1 0,1,1 100 1e-8 .4 .9 100
w= [0. 0.]
grad= [-0.5 -1. ]
grad= [-0.5 -1. ]
d= [-0.66666667 -2.        ]
break
n_bt= 0
s= 1.0
w= [0.66666667 2.        ]
grad= [-0.19560427 -0.2735777 ]
grad= [-0.19560427 -0.2735777 ]
d= [-0.80142645 -1.58165998]
break
n_bt= 0
s= 1.0
w= [1.46809311 3.58165998]
grad= [-0.08581753 -0.11415525]
grad= [-0.08581753 -0.11415525]
d= [-1.05651538 -2.0397714 ]
break
n_bt= 0
s= 1.0
w= [2.52460849 5.62143138]
grad= [-0.03120224 -0.0435283 ]
grad= [-0.03120224 -0.0435283 ]
d= [-1.06191294 -2.09202317]
break
n_bt= 0
s= 1.0
w= [3.58652143 7.71345456]
grad= [-0.0110844  -0.01588853]
grad= [-0.0110844  -0.01588853]
d= [-1.02574483 -2.04024104]
break
n_bt= 0
s= 1.0
w= [4.61226626 9.7536956 ]
grad= [-0.00401694 -0.00581588]
grad= [-0.00401694 -0.00581588]
d= [-1.00968937 -2.0153367 ]
break
n_bt= 0
s= 1.0
w= [ 5.62195563 11.7690323 ]
grad= [-0.00146939 -0.00213519]
grad= [-0.00146939 -0.00213519]
d= [-1.00358623 -2.00569955]
break
n_bt= 0
s= 1.0
w= [ 6.62554186 13.77473185]
grad= [-0.00053942 -0.00078488]
grad= [-0.00053942 -0.00078488]
d= [-1.00132188 -2.00210385]
break
n_bt= 0
s= 1.0
w= [ 7.62686374 15.7768357 ]
grad= [-0.00019829 -0.00028866]
grad= [-0.00019829 -0.00028866]
d= [-1.00048662 -2.00077489]
break
n_bt= 0
s= 1.0
w= [ 8.62735036 17.77761059]
grad= [-7.29256993e-05 -1.06180893e-04]
grad= [-7.29256993e-05 -1.06180893e-04]
d= [-1.00017906 -2.00028519]
break
n_bt= 0
s= 1.0
w= [ 9.62752942 19.77789578]
grad= [-2.68250507e-05 -3.90602429e-05]
grad= [-2.68250507e-05 -3.90602429e-05]
d= [-1.00006588 -2.00010493]
break
n_bt= 0
s= 1.0
w= [10.6275953  21.77800071]
grad= [-9.86800370e-06 -1.43692539e-05]
grad= [-9.86800370e-06 -1.43692539e-05]
d= [-1.00002424 -2.0000386 ]
break
n_bt= 0
s= 1.0
w= [11.62761954 23.77803932]
grad= [-3.63018413e-06 -5.28612516e-06]
grad= [-3.63018413e-06 -5.28612516e-06]
d= [-1.00000892 -2.0000142 ]
log_reg_1_class_1_feature_newton_backtrack_self_202102.py:34: RuntimeWarning: divide by zero encountered in log
  return np.ones(Phi.shape[0])@(t*-np.log(p1)+(1-t)*-np.log(1-p1))
log_reg_1_class_1_feature_newton_backtrack_self_202102.py:34: RuntimeWarning: invalid value encountered in multiply
  return np.ones(Phi.shape[0])@(t*-np.log(p1)+(1-t)*-np.log(1-p1))
break
n_bt= 8
s= 0.4304672100000001
w= [12.05809059 24.63897985]
grad= [-2.36036205e-06 -3.43706482e-06]
grad= [-2.36036205e-06 -3.43706482e-06]
d= [-1.0000058  -2.00000923]
break
n_bt= 42
s= 0.011972515182562033
w= [12.07006317 24.66292499]
grad= [-2.33227107e-06 -3.39615986e-06]
grad= [-2.33227107e-06 -3.39615986e-06]
d= [-1.00000573 -2.00000912]
break
n_bt= 64
s= 0.0011790184577738603
w= [12.0712422  24.66528304]
grad= [-2.32952290e-06 -3.39215809e-06]
grad= [-2.32952290e-06 -3.39215809e-06]
d= [-1.00000572 -2.00000911]
break
n_bt= 89
s= 8.464149782874062e-05
klaar
w= [12.07132684 24.66545232] , i= 15 , cost_prev= 9.113861197352576e-06 , cost= 9.113089817422816e-06


/ 7	. 

[eric@almond my]$ ipython num_args.py 1,2,3
2
[eric@almond my]$ ipython num_args.py -- -1,0,1
2


/  7	.

/ Wolfe weak 

/ lees	,
https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf


(1.5) c2*f'(xk)*dk <= f'(xk+tk*dk)*dk: c2<1, en f' <0, dus f'(xk+tk*dk) is klein negatief, omdat c2*f'(xk) dat ook is, door c2	, dus is f'(xk+tk*dk) ongeveer 0	,

(1.7) |f'(xk+tk*dk)*dk| < c2*|f'(xk)*dk|: c2<1, dus is |f'(xk+tk*dk)*dk|  klein, ongeveer 0	, 

/ 13	. 

eric@almond my]$ cp log_reg_1_class_1_feature_newton_backtrack_iris_202102.py log_reg_1_class_1_feature_newton_backtrack_wolfe_weak_iris_202102.py


[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_backtrack_wolfe_weak_iris_202102.py 3 2 100 1e-8 .4 .9 .9 100
w= [0. 0.]
d= [-2.95221619  1.9067432 ]
break
n_bt= 0
s= 1.0
w= [-2.95221619  1.9067432 ]
d= [-2.46373755  1.46654592]
break
n_bt= 0
s= 1.0
w= [-5.41595373  3.37328912]
d= [-3.63050444  2.16637442]
break
n_bt= 0
s= 1.0
w= [-9.04645818  5.53966354]
d= [-4.6556701   2.82817902]
break
n_bt= 0
s= 1.0
w= [-13.70212828   8.36784257]
d= [-4.34198174  2.67058451]
break
n_bt= 0
s= 1.0
w= [-18.04411003  11.03842708]
d= [-2.49713039  1.5453636 ]
break
n_bt= 0
s= 1.0
w= [-20.54124041  12.58379068]
d= [-0.56197587  0.34968523]
break
n_bt= 0
s= 1.0
w= [-21.10321628  12.9334759 ]
d= [-0.02238976  0.01400991]
break
n_bt= 0
s= 1.0
w= [-21.12560604  12.94748582]
d= [-3.39225916e-05  2.13408485e-05]
break
n_bt= 0
s= 1.0
klaar
w= [-21.12563996  12.94750716] , i= 8 , cost_prev= 16.710404144814625 , cost= 16.71040414478605



/ 7	. 

/ Wolfe strong	,

/ lees	,
https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf

[eric@almond my]$ cp log_reg_1_class_1_feature_newton_backtrack_wolfe_weak_iris_202102.py log_reg_1_class_1_feature_newton_backtrack_wolfe_strong_iris_202102.py

[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_backtrack_wolfe_strong_iris_202102.py 3 2 100 1e-8 .4 .9 .9 100
w= [0. 0.]
d= [-2.95221619  1.9067432 ]
break
n_bt= 0
s= 1.0
w= [-2.95221619  1.9067432 ]
d= [-2.46373755  1.46654592]
break
n_bt= 0
s= 1.0
w= [-5.41595373  3.37328912]
d= [-3.63050444  2.16637442]
break
n_bt= 0
s= 1.0
w= [-9.04645818  5.53966354]
d= [-4.6556701   2.82817902]
break
n_bt= 0
s= 1.0
w= [-13.70212828   8.36784257]
d= [-4.34198174  2.67058451]
break
n_bt= 0
s= 1.0
w= [-18.04411003  11.03842708]
d= [-2.49713039  1.5453636 ]
break
n_bt= 0
s= 1.0
w= [-20.54124041  12.58379068]
d= [-0.56197587  0.34968523]
break
n_bt= 0
s= 1.0
w= [-21.10321628  12.9334759 ]
d= [-0.02238976  0.01400991]
break
n_bt= 0
s= 1.0
w= [-21.12560604  12.94748582]
d= [-3.39225916e-05  2.13408485e-05]
break
n_bt= 0
s= 1.0
klaar
w= [-21.12563996  12.94750716] , i= 8 , cost_prev= 16.710404144814625 , cost= 16.71040414478605

/ 7	. 

[eric@almond my]$ cp log_reg_1_class_1_feature_newton_backtrack_wolfe_weak_iris_202102.py log_reg_1_class_1_feature_newton_backtrack_wolfe_weak_self_202102.py


/ we zien problemen met 

[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_backtrack_wolfe_weak_self_202102.py -- -1,0,1 0,1,1  100 1e-8 .4 .9 .9 100
w= [0. 0.]
d= [0.66666667 2.        ]
break
n_bt= 0
s= 1.0
cost_prev= 2.0794415416798357 , cost= 0.715508387972765 , diff= 1.363933153707071
w= [0.66666667 2.        ]
d= [0.80142645 1.58165998]
break
n_bt= 0
s= 1.0
cost_prev= 0.715508387972765 , cost= 0.32774912883911833 , diff= 0.3877592591336466
w= [1.46809311 3.58165998]
d= [1.05651538 2.0397714 ]
break
n_bt= 0
s= 1.0
cost_prev= 0.32774912883911833 , cost= 0.1215350302252257 , diff= 0.20621409861389262
w= [2.52460849 5.62143138]
d= [1.06191294 2.09202317]
break
n_bt= 0
s= 1.0
cost_prev= 0.1215350302252257 , cost= 0.043333853533934215 , diff= 0.07820117669129148
w= [3.58652143 7.71345456]
d= [1.02574483 2.04024104]
break
n_bt= 0
s= 1.0
cost_prev= 0.043333853533934215 , cost= 0.015713177828359612 , diff= 0.027620675705574603
w= [4.61226626 9.7536956 ]
d= [1.00968937 2.0153367 ]
break
n_bt= 0
s= 1.0
cost_prev= 0.015713177828359612 , cost= 0.005748501756676663 , diff= 0.00996467607168295
w= [ 5.62195563 11.7690323 ]
d= [1.00358623 2.00569955]
break
n_bt= 0
s= 1.0
cost_prev= 0.005748501756676663 , cost= 0.0021103763789105286 , diff= 0.003638125377766134
w= [ 6.62554186 13.77473185]
d= [1.00132188 2.00210385]
break
n_bt= 0
s= 1.0
cost_prev= 0.0021103763789105286 , cost= 0.0007757696189133114 , diff= 0.0013346067599972172
w= [ 7.62686374 15.7768357 ]
d= [1.00048662 2.00077489]
break
n_bt= 0
s= 1.0
cost_prev= 0.0007757696189133114 , cost= 0.00028530915690488 , diff= 0.0004904604620084314
w= [ 8.62735036 17.77761059]
d= [1.00017906 2.00028519]
break
n_bt= 0
s= 1.0
cost_prev= 0.00028530915690488 , cost= 0.00010494846955778317 , diff= 0.00018036068734709684
w= [ 9.62752942 19.77789578]
d= [1.00006588 2.00010493]
break
n_bt= 0
s= 1.0
cost_prev= 0.00010494846955778317 , cost= 3.860690847967536e-05 , diff= 6.634156107810781e-05
w= [10.6275953  21.77800071]
d= [1.00002424 2.0000386 ]
break
n_bt= 0
s= 1.0
cost_prev= 3.860690847967536e-05 , cost= 1.4202488171736918e-05 , diff= 2.4404420307938443e-05
w= [11.62761954 23.77803932]
d= [1.00000892 2.0000142 ]
log_reg_1_class_1_feature_newton_backtrack_wolfe_weak_self_202102.py:35: RuntimeWarning: divide by zero encountered in log
  return np.ones(Phi.shape[0])@(t*-np.log(p1)+(1-t)*-np.log(1-p1))
log_reg_1_class_1_feature_newton_backtrack_wolfe_weak_self_202102.py:35: RuntimeWarning: invalid value encountered in multiply
  return np.ones(Phi.shape[0])@(t*-np.log(p1)+(1-t)*-np.log(1-p1))
break
n_bt= 8
s= 0.4304672100000001
cost_prev= 1.4202488171736918e-05 , cost= 9.234514395860822e-06 , diff= 4.967973775876096e-06
w= [12.05809059 24.63897985]
d= [1.0000058  2.00000923]
n_bt= 100
s= 2.9512665430652825e-05
cost_prev= 9.234514395860822e-06 , cost= 9.234241863966604e-06 , diff= 2.7253189421798754e-10
klaar
w= [12.0581201  24.63903888] , i= 13 , cost_prev= 9.234514395860822e-06 , cost= 9.234241863966604e-06


[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_backtrack_wolfe_weak_self_202102.py -- -1,0,1 0,1,1  100 1e-4 .4 .9 .9 100
...
break
n_bt= 0
s= 1.0
cost_prev= 0.00010494846955778317 , cost= 3.860690847967536e-05 , diff= 6.634156107810781e-05
klaar
w= [10.6275953  21.77800071] , i= 10 , cost_prev= 0.00010494846955778317 , cost= 3.860690847967536e-05


/ Einde NEWTON

/ SKLEARN 

/ 7	. 

/ 13	. 

/ prg	,

phi=np.array([-2,2]).reshape(-1,1)
t=np.array([0,1])
lg=LogisticRegression(solver="newton-cg",C=10**10) 

lg.fit(phi,t)

print(lg.intercept_)
print(lg.coef_)


/ 13	. 

[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_backtrack_wolfe_weak_self_202102.py -- -2,2 0,1    100 1e-4 .4 .9 .9 100  
p1= [0.5 0.5]
t= [0 1]
cost3= 1.3862943611198906
w= [0. 0.]
d= [-0.  1.]
p1= [0.11920292 0.88079708]
t= [0 1]
cost3= 0.25385602208594515
p1= [0.5 0.5]
t= [0 1]
cost3= 1.3862943611198906
break
...

[eric@almond my]$ ipython log_reg_1_class_1_feature_newton_backtrack_wolfe_strong_self_202102.py -- -2,2 0,1    100 1e-4 .4 .9 .9 100  
p1= [0.5 0.5]
t= [0 1]
cost3= 1.3862943611198906
w= [0. 0.]
d= [-0.  1.]
p1= [0.11920292 0.88079708]
t= [0 1]
cost3= 0.25385602208594515
p1= [0.5 0.5]
t= [0 1]
cost3= 1.3862943611198906
break
...

/ 13	. 

/ in sklearn,

optimize.newton_cg
/=
ef newton_cg(grad_hess, func, grad, x0, args=(), tol=1e-4,
              maxiter=100, maxinner=200, line_search=True, warn=True):
...
       xsupi = _cg(fhess_p, fgrad, maxiter=maxinner, tol=termcond)
xsupi=(1,0).T
/ wij hebben (0,.5).T
/ klopt, zij hebben de intercept_ onderaan	,
/ TODO verschil 1 en .5
/ dus zij zeggen in onze volgorde : (0,1).T
/ grad=Phi.T @ (y-t)	, y=sgm(Phi@w)	,
Phi=(1 -2
		1 2)
t=(0
	1)
w=(0
	0)
y=(.5
	.5)
y-t=(.5
	-.5)
Phi.T=(1 1
			-2 2)
grad=(0
		-2)
H=Phi.T @ R @ Phi=
(1 1  @ (.5 0		@ (1 -2
 -2 2)		0 .5)		(1 2)
=(0
	.5)

        if line_search:
            try:
                alphak, fc, gc, old_fval, old_old_fval, gfkp1 = \
                    _line_search_wolfe12(func, grad, xk, xsupi, fgrad,
                                         old_fval, old_old_fval, args=args)
/s,
linesearch.line_search_wolfe1
		...
    stp, fval, old_fval = scalar_search_wolfe1(
            phi, derphi, old_fval, old_old_fval, derphi0,
            c1=c1, c2=c2, amax=amax, amin=amin, xtol=xtol)
/s,
linesearch.scalar_search_wolfe1
    """
    Scalar function search for alpha that satisfies strong Wolfe conditions

/ 7	 .

[eric@almond my]$ ipython log_reg_1_class_features_self_geron_2_20210315.py -- -2,0,2 0,0,1 1e-4
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished
intercept_
[-9.31656843]
coef_
[[9.68514938]]
[eric@almond my]$ ipython log_reg_1_class_features_self_geron_2_20210315.py -- -2,2 0,1 1e-8
/home/eric/miniconda3/lib/python3.6/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.
  "number of iterations.", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished
intercept_
[0.]
coef_
[[7.10137387]]

/ Dus bij tol=1e-8 krijg je een warning, 
/ TODO

[eric@almond my]$ ipython log_reg_1_class_features_self_geron_2_20210315.py -- -2,0,2 0,0,1 1e-4
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished
intercept_
[-9.31656843]
coef_
[[9.68514938]]

[eric@almond my]$ ipython log_reg_1_class_features_self_geron_2_20210315.py -- -2,0,2 0,0,1 1e-8
/home/eric/miniconda3/lib/python3.6/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.
  "number of iterations.", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished
intercept_
[-12.31662785]
coef_
[[12.68513229]]






/ Einde SKLEARN 
