/ See NEWTON NAAR ONEINDIG 



/ 13

/ NEWTON LOGISTIC REGRESSION

/ 1313	. 

/ newton, 
/ sigmoid,
/ -log
/ = cost	, dus zoek min cost

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([-45,5,10]).reshape(-1,1)
beta=np.array([0,0,0]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
resML=np.zeros((10,3+3+3+1+1+3))
for i in range(0,10):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*-np.log(p1gX)+(1-t)*-np.log(1-p1gX)
	ml=np.ones((1,mlv.shape[0])).dot(mlv)
  grad=X.T.dot(p1gX-t)
  D=np.diag((p1gX*(1-p1gX)).ravel())
  H=X.T.dot(D).dot(X)
	nwt=-np.linalg.inv(H).dot(grad)
	resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
  beta=beta+nwt

In [13307]: resML[:,9]
Out[13307]: 
array([103.97207708,  48.1034082 ,  33.56702215,  22.92609003,
        15.70037066,  12.12853775,  10.67459324,  10.3132056 ,
        10.28207149,  10.28175409])

In [7638]: theta
Out[7638]:
array([[-4.27700077],
       [ 0.04030967],
       [ 2.53398464]])

/ als lmbda=.1  ,
In [7644]: theta
Out[7644]:
array([[-11.50605939],
       [  0.74893214],
       [  4.79308489]])

/ als lmbda=.01
In [7651]: theta
Out[7651]:
array([[-24.86777444],
       [  2.70155942],
       [  7.10675456]])


/ 1313	. 

/ newton	,
/ sigmoid	,
/ log
/ = ml	, dus zoek max ml	, 

beta=np.array([-45,5,10]).reshape(-1,1) # singl
beta=np.array([-46,5,10]).reshape(-1,1) # singl
beta=np.array([-46,6,10]).reshape(-1,1) # ok	, 
beta=np.array([-46,5,11]).reshape(-1,1) # singl	, 
beta=np.array([-45,6,11]).reshape(-1,1) # ok, 
beta=np.array([-45,4,11]).reshape(-1,1) # singl, 
beta=np.array([-45,7,10]).reshape(-1,1) # singl, 

/ alpha=.4 OK	, 
/ alpha=.5 ERR	, 

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([-45,5,10]).reshape(-1,1)	 # singl
beta=np.array([0,0,0]).reshape(-1,1)		# ok	,
beta=np.array([-45,5,10]).reshape(-1,1)	 # singl
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
resML=np.zeros((20,3+3+3+1+1+3))
alpha=.4
for i in range(0,20):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX) 	# log ipv -log
	ml=np.ones((1,mlv.shape[0])).dot(mlv)
  grad=X.T.dot(t-p1gX)									# t-p1gX ipv p1gX-t 
  D=np.diag((-p1gX*(1-p1gX)).ravel())		# -p1gX*(1-p1gX) ipv p1gX*(1-p1gX)
  H=X.T.dot(D).dot(X)
	nwt=-alpha*np.linalg.inv(H).dot(grad)
	resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
  beta=beta+nwt

/ bij alpha=.4	,

In [13547]: beta
Out[13547]: 
array([[-45.19638984],
       [  5.744688  ],
       [ 10.42942934]])



/ 1313	. 

/ newton	,
/ sigmoid	,
/ regularization	,
/ -log
/ lmbda >0
/ TODO

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([-45,5,10]).reshape(-1,1)
beta=np.array([0,0,0]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
resML=np.zeros((10,3+3+3+1+1+3))
lmbda=1
i=0
for i in range(0,10):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*-np.log(p1gX)+(1-t)*-np.log(1-p1gX)
	ml=np.ones((1,mlv.shape[0])).dot(mlv)+lmbda*beta.T.dot(beta)
  grad=X.T.dot(p1gX-t)+lmbda*beta
  D=np.diag((p1gX*(1-p1gX)).ravel())
  H=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
	nwt=-np.linalg.inv(H).dot(grad)
	resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
  beta=beta+nwt

m=X.dot(beta)
p1gX=expit(m)
mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)
ml=np.ones((1,mlv.shape[0])).dot(mlv)+lmbda*beta.T.dot(beta)
grad=X.T.dot(t-p1gX)+lmbda*beta
D=np.diag((-p1gX*(1-p1gX)).ravel())
H=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
nwt=-np.linalg.inv(H).dot(grad)
resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
beta=beta+nwt
i=i+1

/ 1313	. 

/ newton	,
/ sigmoid	,
/ regularization	,
/ log
/ lmbda <0
/ TODO

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([-45,5,10]).reshape(-1,1)
beta=np.array([0,0,0]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
resML=np.zeros((10,3+3+3+1+1+3))
lmbda=-1
i=0
for i in range(0,10):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)
	ml=np.ones((1,mlv.shape[0])).dot(mlv)+lmbda*beta.T.dot(beta)
  grad=X.T.dot(t-p1gX)+lmbda*beta
  D=np.diag((-p1gX*(1-p1gX)).ravel())
  H=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
	nwt=-np.linalg.inv(H).dot(grad)
	resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
  beta=beta+nwt

m=X.dot(beta)
p1gX=expit(m)
mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)
ml=np.ones((1,mlv.shape[0])).dot(mlv)+lmbda*beta.T.dot(beta)
grad=X.T.dot(t-p1gX)+lmbda*beta
D=np.diag((-p1gX*(1-p1gX)).ravel())
H=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
nwt=-np.linalg.inv(H).dot(grad)
resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
beta=beta+nwt
i=i+1

/ 13	. 

/ lees	,
https://stackoverflow.com/questions/32308335/how-to-handle-the-divide-by-zero-exception-in-list-comprehensions-while-dividing

def add_handler(handler, exc, func):
    def wrapper(*args, **kwargs):

        try:
            return func(*args, **kwargs)
        except exc:
            return handler(*args, **kwargs)    # ???
    return wrapper

/ 13	. 

X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
a=-21.12563996
b=12.94750716
beta=np.array([a,b]).reshape(-1,1)
beta=np.array([[a+0.8510062],[b-0.52515564]]).reshape(-1,1)
beta=np.array([[a+0.52515564],[b+0.8510062]]).reshape(-1,1) 
beta=np.array([[a+2*0.52515564],[b+2*0.8510062]]).reshape(-1,1)  
# singular hessian als λ=0, niet singular als λ=-1
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=0
for i in range(0,10):
  total()

[[-20.07532868]
 [ 14.64951956]]
[[-21.0024308 ]
 [-31.05819707]]
[[ -6.39444339  -8.77809057]
 [ -8.77809057 -12.11921016]]
[[ 41.06556592]
 [-32.30701084]]
[[1.63424829e-13]
 [2.27373675e-13]]
[ -0.02383632 -18.48981723]
[[ 0.80932593  0.5873598 ]
 [-0.5873598   0.80932593]]
[[ 20.99023724]
 [-17.65749128]]
--------------------
[[ 20.99023724]
 [-17.65749128]]
[[-13.33164173]
 [ 74.48848531]]
[[-3.53652642 -4.34723503]
 [-4.34723503 -5.38073374]]
[[-3027.47155128]
 [ 2459.81673196]]
[[ 9.45021839e-13]
 [-9.52127266e-13]]
[-0.01467603 -8.90258413]
[[ 0.7770123   0.62948542]
 [-0.62948542  0.7770123 ]]
[[-3006.48131405]
 [ 2442.15924068]]
--------------------
/home/eric/miniconda3/bin/ipython:8: RuntimeWarning: divide by zero encountered in log


////////////////////////////////////////////////////////////

/ 7	. 

/ NEWTON NAAR ONEINDIG 

/ 1 feature	,

/ 13

/ in p	,

/ ontbind een vector op evc's ,

In [14460]: Hp=_hessian_ml_to_beta(p)
In [14593]: Ep=np.linalg.eig(Hp)
In [14595]: evap=Ep[0]
In [14597]: Lp=np.diag(evap)
In [14598]: Lp
Out[14598]: 
array([[ -0.0343014 ,   0.        ],
       [  0.        , -17.32006772]])
In [14621]: Vp=Ep[1]
In [14622]: Vp
Out[14622]: 
array([[ 0.8510062 ,  0.52515564],
       [-0.52515564,  0.8510062 ]])

In [14623]: e1=np.array([[1],[0]])
In [14624]: e2=np.array([[0],[1]])

In [14626]: evcp1=Vp.dot(e1)
In [14627]: evcp2=Vp.dot(e2)

In [14631]: Vinvp=np.linalg.inv(Vp)

/ check	,
In [14632]: Vp.dot(Lp).dot(Vinvp)
Out[14632]: 
array([[ -4.8015161 ,  -7.72519407],
       [ -7.72519407, -12.55285302]])
In [14634]: Hp
Out[14634]: 
array([[ -4.8015161 ,  -7.72519407],
       [ -7.72519407, -12.55285302]])

/ evcp1 en evcp2 basic eigenvectors,
/ check	,
In [14640]: Vinvp.dot(evcp1)
Out[14640]: 
array([[1.],
       [0.]])
In [14641]: Vinvp.dot(evcp2)
Out[14641]: 
array([[5.55111512e-17],
       [1.00000000e+00]])

/ wat is effect Hp op beide eigenvectors?
/ we geven coord van effect in basic eigenvectors	,

39b Λ
207b ⁻
b9 ¹ 
/ omdat HV=VΛ is H=VΛV⁻¹ , dus geeft ΛV⁻¹x de coord van Hx tov de eigenvectoren,

In [14642]:  Lp.dot(Vinvp).dot(evcp2)
Out[14642]: 
array([[-5.20417043e-18],
       [-1.73200677e+01]])

In [14643]:  Lp.dot(Vinvp).dot(evcp1)
Out[14643]: 
array([[-0.0343014],
       [ 0.       ]])

In [14650]:  Lp.dot(Vinvp).dot(evcp2*2)
Out[14650]: 
array([[-1.04083409e-17],
       [-3.46401354e+01]])
/ klopt, we zien 2 keer de 2de eigenwaarde	,  

/ in q	,

In [14659]: q=p+2*evcp2
In [14660]: q
Out[14660]: 
array([[-20.07532867],
       [ 14.64951956]])

In [14661]: Hq=_hessian_ml_to_beta(q)
In [14662]: Eq=np.linalg.eig(Hq)
In [14663]: evaq=Eq[0]
In [14664]: Lq=np.diag(evaq)
In [14665]: Lq
Out[14665]: 
array([[ -0.02383632,   0.        ],
       [  0.        , -18.48981723]])

In [14666]: Vq=Eq[1]
In [14667]: Vq
Out[14667]: 
array([[ 0.80932593,  0.5873598 ],
       [-0.5873598 ,  0.80932593]])

In [14668]: evcq1=Vq.dot(e1)
In [14669]: evcq2=Vq.dot(e2)

In [14670]: Vinvq=np.linalg.inv(Vq)

In [14671]: Vq.dot(Lq).dot(Vinvq)
Out[14671]: 
array([[ -6.39444339,  -8.77809057],
       [ -8.77809057, -12.11921016]])
/=
In [14672]: Hq
Out[14672]: 
array([[ -6.39444339,  -8.77809057],
       [ -8.77809057, -12.11921016]])

In [14673]: Vinvq.dot(evcq1)
Out[14673]: 
array([[ 1.00000000e+00],
       [-5.55111512e-17]])
In [14674]: Vinvq.dot(evcq2)
Out[14674]: 
array([[0.],
       [1.]])

39b Λ
207b ⁻
b9 ¹ 
/ omdat HV=VΛ is H=VΛV⁻¹ , dus geeft ΛV⁻¹x de coord van Hx tov de eigenvectoren,

/ coord tov basic eigenvectoren	,
In [14675]: Lq.dot(Vinvq).dot(evcq1)
Out[14675]: 
array([[-2.38363201e-02],
       [ 1.77635684e-15]])
In [14676]: Lq.dot(Vinvq).dot(evcq2)
Out[14676]: 
array([[  0.        ],
       [-18.48981723]])
In [14677]: evaq
Out[14677]: array([ -0.02383632, -18.48981723])

/ Hoe is gradq tov basis eigenvectoren van Hq?
In [14695]: Vinvq.dot(gradq)
Out[14695]: 
array([[  1.24452447],
       [-37.47218791]])

/ Hq⁻¹(-gradq) tov V	,
In [14697]: np.linalg.inv(Lq).dot(Vinvq).dot(-gradq)
Out[14697]: 
array([[52.21126693],
       [-2.02663917]])

/ Hq⁻¹(-gradq) tov standaard basis,
In [14768]: sq=Vq.dot(np.linalg.inv(Lq)).dot(Vinvq).dot(-gradq)
/=
In [14769]: 52.21126693*evcq1+-2.02663917*evcq2
Out[14768]: 
array([[ 41.06556597],
       [-32.30701089]])

/ check
In [14773]: Hq.dot(sq)
Out[14773]: 
array([[21.00243083],
       [31.0581971 ]])

In [14779]: nsq=sq/np.linalg.norm(sq)
In [14780]: nsq
Out[14780]: 
array([[ 0.78593504],
       [-0.61830907]])

In [14793]: Hq.dot(52.21126693*evcq1)
Out[14793]: 
array([[-1.00722593],
       [ 0.73098364]])
In [14794]: Hq.dot(-2.02663917*evcq2)
Out[14794]: 
array([[22.00965672],
       [30.32721341]])
In [14795]: Hq.dot(52.21126693*evcq1)+Hq.dot(-2.02663917*evcq2)
Out[14795]: 
array([[21.00243079],
       [31.05819706]])

/ we plot	,
/ lees,
https://stackoverflow.com/questions/35363444/plotting-lines-connecting-points

/ we plot vanuit q gezien	,
In [14752]: plt.figure()
In [14754]: plt.gca().set_aspect('equal')
# -(q-p)	,
In [14753]: plt.plot([0,1.05031129],[0,1.7020124])
# evcq1
In [14743]: plt.plot([0,0.80932593],[0,-0.5873598])
# evcq2
In [14744]: plt.plot([0,0.5873598],[0,0.80932593])

# voor de plot	,
In [14779]: nsq=sq/np.linalg.norm(sq)
In [14780]: nsq
Out[14780]: 
array([[ 0.78593504],
       [-0.61830907]])
In [14781]: plt.plot([0,0.78593504],[0,-0.61830907])
Out[14781]: [<matplotlib.lines.Line2D at 0x7fbec4c1a048>]

In [14787]: Hq.dot(nsq)
Out[14787]: 
array([[0.4019559],
       [0.5944086]])
/=
In [14785]: ngradq=gradq/np.linalg.norm(sq)
In [14788]: ngradq
Out[14788]: 
array([[-0.4019559],
       [-0.5944086]])

/ plot	, 
/ we zien loodrechte eigenvectoren van Hq	,
/ evcq1 loopt naar rechtsonder	, 
/ evcq2 loopt naar rechtsboven	,
/ -gradq ligt net boven evcq2	,
/ sq list net onder evcq1	, 
/ in sq moet Hq.dot(sq) gelijk zijn aan -gradq	,  en zo is het lin vv in sq =0	,
/ sq heeft grote positieve component langs evcq1, en kleine negatieve component langs evcq2	, sq=a*evcq1 + b*evcq2	, a>>0, b<0 klein	,
/ wat is Hq.sq? evaq1 is klein neg, evaq2 groot neg, dus Hq.a*evcq1=a*-.03*evcq1 en is kleine O(1) vector langs evaq1, die naar links wijst	, en Hq.b*evcq2 = b*-17*evcq2, en omdat b<0, is dit een vector langs evcq2 naar rechtsboven	, dus dus in sq is het l vv een vector // evcq2, maar iets naar links, en is precies -gradq	.

/ 1313	 .

/ eerste 2 stappen 	,

[[-20.07532868]
 [ 14.64951956]]
[[-21.0024308 ]
 [-31.05819707]]
[[ -6.39444339  -8.77809057]
 [ -8.77809057 -12.11921016]]
[[ 41.06556592]
 [-32.30701084]]
[[1.63424829e-13]
 [2.27373675e-13]]
[ -0.02383632 -18.48981723]
[[ 0.80932593  0.5873598 ]
 [-0.5873598   0.80932593]]
[[ 20.99023724]
 [-17.65749128]]
--------------------
[[ 20.99023724]
 [-17.65749128]]
[[-13.33164173]
 [ 74.48848531]]
[[-3.53652642 -4.34723503]
 [-4.34723503 -5.38073374]]
[[-3027.47155128]
 [ 2459.81673196]]
[[ 9.45021839e-13]
 [-9.52127266e-13]]
[-0.01467603 -8.90258413]
[[ 0.7770123   0.62948542]
 [-0.62948542  0.7770123 ]]
[[-3006.48131405]
 [ 2442.15924068]]
--------------------
/home/eric/miniconda3/bin/ipython:8: RuntimeWarning: divide by zero encountered in log

/ 13	. 

/ 1313	. 

/ newton	,
/ sigmoid	,
/ regularization	,
/ log
/ lmbda <0
/ TODO

/ als 
In [14426]: beta
Out[14426]: 
array([[-3006.48131405],
       [ 2442.15924068]])


# als je 150 waarnemingen xn doet,	 dan bepaal je de beta waarvoor deze 150 waarnemingen de grootste kans hebben	,

/ 131313	 .

/ als 
In [14426]: beta
Out[14426]: 
array([[-3006.48131405],
       [ 2442.15924068]])
/ en omdat	,
In [14427]: X[:9]
Out[14427]: 
array([[1. , 0.2],
       [1. , 0.2],
       [1. , 0.2],
       [1. , 0.2],
...
/ is	, 
In [14428]: expit(X.dot(beta))
       ...: 
Out[14428]: 
array([[0.00000000e+000],
       [0.00000000e+000],
/ en zien we dus  in _ml	,
/home/eric/miniconda3/bin/ipython:8: RuntimeWarning: divide by zero encountered in log
/ Hoe vangen we deze af?
/ TODO
/ De reden is dat 1 van de eigenwaarden is ongeveer (negatief) 0 en de andere groot negatief	,


def _ml_alt(beta):
  p1gX_at_beta=expit(X.dot(beta))
	ml_vector=t*np.log(p1gX_at_beta)+(1-t)*np.log(1-p1gX_at_beta)
	return np.ones((1,ml_vector.shape[0])).dot(ml_vector)+lmbda*beta.T.dot(beta)

def _ml(beta):
  p1gX_at_beta=expit(X.dot(beta))
	return t.T.dot(np.log(p1gX_at_beta))+(1-t).T.dot(np.log(1-p1gX_at_beta))++lmbda*beta.T.dot(beta)

def _grad_ml_to_beta(beta):
  p1gX_at_beta=expit(X.dot(beta))+lmbda*beta
  return X.T.dot(t-p1gX_at_beta)		# X staat er 2 keer in	,

def _hessian_ml_to_beta(beta):  
  p1gX_at_beta=expit(X.dot(beta))
  D=np.diag((-p1gX_at_beta*(1-p1gX_at_beta)).ravel())
  return X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])

def _grad_ml_to_beta_lin(beta,delta):
	return _grad_ml_to_beta(beta)+_hessian_ml_to_beta(beta).dot(delta)

def _eig_values(m):
	return np.linalg.eig(m)[0]
def _min_abs_eig_values(m):
	return np.min(np.abs(np.linalg.eig(m)[0]))
def _eig_vectors(m):
	return np.linalg.eig(m)[1]
def _det(m):
	return np.linalg.det(m)

def _direction(v):
	return v[1]/v[0]

def _size(v):
	return np.linalg.norm(v) 

def total():
  global beta
  ml=_ml(beta)
  grad_ml_to_beta=_grad_ml_to_beta(beta)
  hessian_ml_to_beta=_hessian_ml_to_beta(beta)
  nwt=-np.linalg.inv(hessian_ml_to_beta).dot(grad_ml_to_beta)
  check_if_grad_ml_lin_is_zero=_grad_ml_to_beta_lin(beta,nwt)
  print(beta)
  print(grad_ml_to_beta)
  print(hessian_ml_to_beta)
  print(nwt)
  print(check_if_grad_ml_lin_is_zero)
  print(_eig_values(hessian_ml_to_beta))
  print(_eig_vectors(hessian_ml_to_beta))
  beta=beta+nwt
  print(beta)
  print("--------------------")


X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
a=-21.12563996
b=12.94750716
beta=np.array([a,b]).reshape(-1,1)
beta=np.array([[a+0.8510062],[b-0.52515564]]).reshape(-1,1)
beta=np.array([[a+0.52515564],[b+0.8510062]]).reshape(-1,1) 
beta=np.array([[a+2*0.52515564],[b+2*0.8510062]]).reshape(-1,1)  
# singular hessian als λ=0, niet singular als λ=-1
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=0
for i in range(0,10):
  total()


/ unicodes	,
3bb λ
39b Λ
3bc μ
221a √
b2 ²
b3 ³
b9 ¹
207b ⁻

/ 1313	. 

/ zonder total() fct	,


X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([-21,12]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=0
for i in range(0,10):
	ml=_ml(beta)
  grad_ml_to_beta=_grad_ml_to_beta(beta)
	hessian_ml_to_beta=_hessian_ml_to_beta(beta)
	nwt=-np.linalg.inv(hessian_ml_to_beta).dot(grad_ml_to_beta)
	check_if_grad_ml_lin_is_zero=_grad_ml_to_beta_lin(beta,nwt)
	print(beta)
	print(grad_ml_to_beta)
  print(hessian_ml_to_beta)
	print(nwt)
	print(check_if_grad_ml_lin_is_zero)
  print(_eig_values(hessian_ml_to_beta))
  print(_eig_vectors(hessian_ml_to_beta))
  if (grad_ml_to_beta == np.zeros(beta.shape[0])).all():
    break
	beta=beta+nwt
	print(beta)
	print("--------------------")

ml=_ml(beta)
grad_ml_to_beta=_grad_ml_to_beta(beta)
hessian_ml_to_beta=_hessian_ml_to_beta(beta)
nwt=-np.linalg.inv(hessian_ml_to_beta).dot(grad_ml_to_beta)
check_if_grad_ml_lin_is_zero=_grad_ml_to_beta_lin(beta,nwt)
print(beta)
print(grad_ml_to_beta)
print(hessian_ml_to_beta)
print(nwt)
print(check_if_grad_ml_lin_is_zero)
print(_eig_values(hessian_ml_to_beta))
print(_eig_vectors(hessian_ml_to_beta))
beta=beta+nwt
print(beta)
print("--------------------")

In [14045]: beta=np.array([[a],[b]])
[[-21.12563996]
 [ 12.94750716]]
[[-4.86671210e-08]
 [-7.87996095e-08]]
[[ -4.8015161   -7.72519407]
 [ -7.72519407 -12.55285302]]
[[-3.65078583e-09]
 [-4.03068373e-09]]
[[9.79381845e-22]
 [1.58818678e-21]]
[ -0.0343014  -17.32006772]
[[ 0.8510062   0.52515564]
 [-0.52515564  0.8510062 ]]
[[-21.12563996]
 [ 12.94750716]]
--------------------

In [14049]: beta=np.array([[a+0.8510062],[b-0.52515564]])
 [ 12.42235152]]
[[-0.02950951]
 [ 0.0213212 ]]
[[-0.8044572 ]
 [ 0.49627096]]
[[4.4408921e-16]
 [8.8817842e-16]]
[ -0.03841331 -18.33092609]
[[ 0.85116205  0.524903  ]
 [-0.524903    0.85116205]]
[[-21.07909096]
 [ 12.91862248]]
--------------------

In [14047]: beta=np.array([[a+0.52515564],[b+0.8510062]])
[[-20.60048432]
 [ 13.79851336]]
[[ -9.93540995]
 [-15.26636816]]
[[ -6.15281497  -9.04278227]
 [ -9.04278227 -13.37175639]]
[[10.35106068]
 [-8.14169455]]
[[2.66453526e-13]
 [3.94351218e-13]]
[ -0.02574677 -19.49882459]
[[ 0.82786289  0.5609305 ]
 [-0.5609305   0.82786289]]
[[-10.24942364]
 [  5.65681881]]
--------------------

In [14107]: Hessian_of_ml_to_beta=np.array([[ -6.15281497 , -9.04278227]
       ...: , [ -9.04278227, -13.37175639]])

In [14117]: np.linalg.eig(Hessian_of_ml_to_beta)
Out[14117]: 
(array([ -0.02574677, -19.49882459]), array([[ 0.82786289,  0.5609305 ],
        [-0.5609305 ,  0.82786289]]))

In [14118]: V=np.array([[ 0.82786289,  0.5609305 ],[-0.5609305 ,  0.82786289]])

In [14119]: V
Out[14119]: 
array([[ 0.82786289,  0.5609305 ],
       [-0.5609305 ,  0.82786289]])

In [14120]: Vinv=np.linalg.inv(V)

In [14121]: Vinv
Out[14121]: 
array([[ 0.8278629 , -0.56093051],
       [ 0.56093051,  0.8278629 ]])

In [14122]: Lmbda=np.diag([ -0.02574677, -19.49882459])

In [14123]: Lmbda
Out[14123]: 
array([[ -0.02574677,   0.        ],
       [  0.        , -19.49882459]])

In [14124]: V.dot(Lmbda).dot(Vinv)
Out[14124]: 
array([[ -6.15281496,  -9.04278226],
       [ -9.04278226, -13.3717564 ]])

In [14128]: nwt
Out[14128]: 
array([[10.35106068],
       [-8.14169455]])

In [14129]: Vinv.dot(nwt)
Out[14129]: 
array([[13.13618393],
       [-0.93398115]])
In [14130]: Lmbda.dot(Vinv).dot(nwt)
Out[14130]: 
array([[-0.33821431],
       [18.21153453]])
In [14132]: V.dot(Lmbda).dot(Vinv).dot(nwt)
Out[14132]: 
array([[ 9.9354101 ],
       [15.26636833]])

/ ook	,

In [14145]: hessian_ml_to_beta
Out[14145]: 
array([[ -6.15281497,  -9.04278227],
       [ -9.04278227, -13.37175639]])

In [14146]: nwt
Out[14146]: 
array([[10.35106068],
       [-8.14169455]])

In [14147]: hessian_ml_to_beta.dot(nwt)
Out[14147]: 
array([[ 9.93540995],
       [15.26636816]])

/ ook,

/ ontbind nwt in eigv	,
/ nwt in V basis	,
In [14152]: y=Vinv.dot(nwt)
y
Out[14152]: 
array([[13.13618393],
       [-0.93398115]])

In [14183]: Lmbda
Out[14183]: 
array([[ -0.02574677,   0.        ],
       [  0.        , -19.49882459]])

In [14184]: z=Lmbda.dot(y)
z
Out[14184]: 
array([[-0.33821431],
       [18.21153453]])

In [14186]: V.dot(z)
Out[14186]: 
array([[ 9.9354101 ],
       [15.26636833]])


/ 1313	. 

/ plot	, 

def _total(beta):
	for i in range(0,10):
		path[i]=beta.ravel()
		ml=_ml(beta)
	  grad_ml_to_beta=_grad_ml_to_beta(beta)
		hessian_ml_to_beta=_hessian_ml_to_beta(beta)
		nwt=-np.linalg.inv(hessian_ml_to_beta).dot(grad_ml_to_beta)
		check_if_grad_ml_lin_is_zero=_grad_ml_to_beta_lin(beta,nwt)
		print(beta)
		print(grad_ml_to_beta)
	  print(hessian_ml_to_beta)
		print(nwt)
		print(check_if_grad_ml_lin_is_zero)
	  print(_eig_values(hessian_ml_to_beta))
	  print(_eig_vectors(hessian_ml_to_beta))
	  if (grad_ml_to_beta == np.zeros(beta.shape[0])).all():
	    break
		beta=beta+nwt
		print(beta)
		print("--------------------")


X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=0
a=-21.12563996
b=12.94750716
_max=np.array([[a],[b]])
v1=np.array([[ 0.82786289], [-0.5609305 ]])
v2=np.array([[0.5609305 ], [0.82786289]])
plt.figure()

path=np.zeros((10,2))
beta=_max+v1
_total(beta)
plt.plot(path[:,0],path[:,1])

path=np.zeros((10,2))
beta=_max+v2
_total(beta)
plt.plot(path[:,0],path[:,1])

path=np.zeros((10,2))
beta=_max-v2
_total(beta)
plt.plot(path[:,0],path[:,1])

# singular matrix
path=np.zeros((10,2))
beta=_max-2*v2
_total(beta)
plt.plot(path[:,0],path[:,1])

# singular matrix
path=np.zeros((10,2))
beta=_max+2*v2
_total(beta)
plt.plot(path[:,0],path[:,1])

/ 1313	 

/ maak contour	,

/ we maken een 441x2 array	,
/ elk elem is een (1,2)	, 
/ _ml verwacht een (2,1)
/ daarom _ml2	,

def _ml2(beta):
	return _ml(beta.reshape(-1,1))

In [14303]: _ml2(beta.reshape(1,-1))
Out[14303]: array([[-25.60583895]])

x=np.linspace(a-2,a+2,21)
y=np.linspace(b-2,b+2,21)
g1,g2=np.meshgrid(x,y)
g=np.c_[g1.ravel(),g2.ravel()]

In [14314]: _ml2(g[0])
Out[14314]: array([[-97.09921453]])

In [14316]: _ml2(g)
ValueError: shapes (150,2) and (882,1) not aligned: 2 (dim 1) != 882 (dim 0)
/ TODO Afmaken	,

/ Einde NEWTON NAAR ONEINDIG

/ NEWTON NAAR ONEINDIG 2 FEATURES

/ we copy de fcts	,

def _ml_alt(beta):
  p1gX_at_beta=expit(X.dot(beta))
	ml_vector=t*np.log(p1gX_at_beta)+(1-t)*np.log(1-p1gX_at_beta)
	return np.ones((1,ml_vector.shape[0])).dot(ml_vector)+lmbda*beta.T.dot(beta)

def _ml(beta):
  p1gX_at_beta=expit(X.dot(beta))
	return t.T.dot(np.log(p1gX_at_beta))+(1-t).T.dot(np.log(1-p1gX_at_beta))++lmbda*beta.T.dot(beta)

def _grad_ml_to_beta(beta):
  p1gX_at_beta=expit(X.dot(beta))+lmbda*beta
  return X.T.dot(t-p1gX_at_beta)		# X staat er 2 keer in	,

def _hessian_ml_to_beta(beta):  
  p1gX_at_beta=expit(X.dot(beta))
  D=np.diag((-p1gX_at_beta*(1-p1gX_at_beta)).ravel())
  return X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])

def _grad_ml_to_beta_lin(beta,delta):
	return _grad_ml_to_beta(beta)+_hessian_ml_to_beta(beta).dot(delta)

def _eig_values(m):
	return np.linalg.eig(m)[0]
def _min_abs_eig_values(m):
	return np.min(np.abs(np.linalg.eig(m)[0]))
def _eig_vectors(m):
	return np.linalg.eig(m)[1]
def _det(m):
	return np.linalg.det(m)

def _direction(v):
	return v[1]/v[0]

def _size(v):
	return np.linalg.norm(v) 

def total():
  global beta
  ml=_ml(beta)
  grad_ml_to_beta=_grad_ml_to_beta(beta)
  hessian_ml_to_beta=_hessian_ml_to_beta(beta)
  nwt=-np.linalg.inv(hessian_ml_to_beta).dot(grad_ml_to_beta)
  check_if_grad_ml_lin_is_zero=_grad_ml_to_beta_lin(beta,nwt)
  print(beta)
  print(grad_ml_to_beta)
  print(hessian_ml_to_beta)
  print(nwt)
  print(check_if_grad_ml_lin_is_zero)
  print(_eig_values(hessian_ml_to_beta))
  print(_eig_vectors(hessian_ml_to_beta))
  beta=beta+nwt
  print(beta)
  print("--------------------")


X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
a=-21.12563996
b=12.94750716
beta=np.array([a,b]).reshape(-1,1)
beta=np.array([[a+0.8510062],[b-0.52515564]]).reshape(-1,1)
beta=np.array([[a+0.52515564],[b+0.8510062]]).reshape(-1,1) 
beta=np.array([[a+2*0.52515564],[b+2*0.8510062]]).reshape(-1,1)  
beta=np.array([0,0]).reshape(-1,1)
# singular hessian als λ=0, niet singular als λ=-1
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=0
for i in range(0,10):
  total()


/ unicodes	,
3bb λ
39b Λ
3bc μ
221a √
b2 ²
b3 ³
b9 ¹
207b ⁻






/ 131313	. 
	
X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([-21,12]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
#resML=np.zeros((10,2+2+2+1+1+2))
lmbda=0
i=0
for i in range(0,10):
  #m=X.dot(beta)
  p1gX=expit(X.dot(beta))
	mlvect=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)
	ml=np.ones((1,mlvect.shape[0])).dot(mlvect)+lmbda*beta.T.dot(beta)
  grad_ml_to_beta=X.T.dot(t-p1gX)+lmbda*beta 		# X staat er 2 keer in	,
  D=np.diag((-p1gX*(1-p1gX)).ravel())
  hessian_ml_to_beta=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
	nwt=-np.linalg.inv(hessian_ml_to_beta).dot(grad_ml_to_beta)
	grad_ml_to_beta_lin_in_nwt=grad_ml_to_beta+hessian_ml_to_beta.dot(nwt)
	print(beta)
	print(grad_ml_to_beta)
	print(nwt)
	print(grad_ml_to_beta_lin_in_nwt)
	print("--------------------")
  if (grad_ml_to_beta == np.zeros(beta.shape[0])).all():
    break
	beta=beta+nwt

p1gX=expit(X.dot(beta))
mlvect=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)
ml=np.ones((1,mlvect.shape[0])).dot(mlvect)+lmbda*beta.T.dot(beta)
grad_ml_to_beta=X.T.dot(t-p1gX)+lmbda*beta 		# X staat er 2 keer in	,
D=np.diag((-p1gX*(1-p1gX)).ravel())
hessian_ml_to_beta=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
nwt=-np.linalg.inv(hessian_ml_to_beta).dot(grad_ml_to_beta)
grad_ml_to_beta_lin_in_nwt=grad_ml_to_beta+hessian_ml_to_beta.dot(nwt)
print(beta)
print(grad_ml_to_beta)
print(nwt)
print(grad_ml_to_beta_lin_in_nwt)
print("--------------------")
beta=beta+nwt

/ HIER HIER HIER

	
	resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
  beta=beta+nwt

m=X.dot(beta)
p1gX=expit(m)
mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)
ml=np.ones((1,mlv.shape[0])).dot(mlv)+lmbda*beta.T.dot(beta)
grad=X.T.dot(t-p1gX)+lmbda*beta
D=np.diag((-p1gX*(1-p1gX)).ravel())
H=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
nwt=-np.linalg.inv(H).dot(grad)
resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
beta=beta+nwt
i=i+1

# in de 1ste stap ga je de verkeerde kant op	, daarna goed:
6.49116244e+00, -2.92355732e+00, 	# verkeerd	,
-4.03932128e+00,  2.33840635e+00	# ok	,
...
In [13358]: resML
Out[13358]: 
array([[-2.10000000e+01,  1.20000000e+01,  7.32980154e+00,
         1.24971138e+01,  6.49116244e+00, -2.92355732e+00,
        -2.18780514e+01,  1.44880587e+01, -3.16189918e-02,
        -2.21976998e+01],
       [-1.45088376e+01,  9.07644268e+00, -2.85554275e+00,
        -3.77084753e+00, -4.03932128e+00,  2.33840635e+00,
        -1.85144814e+01,  4.73005449e+00, -8.72161555e-02,
        -2.71034126e+01],
       [-1.85481588e+01,  1.14148490e+01, -6.23277201e-01,
        -7.75108084e-01, -2.16067493e+00,  1.28278799e+00,
        -1.69074812e+01,  9.94619028e-01, -4.83104324e-02,
        -2.05265739e+01],
       [-2.07088338e+01,  1.26976370e+01, -7.26514138e-02,
        -8.51030288e-02, -4.05525346e-01,  2.43065060e-01,
        -1.67148719e+01,  1.11896173e-01, -3.61946798e-02,
        -1.77947749e+01],
       [-2.11143591e+01,  1.29407021e+01, -1.59553397e-03,
        -1.72443592e-03, -1.12725670e-02,  6.80005759e-03,
        -1.67104073e+01,  2.34934201e-03, -3.43512445e-02,
        -1.73327510e+01],
       [-2.11256317e+01,  1.29475021e+01, -9.42868266e-07,
        -8.95445294e-07, -8.27835686e-06,  5.02327785e-06,
        -1.67104041e+01,  1.30031644e-06, -3.43014348e-02,
        -1.73200771e+01],
       [-2.11256400e+01,  1.29475072e+01, -3.89910326e-13,
        -2.90434343e-13, -4.46189873e-12,  2.72277538e-12,
        -1.67104041e+01,  4.86191495e-13, -3.43013980e-02,
        -1.73200677e+01],
       [-2.11256400e+01,  1.29475072e+01, -3.10862447e-15,
        -3.55271368e-15, -1.94860360e-14,  1.17089474e-14,
        -1.67104041e+01,  4.72073305e-15, -3.43013980e-02,
        -1.73200677e+01],
       [-2.11256400e+01,  1.29475072e+01, -1.46549439e-14,
        -2.22044605e-14, -2.09182591e-14,  1.11044995e-14,
        -1.67104041e+01,  2.66046133e-14, -3.43013980e-02,
        -1.73200677e+01],
       [-2.11256400e+01,  1.29475072e+01,  3.55271368e-15,
         7.10542736e-15, -1.73271648e-14,  1.12294104e-14,
        -1.67104041e+01,  7.94410929e-15, -3.43013980e-02,
        -1.73200677e+01]])

/ 1313	. 

/ 1 feature	,
/ newton	,
/ print 

beta0=np.linspace(-22,-20,11)
beta1=np.linspace(11,13,11)
beta0c,beta1c=np.meshgrid(beta0,beta1,indexing='ij')
betag=np.c_[beta0c.ravel(),beta1c.ravel()]

X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
t=(iris['target']==2).reshape(-1,1).astype(int)
#resML=np.zeros((g.shape[0],g.shape[1]+2+1))
# :2		2:4					 4     5:7 7	 8:10	
# beta, normed grad,|grad|,nwt,ml,eigv(hessian)
resML=np.zeros((g.shape[0],g.shape[1]+2+1+2+1+2))
lmbda=0

for i in np.arange(0,g.shape[0]):
	beta=g[i].reshape(-1,1)
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)
	ml=np.ones((1,mlv.shape[0])).dot(mlv)+lmbda*beta.T.dot(beta)
  grad=X.T.dot(t-p1gX)+lmbda*beta
  D=np.diag((-p1gX*(1-p1gX)).ravel())
  H=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
	nwt=-np.linalg.inv(H).dot(grad)
	#resML[i]=np.r_[beta.ravel(),(grad/np.linalg.norm(grad)).ravel(),ml.ravel()]
	resML[i]=np.r_[beta.ravel(),(grad/np.linalg.norm(grad)).ravel(),np.linalg.norm(grad).ravel(),nwt.ravel(),ml.ravel(),np.linalg.eig(H)[0]]
  beta=beta+nwt

df=pd.DataFrame(resML,columns=['b1','b2','g1','g2','ml'])
In [13397]: df.sort_values(by='ml',ascending=False)
Out[13397]: 
       b1    b2        g1        g2         ml
54  -21.2  13.0 -0.497811 -0.867286 -16.710817
85  -20.6  12.6  0.469843  0.882750 -16.720871
75  -20.8  12.8 -0.539241 -0.842151 -16.730836
106 -20.2  12.4 -0.612557 -0.790426 -16.734740
64  -21.0  12.8  0.518454  0.855106 -16.742067
116 -20.0  12.2  0.472626  0.881263 -16.762515
96  -20.4  12.6 -0.542745 -0.839898 -16.787476
95  -20.4  12.4  0.504827  0.863221 -16.790921
43  -21.4  13.0  0.527250  0.849710 -16.796113
65  -21.0  13.0 -0.528959 -0.848647 -16.816883
74  -20.8  12.6  0.515805  0.856706 -16.852889
117 -20.0  12.4 -0.544585 -0.838706 -16.883189
86  -20.6  12.8 -0.534463 -0.845192 -16.916849
105 -20.2  12.2  0.507916  0.861406 -16.945443
53  -21.2  12.8  0.521030  0.853539 -16.946280
84  -20.6  12.4  0.513824  0.857896 -17.047804
107 -20.2  12.6 -0.538024 -0.842929 -17.057554
32  -21.6  13.0  0.523882  0.851791 -17.069108
76  -20.8  13.0 -0.532087 -0.846690 -17.119464
63  -21.0  12.6  0.517380  0.855756 -17.180220
115 -20.0  12.0  0.508105  0.861295 -17.189431
97  -20.4  12.8 -0.535557 -0.844499 -17.305801
94  -20.4  12.2  0.511990  0.858991 -17.331875
42  -21.4  12.8  0.519636  0.854388 -17.340786
73  -20.8  12.4  0.514597  0.857432 -17.503062
21  -21.8  13.0  0.521100  0.853496 -17.527741
118 -20.0  12.6 -0.538328 -0.842735 -17.537314
87  -20.6  13.0 -0.534666 -0.845063 -17.625011
52  -21.2  12.6  0.516388  0.856355 -17.701171
104 -20.2  12.0  0.510213  0.860048 -17.710710
...

In [13416]: resML[(resML[:,0]==-21)&(resML[:,1]==12)]
Out[13416]: 
array([[-21.        ,  12.        ,   0.5059202 ,   0.86258029,
         14.48805871,   6.49116244,  -2.92355732, -21.87805139,
         -0.03161899, -22.19769975]])
/ Hoe met df?
/ TODO

In [13422]: df=pd.DataFrame(resML,columns=['b1','b2','g1','g2','norm grad','nwt1','nwt2','ml','eigv1','eigv2'])


/ Einde NEWTON LOGISTIC REGRESSION

/ GD LOGISTIC REGRESSION

/ 13	.

/ gd,
/ sigmoid	,
/ 1 feature	,
/ log	, = ml
/ zoek max, dus volg grad	,

/ newton,
In [13606]: beta
Out[13606]: 
array([[-21.12563996],
       [ 12.94750716]])

X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([0,0]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :2		2:4 	4:6  6		
# beta, grad,gd,ml
resML=np.zeros((200,2+2+2+1))
alpha=.1
i=0
for i in range(0,200):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX) 	# log ipv -log
	ml=np.ones((1,mlv.shape[0])).dot(mlv)
  grad=X.T.dot(t-p1gX)									# t-p1gX ipv p1gX-t 
	gd=alpha*grad
	resML[i]=np.r_[beta.ravel(),grad.ravel(),gd.ravel(),ml.ravel(),]
  beta=beta+gd

m=X.dot(beta)
p1gX=expit(m)
mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX) 	# log ipv -log
ml=np.ones((1,mlv.shape[0])).dot(mlv)
grad=X.T.dot(t-p1gX)									# t-p1gX ipv p1gX-t 
gd=alpha*grad
resML[i]=np.r_[beta.ravel(),grad.ravel(),gd.ravel(),ml.ravel(),]
beta=beta+gd
i=i+1

def compute_ml(beta):
  m=X.dot(beta)
  p1gX=expit(m)
  mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)   # log ipv -log
  ml=np.ones((1,mlv.shape[0])).dot(mlv)
	return ml

/ je moet niet de hele gradient nemen: δc/δβ, maar een factor:
In [13570]: compute_ml(np.array([[0],[0]]))
Out[13570]: array([[-103.97207708]])
In [13571]: compute_ml(np.array([[-25],[11.4]]))
Out[13571]: array([[-129.89382314]])

/ we kijken in de buurt, en we nemen factor=.1	,
/ dan zie je dat .1*δc/δβ het beste is	,
In [13573]: compute_ml(np.array([[-25/10],[11.4/10]]))
Out[13573]: array([[-61.47709908]])
In [13574]: compute_ml(np.array([[-25/10],[-11.4/10]]))
Out[13574]: array([[-244.86112075]])
In [13575]: compute_ml(np.array([[25/10],[-11.4/10]]))
Out[13575]: array([[-212.46909908]])
In [13576]: compute_ml(np.array([[25/10],[11.4/10]]))
Out[13576]: array([[-343.86912075]])
/ en	,
In [13577]: compute_ml(np.array([[-25/10],[0]]))
Out[13577]: array([[-136.83346014]])
In [13580]: compute_ml(np.array([[0],[11.4/10]]))
Out[13580]: array([[-132.84899647]])
/ en	,
In [13584]: compute_ml(np.array([[-25*.2],[11.4*.2]]))			<-
Out[13584]: array([[-54.93961992]])
In [13586]: compute_ml(np.array([[-25*.3],[11.4*.3]]))
Out[13586]: array([[-58.75440841]])


/ 13	. 

/ gd,
/ 1 feature	,
/ sigmoid	,
/ -log, 
/ zoek min cost	, dus volg -grad,	

X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([0,0]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :2	  2:4		4:6 6	
# beta, grad,gd,ml
resML=np.zeros((10,2+2+2+1))
alpha=1
i=0
for i in range(0,10):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*-np.log(p1gX)+(1-t)*-np.log(1-p1gX) 	
	ml=np.ones((1,mlv.shape[0])).dot(mlv)
  grad=X.T.dot(p1gX-t)								
	gd=alpha*-grad
	resML[i]=np.r_[beta.ravel(),grad.ravel(),gd.ravel(),ml.ravel(),]
  beta=beta+gd

m=X.dot(beta)
p1gX=expit(m)
mlv=t*-np.log(p1gX)+(1-t)*-np.log(1-p1gX)
ml=np.ones((1,mlv.shape[0])).dot(mlv)
grad=X.T.dot(p1gX-t)			
gd=alpha*-grad
resML[i]=np.r_[beta.ravel(),grad.ravel(),gd.ravel(),ml.ravel(),]
beta=beta+gd
i=i+1

/ 13. 

/ gd,
/ sigmoid	,
/ -log, 
/ zoek min cost	, dus volg -grad,	

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([-45,5,10]).reshape(-1,1)
beta=np.array([0,0,0]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
resML=np.zeros((1000,3+3+3+1))
alpha=1
i=0
for i in range(0,10):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*-np.log(p1gX)+(1-t)*-np.log(1-p1gX) 	
	ml=np.ones((1,mlv.shape[0])).dot(mlv)
  grad=X.T.dot(p1gX-t)								
	gd=alpha*-grad
	resML[i]=np.r_[beta.ravel(),grad.ravel(),gd.ravel(),ml.ravel(),]
  beta=beta+gd

m=X.dot(beta)
p1gX=expit(m)
mlv=t*-np.log(p1gX)+(1-t)*-np.log(1-p1gX)
ml=np.ones((1,mlv.shape[0])).dot(mlv)
grad=X.T.dot(p1gX-t)			
nwt=alpha*-grad
resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),]
beta=beta+gd
i=i+1

/ 13	.

/ gd,
/ sigmoid	,
/ log	, = ml
/ zoek max, dus volg grad	,

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([0,0,0]).reshape(-1,1)
beta=np.array([-45,5,10]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,gd,ml,|grad|,eigv(hessian)
resML=np.zeros((10000,3+3+3+1))
alpha=.1
i=0
for i in range(0,10000):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX) 	# log ipv -log
	ml=np.ones((1,mlv.shape[0])).dot(mlv)
  grad=X.T.dot(t-p1gX)									# t-p1gX ipv p1gX-t 
	gd=alpha*grad
	resML[i]=np.r_[beta.ravel(),grad.ravel(),gd.ravel(),ml.ravel(),]
  beta=beta+gd

m=X.dot(beta)
p1gX=expit(m)
mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX) 	# log ipv -log
ml=np.ones((1,mlv.shape[0])).dot(mlv)
grad=X.T.dot(t-p1gX)									# t-p1gX ipv p1gX-t 
gd=alpha*grad
resML[i]=np.r_[beta.ravel(),grad.ravel(),gd.ravel(),ml.ravel(),]
beta=beta+gd
i=i+1

/ Einde GD LOGISTIC REGRESSION

/  7	. 

/ NEWTON 

/ 13	.  

/ zonder demping	,

In [13439]: np.sqrt(1/3)
Out[13439]: 0.5773502691896257
/ opl + en -	,

def f(x):
	return x**3-x
def gradf(x):
	return 3*x**2-1
def nwt(s):
	return -f(s)/gradf(s)	

x=1/np.sqrt(3)
res=np.zeros((10,2))
for i in np.arange(0,10):
	d=nwt(x)
	res[i]=(x,d)
	x=x+d

d=nwt(x)
res[i]=(x,d)
x=x+d

/ links en vlakbij 1/√3 -> -1 en verder naar links naar 0 -> 0
/ rechts van 1/√3 -> 1
/ rechts vlakbij -1/√3 -> 1
/ links van -1/√3 -> -1
      ->-1     | ->1               0<-           -1<- |          ->1
-------------------------------------------------------------------
	   -1				-1/√3								0            1/√3              1

/ 1/√3 = 0.5773502691896257
/ we zien dat .45 -> -1 omdat dicht bij 1/√3	, en .44 -> 0 omdat dicht bij 0	

/ bij .44 < 1/√5  <.45 springt hij heen en weer	, 
/ TODO

/ .5 -> -1	, en dus niet naar 0	, die dichterbij ligt	,
/ maar het schiet over 0 heen	,
In [13458]: res
Out[13458]: 
array([[ 0.5, -1.5],
       [-1. , -0. ],
       [-1. , -0. ],
       [-1. , -0. ],
       [-1. , -0. ],
       [-1. , -0. ],
       [-1. , -0. ],
       [-1. , -0. ],
       [-1. , -0. ],
       [-1. , -0. ]])

/ .4 -> 0 wel	,
In [13460]: res
Out[13460]: 
array([[ 4.00000000e-01, -6.46153846e-01],
       [-2.46153846e-01,  2.82610534e-01],
       [ 3.64566877e-02, -3.65539840e-02],
       [-9.72963905e-05,  9.72963923e-05],
       [ 1.84212965e-12, -1.84212965e-12],
       [ 0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00]])

/ 0 -> 0
In [13462]: res
Out[13462]: 
array([[ 0., -0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.]])

/ .57 -> -1
In [13464]: res
Out[13464]: 
array([[  0.57      , -15.20976285],
       [-14.63976285,   4.86471794],
       [ -9.7750449 ,   3.23553509],
       [ -6.53950981,   2.14558817],
       [ -4.39392164,   1.4131771 ],
       [ -2.98074454,   0.9161229 ],
       [ -2.06462164,   0.57144306],
       [ -1.49317858,   0.32273996],
       [ -1.17043862,   0.13923055],
       [ -1.03120807,   0.02984625]])

/ .49 -> -1
In [13466]: res
Out[13466]: 
array([[ 4.90000000e-01, -1.33125134e+00],
       [-8.41251341e-01, -2.18940474e-01],
       [-1.06019182e+00,  5.54257002e-02],
       [-1.00476611e+00,  4.73241592e-03],
       [-1.00003370e+00,  3.36973520e-05],
       [-1.00000000e+00,  1.70330549e-09],
       [-1.00000000e+00, -0.00000000e+00],
       [-1.00000000e+00, -0.00000000e+00],
       [-1.00000000e+00, -0.00000000e+00],
       [-1.00000000e+00, -0.00000000e+00]])

/ .58 -> 1
In [13468]: res
Out[13468]: 
array([[  0.58      ,  41.83565217],
       [ 42.41565217, -14.1333106 ],
       [ 28.28234158,  -9.41958664],
       [ 18.86275494,  -6.27579293],
       [ 12.58696201,  -4.17796183],
       [  8.40900018,  -2.77644818],
       [  5.632552  ,  -1.8376452 ],
       [  3.7949068 ,  -1.20502341],
       [  2.58988338,  -0.77300344],
       [  1.81687994,  -0.46957905]])


/ 13	. 

/ met demping	, 

/ zonder demping zagen we dat .45 -> -1 omdat dichtbij 1/√3	, en .44 -> 0
/ maar met demping gaat .45 -> 0	,

def f(x):
  return x**3-x
def gradf(x):
  return 3*x**2-1

def nwt(s):
  return -f(s)/gradf(s)

x=.45
alpha=.1
res=np.zeros((100,2))
for i in np.arange(0,100):
  d=alpha*nwt(x)
  res[i]=(x,d)
  x=x+d

In [13499]: res
Out[13499]: 
array([[ 4.40000000e-01, -8.46412214e-01],
       [-4.06412214e-01,  6.72533724e-01],
       [ 2.66121510e-01, -3.13984343e-01],
...
       [ 1.22894507e-05, -1.22894507e-06],
       [ 1.10605056e-05, -1.10605056e-06],
       [ 9.95445506e-06, -9.95445506e-07]])

/ 7	. 

/  nulpunten afgeleiden	,

# (x³-x)(x²-4)	 		
/ x³= x ctrl+shift+u b3
/ x²= x ctrl+shift+u b2
def f(x):
  return x**5-5*x**3+4*x 
def gradf(x):
  return 5*x**4-15*x**2+4 
def hessianf(x):
	return 20*x**3-30*x 

def nwt(s):
  return -gradf(s)/hessianf(s)

plt.figure()
x=np.linspace(-2,2,21)
plt.plot(x,f(x))
/ we zien de max/min bij ongeveer -1.6, -.6	, .6, 1.6

x=.45
alpha=.1
res=np.zeros((100,2))
for i in np.arange(0,100):
  d=alpha*nwt(x)
  res[i]=(x,d)
  x=x+d

/ 	7 .

/ met newton op zoek naar gradient =0

def r2(x):
	return x[0,0]**2+x[1,0]**2

def gradr2(x):
	return 2*x

def hessianr2(x):
	return 2*np.eye(x.shape[0])

def nwtr2(x):
	return -np.linalg.inv(hessianr2(x)).dot(gradr2(x))

x=np.array([[3],[4]])
d=nwtr2(x)
x=x+d

/ je bent er in 1 stap	,  

/ 7	. 

/ met newton op zoek naar gradient =0

/ 13	. 

def r2(x):
	return x[0,0]**2+x[1,0]**2
def gradr2(x):
	return 2*x
def hessianr2(x):
	return 2*np.eye(x.shape[0])
def nwtr2(x):
	return -np.linalg.inv(hessianr2(x)).dot(gradr2(x))

# in 1 stap gradr2(x)=0
x=np.array([[3],[4]])
d=nwtr2(x)
x=x+d
gradr2(x)

/ 13	. 

def r(x):
	return np.sqrt(r2(x))
def gradr(x):
	return gradr2(x)/2/r(x)
def hessianr_alt(x):
	return hessianr2(x)/2/r(x)-gradr2(x).dot(gradr(x).T)/2/r(x)**2
def hessianr(x):
	return np.eye(x.shape[0])/r(x)-1/(r(x)**3)*x.dot(x.T)
def hessianr_alt2(x):
	return np.eye(x.shape[0])/r(x)-x.dot(x.T)/r(x)**3
def hessianr_alt3(x):
	return (r(x)**2-x.dot(x.T))/r(x)**3
def nwtr(x):
	return -np.linalg.inv(hessianr(x)).dot(gradr(x))

x=np.array([[3],[4]])
d=nwtr(x)
x=x+d
gradr(x)
# Singular matrix	,

/ 13	. 

/////////////////////////////////////////

def r3(x):
	return r(x)**3
# vv
def gradr3(x):
	return 3*r(x)**2*gradr(x)
def gradr3_alt(x):
	return 3*r(x)*x
def hessianr3(x):
	return 6*r(x)*gradr(x).dot(gradr(x).T)+3*r(x)**2*hessianr(x)
def hessianr3_alt(x):
	return 3*x.dot(x.T)/r(x)+3*r(x)*np.eye(x.shape[0])
# linear vv
def gradr3_lin(x,d):
	return gradr3(x)+hessianr3(x).dot(d)
def nwtr3(x):
	return -np.linalg.inv(hessianr3(x)).dot(gradr3(x))
def nwtr3_alt(x):
	return -np.linalg.inv(hessianr3_alt(x)).dot(gradr3_alt(x))

x=np.array([[1],[2]])
for iter in np.arange(10):
  d=nwtr3(x)
  print(x)
	print(d)
  print(gradr3_lin(x,d))
  print(gradr3(x))
	print("-----------")
  x=x+d
	if (gradr3(x)==np.array([[0],[0]])).all():
		break

d=nwtr3(x)
print(x)
print(d)
print(gradr3_lin(x,d))
print(gradr3(x))
print("-----------")
x=x+d

/ 13	. 

def c(x):
	return r(x)**3 -r(x)
def gradc(x):
	return (3*r(x)**2-1)*gradr(x)
def hessianc(x):
	return 6*r(x)*gradr(x).dot(gradr(x).T)+(3*r(x)**2-1)*hessianr(x)
def nwtc(x):
	return -np.linalg.inv(hessianc(x)).dot(gradc(x))

x=np.array([[3],[4]])
for iter in np.arange(10):
  d=nwtc(x)
  x=x+d
  print(gradc(x))

/ 13	. 

def c(x):
	return r(x)**3 -r(x)**2
def gradc(x):
	return (3*r(x)**2-2*r(x))*gradr(x)
def hessianc(x):
	return (6*r(x)-2)*gradr(x).dot(gradr(x).T)+(3*r(x)**2-2*r(x))*hessianr(x)
def nwtc(x):
	return -np.linalg.inv(hessianc(x)).dot(gradc(x))

x=np.array([[3],[4]])
for iter in np.arange(10):
  d=nwtc(x)
  x=x+d
  display(gradc(x))
	if (gradc(x)==np.zeros(2)).all():
		break
# 
In [13835]: x
Out[13835]: 
array([[0.4       ],
       [0.53333333]])
In [13834]: gradc(x)
Out[13834]: 
array([[0.],
       [0.]])
In [13833]: hessianc(x)
Out[13833]: 
array([[0.72, 0.96],
       [0.96, 1.28]])
# is singular	,

x=np.array([[3],[4]])
d=nwtc(x)
x=x+d
gradc(x)


/ 7	 




/ Einde NEWTON
