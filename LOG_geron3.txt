/ See NEWTON NAAR ONEINDIG 



/ 13

/ NEWTON LOGISTIC REGRESSION

/ 1313	. 

/ newton, 
/ sigmoid,
/ -log
/ = cost	, dus zoek min cost

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([-45,5,10]).reshape(-1,1)
beta=np.array([0,0,0]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
resML=np.zeros((10,3+3+3+1+1+3))
for i in range(0,10):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*-np.log(p1gX)+(1-t)*-np.log(1-p1gX)
	ml=np.ones((1,mlv.shape[0])).dot(mlv)
  grad=X.T.dot(p1gX-t)
  D=np.diag((p1gX*(1-p1gX)).ravel())
  H=X.T.dot(D).dot(X)
	nwt=-np.linalg.inv(H).dot(grad)
	resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
  beta=beta+nwt

In [13307]: resML[:,9]
Out[13307]: 
array([103.97207708,  48.1034082 ,  33.56702215,  22.92609003,
        15.70037066,  12.12853775,  10.67459324,  10.3132056 ,
        10.28207149,  10.28175409])

In [7638]: theta
Out[7638]:
array([[-4.27700077],
       [ 0.04030967],
       [ 2.53398464]])

/ als lmbda=.1  ,
In [7644]: theta
Out[7644]:
array([[-11.50605939],
       [  0.74893214],
       [  4.79308489]])

/ als lmbda=.01
In [7651]: theta
Out[7651]:
array([[-24.86777444],
       [  2.70155942],
       [  7.10675456]])


/ 1313	. 

/ newton	,
/ sigmoid	,
/ log
/ = ml	, dus zoek max ml	, 

beta=np.array([-45,5,10]).reshape(-1,1) # singl
beta=np.array([-46,5,10]).reshape(-1,1) # singl
beta=np.array([-46,6,10]).reshape(-1,1) # ok	, 
beta=np.array([-46,5,11]).reshape(-1,1) # singl	, 
beta=np.array([-45,6,11]).reshape(-1,1) # ok, 
beta=np.array([-45,4,11]).reshape(-1,1) # singl, 
beta=np.array([-45,7,10]).reshape(-1,1) # singl, 

/ alpha=.4 OK	, 
/ alpha=.5 ERR	, 

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([-45,5,10]).reshape(-1,1)	 # singl
beta=np.array([0,0,0]).reshape(-1,1)		# ok	,
beta=np.array([-45,5,10]).reshape(-1,1)	 # singl
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
resML=np.zeros((20,3+3+3+1+1+3))
alpha=.4
for i in range(0,20):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX) 	# log ipv -log
	ml=np.ones((1,mlv.shape[0])).dot(mlv)
  grad=X.T.dot(t-p1gX)									# t-p1gX ipv p1gX-t 
  D=np.diag((-p1gX*(1-p1gX)).ravel())		# -p1gX*(1-p1gX) ipv p1gX*(1-p1gX)
  H=X.T.dot(D).dot(X)
	nwt=-alpha*np.linalg.inv(H).dot(grad)
	resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
  beta=beta+nwt

/ bij alpha=.4	,

In [13547]: beta
Out[13547]: 
array([[-45.19638984],
       [  5.744688  ],
       [ 10.42942934]])



/ 1313	. 

/ newton	,
/ sigmoid	,
/ regularization	,
/ -log
/ lmbda >0
/ TODO

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([-45,5,10]).reshape(-1,1)
beta=np.array([0,0,0]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
resML=np.zeros((10,3+3+3+1+1+3))
lmbda=1
i=0
for i in range(0,10):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*-np.log(p1gX)+(1-t)*-np.log(1-p1gX)
	ml=np.ones((1,mlv.shape[0])).dot(mlv)+lmbda*beta.T.dot(beta)
  grad=X.T.dot(p1gX-t)+lmbda*beta
  D=np.diag((p1gX*(1-p1gX)).ravel())
  H=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
	nwt=-np.linalg.inv(H).dot(grad)
	resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
  beta=beta+nwt

m=X.dot(beta)
p1gX=expit(m)
mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)
ml=np.ones((1,mlv.shape[0])).dot(mlv)+lmbda*beta.T.dot(beta)
grad=X.T.dot(t-p1gX)+lmbda*beta
D=np.diag((-p1gX*(1-p1gX)).ravel())
H=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
nwt=-np.linalg.inv(H).dot(grad)
resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
beta=beta+nwt
i=i+1

/ 1313	. 

/ newton	,
/ sigmoid	,
/ regularization	,
/ log
/ lmbda <0
/ TODO

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([-45,5,10]).reshape(-1,1)
beta=np.array([0,0,0]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
resML=np.zeros((10,3+3+3+1+1+3))
lmbda=-1
i=0
for i in range(0,10):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)
	ml=np.ones((1,mlv.shape[0])).dot(mlv)+lmbda*beta.T.dot(beta)
  grad=X.T.dot(t-p1gX)+lmbda*beta
  D=np.diag((-p1gX*(1-p1gX)).ravel())
  H=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
	nwt=-np.linalg.inv(H).dot(grad)
	resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
  beta=beta+nwt

m=X.dot(beta)
p1gX=expit(m)
mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)
ml=np.ones((1,mlv.shape[0])).dot(mlv)+lmbda*beta.T.dot(beta)
grad=X.T.dot(t-p1gX)+lmbda*beta
D=np.diag((-p1gX*(1-p1gX)).ravel())
H=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
nwt=-np.linalg.inv(H).dot(grad)
resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
beta=beta+nwt
i=i+1

/ 13	. 

/ lees	,
https://stackoverflow.com/questions/32308335/how-to-handle-the-divide-by-zero-exception-in-list-comprehensions-while-dividing

def add_handler(handler, exc, func):
    def wrapper(*args, **kwargs):

        try:
            return func(*args, **kwargs)
        except exc:
            return handler(*args, **kwargs)    # ???
    return wrapper

/ 13	. 

X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
a=-21.12563996
b=12.94750716
beta=np.array([a,b]).reshape(-1,1)
beta=np.array([[a+0.8510062],[b-0.52515564]]).reshape(-1,1)
beta=np.array([[a+0.52515564],[b+0.8510062]]).reshape(-1,1) 
beta=np.array([[a+2*0.52515564],[b+2*0.8510062]]).reshape(-1,1)  
# singular hessian als λ=0, niet singular als λ=-1
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=0
for i in range(0,10):
  total()

[[-20.07532868]
 [ 14.64951956]]
[[-21.0024308 ]
 [-31.05819707]]
[[ -6.39444339  -8.77809057]
 [ -8.77809057 -12.11921016]]
[[ 41.06556592]
 [-32.30701084]]
[[1.63424829e-13]
 [2.27373675e-13]]
[ -0.02383632 -18.48981723]
[[ 0.80932593  0.5873598 ]
 [-0.5873598   0.80932593]]
[[ 20.99023724]
 [-17.65749128]]
--------------------
[[ 20.99023724]
 [-17.65749128]]
[[-13.33164173]
 [ 74.48848531]]
[[-3.53652642 -4.34723503]
 [-4.34723503 -5.38073374]]
[[-3027.47155128]
 [ 2459.81673196]]
[[ 9.45021839e-13]
 [-9.52127266e-13]]
[-0.01467603 -8.90258413]
[[ 0.7770123   0.62948542]
 [-0.62948542  0.7770123 ]]
[[-3006.48131405]
 [ 2442.15924068]]
--------------------
/home/eric/miniconda3/bin/ipython:8: RuntimeWarning: divide by zero encountered in log


////////////////////////////////////////////////////////////

/ 7	. 

/ NEWTON NAAR ONEINDIG 

/ 1 feature	,

/ 13

/ in p	,

/ ontbind een vector op evc's ,

In [14460]: Hp=_hessian_ml_to_beta(p)
In [14593]: Ep=np.linalg.eig(Hp)
In [14595]: evap=Ep[0]
In [14597]: Lp=np.diag(evap)
In [14598]: Lp
Out[14598]: 
array([[ -0.0343014 ,   0.        ],
       [  0.        , -17.32006772]])
In [14621]: Vp=Ep[1]
In [14622]: Vp
Out[14622]: 
array([[ 0.8510062 ,  0.52515564],
       [-0.52515564,  0.8510062 ]])

In [14623]: e1=np.array([[1],[0]])
In [14624]: e2=np.array([[0],[1]])

In [14626]: evcp1=Vp.dot(e1)
In [14627]: evcp2=Vp.dot(e2)

In [14631]: Vinvp=np.linalg.inv(Vp)

/ check	,
In [14632]: Vp.dot(Lp).dot(Vinvp)
Out[14632]: 
array([[ -4.8015161 ,  -7.72519407],
       [ -7.72519407, -12.55285302]])
In [14634]: Hp
Out[14634]: 
array([[ -4.8015161 ,  -7.72519407],
       [ -7.72519407, -12.55285302]])

/ evcp1 en evcp2 basic eigenvectors,
/ check	,
In [14640]: Vinvp.dot(evcp1)
Out[14640]: 
array([[1.],
       [0.]])
In [14641]: Vinvp.dot(evcp2)
Out[14641]: 
array([[5.55111512e-17],
       [1.00000000e+00]])

/ wat is effect Hp op beide eigenvectors?
/ we geven coord van effect in basic eigenvectors	,

39b Λ
207b ⁻
b9 ¹ 
/ omdat HV=VΛ is H=VΛV⁻¹ , dus geeft ΛV⁻¹x de coord van Hx tov de eigenvectoren,

In [14642]:  Lp.dot(Vinvp).dot(evcp2)
Out[14642]: 
array([[-5.20417043e-18],
       [-1.73200677e+01]])

In [14643]:  Lp.dot(Vinvp).dot(evcp1)
Out[14643]: 
array([[-0.0343014],
       [ 0.       ]])

In [14650]:  Lp.dot(Vinvp).dot(evcp2*2)
Out[14650]: 
array([[-1.04083409e-17],
       [-3.46401354e+01]])
/ klopt, we zien 2 keer de 2de eigenwaarde	,  

/ in q	,

In [14659]: q=p+2*evcp2
In [14660]: q
Out[14660]: 
array([[-20.07532867],
       [ 14.64951956]])

In [14661]: Hq=_hessian_ml_to_beta(q)
In [14662]: Eq=np.linalg.eig(Hq)
In [14663]: evaq=Eq[0]
In [14664]: Lq=np.diag(evaq)
In [14665]: Lq
Out[14665]: 
array([[ -0.02383632,   0.        ],
       [  0.        , -18.48981723]])

In [14666]: Vq=Eq[1]
In [14667]: Vq
Out[14667]: 
array([[ 0.80932593,  0.5873598 ],
       [-0.5873598 ,  0.80932593]])

In [14668]: evcq1=Vq.dot(e1)
In [14669]: evcq2=Vq.dot(e2)

In [14670]: Vinvq=np.linalg.inv(Vq)

In [14671]: Vq.dot(Lq).dot(Vinvq)
Out[14671]: 
array([[ -6.39444339,  -8.77809057],
       [ -8.77809057, -12.11921016]])
/=
In [14672]: Hq
Out[14672]: 
array([[ -6.39444339,  -8.77809057],
       [ -8.77809057, -12.11921016]])

In [14673]: Vinvq.dot(evcq1)
Out[14673]: 
array([[ 1.00000000e+00],
       [-5.55111512e-17]])
In [14674]: Vinvq.dot(evcq2)
Out[14674]: 
array([[0.],
       [1.]])

39b Λ
207b ⁻
b9 ¹ 
/ omdat HV=VΛ is H=VΛV⁻¹ , dus geeft ΛV⁻¹x de coord van Hx tov de eigenvectoren,

/ coord tov basic eigenvectoren	,
In [14675]: Lq.dot(Vinvq).dot(evcq1)
Out[14675]: 
array([[-2.38363201e-02],
       [ 1.77635684e-15]])
In [14676]: Lq.dot(Vinvq).dot(evcq2)
Out[14676]: 
array([[  0.        ],
       [-18.48981723]])
In [14677]: evaq
Out[14677]: array([ -0.02383632, -18.48981723])

/ Hoe is gradq tov basis eigenvectoren van Hq?
In [14695]: Vinvq.dot(gradq)
Out[14695]: 
array([[  1.24452447],
       [-37.47218791]])

/ Hq⁻¹(-gradq) tov V	,
In [14697]: np.linalg.inv(Lq).dot(Vinvq).dot(-gradq)
Out[14697]: 
array([[52.21126693],
       [-2.02663917]])

/ Hq⁻¹(-gradq) tov standaard basis,
In [14768]: sq=Vq.dot(np.linalg.inv(Lq)).dot(Vinvq).dot(-gradq)
/=
In [14769]: 52.21126693*evcq1+-2.02663917*evcq2
Out[14768]: 
array([[ 41.06556597],
       [-32.30701089]])

/ check
In [14773]: Hq.dot(sq)
Out[14773]: 
array([[21.00243083],
       [31.0581971 ]])

In [14779]: nsq=sq/np.linalg.norm(sq)
In [14780]: nsq
Out[14780]: 
array([[ 0.78593504],
       [-0.61830907]])

In [14793]: Hq.dot(52.21126693*evcq1)
Out[14793]: 
array([[-1.00722593],
       [ 0.73098364]])
In [14794]: Hq.dot(-2.02663917*evcq2)
Out[14794]: 
array([[22.00965672],
       [30.32721341]])
In [14795]: Hq.dot(52.21126693*evcq1)+Hq.dot(-2.02663917*evcq2)
Out[14795]: 
array([[21.00243079],
       [31.05819706]])

/ we plot	,
/ lees,
https://stackoverflow.com/questions/35363444/plotting-lines-connecting-points

/ we plot vanuit q gezien	,
In [14752]: plt.figure()
In [14754]: plt.gca().set_aspect('equal')
# -(q-p)	,
In [14753]: plt.plot([0,1.05031129],[0,1.7020124])
# evcq1
In [14743]: plt.plot([0,0.80932593],[0,-0.5873598])
# evcq2
In [14744]: plt.plot([0,0.5873598],[0,0.80932593])

# voor de plot	,
In [14779]: nsq=sq/np.linalg.norm(sq)
In [14780]: nsq
Out[14780]: 
array([[ 0.78593504],
       [-0.61830907]])
In [14781]: plt.plot([0,0.78593504],[0,-0.61830907])
Out[14781]: [<matplotlib.lines.Line2D at 0x7fbec4c1a048>]

In [14787]: Hq.dot(nsq)
Out[14787]: 
array([[0.4019559],
       [0.5944086]])
/=
In [14785]: ngradq=gradq/np.linalg.norm(sq)
In [14788]: ngradq
Out[14788]: 
array([[-0.4019559],
       [-0.5944086]])

/ plot	, 
/ we zien loodrechte eigenvectoren van Hq	,
/ evcq1 loopt naar rechtsonder	, 
/ evcq2 loopt naar rechtsboven	,
/ -gradq ligt net boven evcq2	,
/ sq list net onder evcq1	, 
/ in sq moet Hq.dot(sq) gelijk zijn aan -gradq	,  en zo is het lin vv in sq =0	,
/ sq heeft grote positieve component langs evcq1, en kleine negatieve component langs evcq2	, sq=a*evcq1 + b*evcq2	, a>>0, b<0 klein	,
/ wat is Hq.sq? evaq1 is klein neg, evaq2 groot neg, dus Hq.a*evcq1=a*-.03*evcq1 en is kleine O(1) vector langs evaq1, die naar links wijst	, en Hq.b*evcq2 = b*-17*evcq2, en omdat b<0, is dit een vector langs evcq2 naar rechtsboven	, dus dus in sq is het l vv een vector // evcq2, maar iets naar links, en is precies -gradq	.

/ 1313	 .

/ eerste 2 stappen 	,

[[-20.07532868]
 [ 14.64951956]]
[[-21.0024308 ]
 [-31.05819707]]
[[ -6.39444339  -8.77809057]
 [ -8.77809057 -12.11921016]]
[[ 41.06556592]
 [-32.30701084]]
[[1.63424829e-13]
 [2.27373675e-13]]
[ -0.02383632 -18.48981723]
[[ 0.80932593  0.5873598 ]
 [-0.5873598   0.80932593]]
[[ 20.99023724]
 [-17.65749128]]
--------------------
[[ 20.99023724]
 [-17.65749128]]
[[-13.33164173]
 [ 74.48848531]]
[[-3.53652642 -4.34723503]
 [-4.34723503 -5.38073374]]
[[-3027.47155128]
 [ 2459.81673196]]
[[ 9.45021839e-13]
 [-9.52127266e-13]]
[-0.01467603 -8.90258413]
[[ 0.7770123   0.62948542]
 [-0.62948542  0.7770123 ]]
[[-3006.48131405]
 [ 2442.15924068]]
--------------------
/home/eric/miniconda3/bin/ipython:8: RuntimeWarning: divide by zero encountered in log

/ 13	. 

/ 1313	. 

/ newton	,
/ sigmoid	,
/ regularization	,
/ log
/ lmbda <0
/ TODO

/ als 
In [14426]: beta
Out[14426]: 
array([[-3006.48131405],
       [ 2442.15924068]])


# als je 150 waarnemingen xn doet,	 dan bepaal je de beta waarvoor deze 150 waarnemingen de grootste kans hebben	,

/ 131313	 .

/ als 
In [14426]: beta
Out[14426]: 
array([[-3006.48131405],
       [ 2442.15924068]])
/ en omdat	,
In [14427]: X[:9]
Out[14427]: 
array([[1. , 0.2],
       [1. , 0.2],
       [1. , 0.2],
       [1. , 0.2],
...
/ is	, 
In [14428]: expit(X.dot(beta))
       ...: 
Out[14428]: 
array([[0.00000000e+000],
       [0.00000000e+000],
/ en zien we dus  in _ml	,
/home/eric/miniconda3/bin/ipython:8: RuntimeWarning: divide by zero encountered in log
/ Hoe vangen we deze af?
/ TODO
/ De reden is dat 1 van de eigenwaarden is ongeveer (negatief) 0 en de andere groot negatief	,


def _ml_alt(beta):
  p1gX_at_beta=expit(X.dot(beta))
	ml_vector=t*np.log(p1gX_at_beta)+(1-t)*np.log(1-p1gX_at_beta)
	return np.ones((1,ml_vector.shape[0])).dot(ml_vector)+lmbda*beta.T.dot(beta)

def _ml(beta):
  p1gX_at_beta=expit(X.dot(beta))
	return t.T.dot(np.log(p1gX_at_beta))+(1-t).T.dot(np.log(1-p1gX_at_beta))++lmbda*beta.T.dot(beta)

def _grad_ml_to_beta(beta):
  p1gX_at_beta=expit(X.dot(beta))+lmbda*beta
  return X.T.dot(t-p1gX_at_beta)		# X staat er 2 keer in	,

def _hessian_ml_to_beta(beta):  
  p1gX_at_beta=expit(X.dot(beta))
  D=np.diag((-p1gX_at_beta*(1-p1gX_at_beta)).ravel())
  return X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])

def _grad_ml_to_beta_lin(beta,delta):
	return _grad_ml_to_beta(beta)+_hessian_ml_to_beta(beta).dot(delta)

def _eig_values(m):
	return np.linalg.eig(m)[0]
def _min_abs_eig_values(m):
	return np.min(np.abs(np.linalg.eig(m)[0]))
def _eig_vectors(m):
	return np.linalg.eig(m)[1]
def _det(m):
	return np.linalg.det(m)

def _direction(v):
	return v[1]/v[0]

def _size(v):
	return np.linalg.norm(v) 

def total():
  global beta
  ml=_ml(beta)
  grad_ml_to_beta=_grad_ml_to_beta(beta)
  hessian_ml_to_beta=_hessian_ml_to_beta(beta)
  nwt=-np.linalg.inv(hessian_ml_to_beta).dot(grad_ml_to_beta)
  check_if_grad_ml_lin_is_zero=_grad_ml_to_beta_lin(beta,nwt)
  print(beta)
  print(grad_ml_to_beta)
  print(hessian_ml_to_beta)
  print(nwt)
  print(check_if_grad_ml_lin_is_zero)
  print(_eig_values(hessian_ml_to_beta))
  print(_eig_vectors(hessian_ml_to_beta))
  beta=beta+nwt
  print(beta)
  print("--------------------")


X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
a=-21.12563996
b=12.94750716
beta=np.array([a,b]).reshape(-1,1)
beta=np.array([[a+0.8510062],[b-0.52515564]]).reshape(-1,1)
beta=np.array([[a+0.52515564],[b+0.8510062]]).reshape(-1,1) 
beta=np.array([[a+2*0.52515564],[b+2*0.8510062]]).reshape(-1,1)  
# singular hessian als λ=0, niet singular als λ=-1
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=0
for i in range(0,10):
  total()


/ unicodes	,
3bb λ
39b Λ
3bc μ
221a √
b2 ²
b3 ³
b9 ¹
207b ⁻

/ 1313	. 

/ zonder total() fct	,


X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([-21,12]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=0
for i in range(0,10):
	ml=_ml(beta)
  grad_ml_to_beta=_grad_ml_to_beta(beta)
	hessian_ml_to_beta=_hessian_ml_to_beta(beta)
	nwt=-np.linalg.inv(hessian_ml_to_beta).dot(grad_ml_to_beta)
	check_if_grad_ml_lin_is_zero=_grad_ml_to_beta_lin(beta,nwt)
	print(beta)
	print(grad_ml_to_beta)
  print(hessian_ml_to_beta)
	print(nwt)
	print(check_if_grad_ml_lin_is_zero)
  print(_eig_values(hessian_ml_to_beta))
  print(_eig_vectors(hessian_ml_to_beta))
  if (grad_ml_to_beta == np.zeros(beta.shape[0])).all():
    break
	beta=beta+nwt
	print(beta)
	print("--------------------")

ml=_ml(beta)
grad_ml_to_beta=_grad_ml_to_beta(beta)
hessian_ml_to_beta=_hessian_ml_to_beta(beta)
nwt=-np.linalg.inv(hessian_ml_to_beta).dot(grad_ml_to_beta)
check_if_grad_ml_lin_is_zero=_grad_ml_to_beta_lin(beta,nwt)
print(beta)
print(grad_ml_to_beta)
print(hessian_ml_to_beta)
print(nwt)
print(check_if_grad_ml_lin_is_zero)
print(_eig_values(hessian_ml_to_beta))
print(_eig_vectors(hessian_ml_to_beta))
beta=beta+nwt
print(beta)
print("--------------------")

In [14045]: beta=np.array([[a],[b]])
[[-21.12563996]
 [ 12.94750716]]
[[-4.86671210e-08]
 [-7.87996095e-08]]
[[ -4.8015161   -7.72519407]
 [ -7.72519407 -12.55285302]]
[[-3.65078583e-09]
 [-4.03068373e-09]]
[[9.79381845e-22]
 [1.58818678e-21]]
[ -0.0343014  -17.32006772]
[[ 0.8510062   0.52515564]
 [-0.52515564  0.8510062 ]]
[[-21.12563996]
 [ 12.94750716]]
--------------------

In [14049]: beta=np.array([[a+0.8510062],[b-0.52515564]])
 [ 12.42235152]]
[[-0.02950951]
 [ 0.0213212 ]]
[[-0.8044572 ]
 [ 0.49627096]]
[[4.4408921e-16]
 [8.8817842e-16]]
[ -0.03841331 -18.33092609]
[[ 0.85116205  0.524903  ]
 [-0.524903    0.85116205]]
[[-21.07909096]
 [ 12.91862248]]
--------------------

In [14047]: beta=np.array([[a+0.52515564],[b+0.8510062]])
[[-20.60048432]
 [ 13.79851336]]
[[ -9.93540995]
 [-15.26636816]]
[[ -6.15281497  -9.04278227]
 [ -9.04278227 -13.37175639]]
[[10.35106068]
 [-8.14169455]]
[[2.66453526e-13]
 [3.94351218e-13]]
[ -0.02574677 -19.49882459]
[[ 0.82786289  0.5609305 ]
 [-0.5609305   0.82786289]]
[[-10.24942364]
 [  5.65681881]]
--------------------

In [14107]: Hessian_of_ml_to_beta=np.array([[ -6.15281497 , -9.04278227]
       ...: , [ -9.04278227, -13.37175639]])

In [14117]: np.linalg.eig(Hessian_of_ml_to_beta)
Out[14117]: 
(array([ -0.02574677, -19.49882459]), array([[ 0.82786289,  0.5609305 ],
        [-0.5609305 ,  0.82786289]]))

In [14118]: V=np.array([[ 0.82786289,  0.5609305 ],[-0.5609305 ,  0.82786289]])

In [14119]: V
Out[14119]: 
array([[ 0.82786289,  0.5609305 ],
       [-0.5609305 ,  0.82786289]])

In [14120]: Vinv=np.linalg.inv(V)

In [14121]: Vinv
Out[14121]: 
array([[ 0.8278629 , -0.56093051],
       [ 0.56093051,  0.8278629 ]])

In [14122]: Lmbda=np.diag([ -0.02574677, -19.49882459])

In [14123]: Lmbda
Out[14123]: 
array([[ -0.02574677,   0.        ],
       [  0.        , -19.49882459]])

In [14124]: V.dot(Lmbda).dot(Vinv)
Out[14124]: 
array([[ -6.15281496,  -9.04278226],
       [ -9.04278226, -13.3717564 ]])

In [14128]: nwt
Out[14128]: 
array([[10.35106068],
       [-8.14169455]])

In [14129]: Vinv.dot(nwt)
Out[14129]: 
array([[13.13618393],
       [-0.93398115]])
In [14130]: Lmbda.dot(Vinv).dot(nwt)
Out[14130]: 
array([[-0.33821431],
       [18.21153453]])
In [14132]: V.dot(Lmbda).dot(Vinv).dot(nwt)
Out[14132]: 
array([[ 9.9354101 ],
       [15.26636833]])

/ ook	,

In [14145]: hessian_ml_to_beta
Out[14145]: 
array([[ -6.15281497,  -9.04278227],
       [ -9.04278227, -13.37175639]])

In [14146]: nwt
Out[14146]: 
array([[10.35106068],
       [-8.14169455]])

In [14147]: hessian_ml_to_beta.dot(nwt)
Out[14147]: 
array([[ 9.93540995],
       [15.26636816]])

/ ook,

/ ontbind nwt in eigv	,
/ nwt in V basis	,
In [14152]: y=Vinv.dot(nwt)
y
Out[14152]: 
array([[13.13618393],
       [-0.93398115]])

In [14183]: Lmbda
Out[14183]: 
array([[ -0.02574677,   0.        ],
       [  0.        , -19.49882459]])

In [14184]: z=Lmbda.dot(y)
z
Out[14184]: 
array([[-0.33821431],
       [18.21153453]])

In [14186]: V.dot(z)
Out[14186]: 
array([[ 9.9354101 ],
       [15.26636833]])


/ 1313	. 

/ plot	, 

def _total(beta):
	for i in range(0,10):
		path[i]=beta.ravel()
		ml=_ml(beta)
	  grad_ml_to_beta=_grad_ml_to_beta(beta)
		hessian_ml_to_beta=_hessian_ml_to_beta(beta)
		nwt=-np.linalg.inv(hessian_ml_to_beta).dot(grad_ml_to_beta)
		check_if_grad_ml_lin_is_zero=_grad_ml_to_beta_lin(beta,nwt)
		print(beta)
		print(grad_ml_to_beta)
	  print(hessian_ml_to_beta)
		print(nwt)
		print(check_if_grad_ml_lin_is_zero)
	  print(_eig_values(hessian_ml_to_beta))
	  print(_eig_vectors(hessian_ml_to_beta))
	  if (grad_ml_to_beta == np.zeros(beta.shape[0])).all():
	    break
		beta=beta+nwt
		print(beta)
		print("--------------------")


X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=0
a=-21.12563996
b=12.94750716
_max=np.array([[a],[b]])
v1=np.array([[ 0.82786289], [-0.5609305 ]])
v2=np.array([[0.5609305 ], [0.82786289]])
plt.figure()

path=np.zeros((10,2))
beta=_max+v1
_total(beta)
plt.plot(path[:,0],path[:,1])

path=np.zeros((10,2))
beta=_max+v2
_total(beta)
plt.plot(path[:,0],path[:,1])

path=np.zeros((10,2))
beta=_max-v2
_total(beta)
plt.plot(path[:,0],path[:,1])

# singular matrix
path=np.zeros((10,2))
beta=_max-2*v2
_total(beta)
plt.plot(path[:,0],path[:,1])

# singular matrix
path=np.zeros((10,2))
beta=_max+2*v2
_total(beta)
plt.plot(path[:,0],path[:,1])

/ 1313	 

/ maak contour	,

/ we maken een 441x2 array	,
/ elk elem is een (1,2)	, 
/ _ml verwacht een (2,1)
/ daarom _ml2	,

def _ml2(beta):
	return _ml(beta.reshape(-1,1))

In [14303]: _ml2(beta.reshape(1,-1))
Out[14303]: array([[-25.60583895]])

x=np.linspace(a-2,a+2,21)
y=np.linspace(b-2,b+2,21)
g1,g2=np.meshgrid(x,y)
g=np.c_[g1.ravel(),g2.ravel()]

In [14314]: _ml2(g[0])
Out[14314]: array([[-97.09921453]])

In [14316]: _ml2(g)
ValueError: shapes (150,2) and (882,1) not aligned: 2 (dim 1) != 882 (dim 0)
/ TODO Afmaken	,

/ Einde NEWTON NAAR ONEINDIG

/ NEWTON NAAR ONEINDIG 2 FEATURES

/ we copy de fcts	,

def _ml_alt(beta):
  p1gX_at_beta=expit(X.dot(beta))
	ml_vector=t*np.log(p1gX_at_beta)+(1-t)*np.log(1-p1gX_at_beta)
	return np.ones((1,ml_vector.shape[0])).dot(ml_vector)+lmbda*beta.T.dot(beta)

def _ml(beta):
  p1gX_at_beta=expit(X.dot(beta))
	return t.T.dot(np.log(p1gX_at_beta))+(1-t).T.dot(np.log(1-p1gX_at_beta))++lmbda*beta.T.dot(beta)

def _grad_ml_to_beta(beta):
  p1gX_at_beta=expit(X.dot(beta))+lmbda*beta
  return X.T.dot(t-p1gX_at_beta)		# X staat er 2 keer in	,

def _hessian_ml_to_beta(beta):  
  p1gX_at_beta=expit(X.dot(beta))
  D=np.diag((-p1gX_at_beta*(1-p1gX_at_beta)).ravel())
  return X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])

def _grad_ml_to_beta_lin(beta,delta):
	return _grad_ml_to_beta(beta)+_hessian_ml_to_beta(beta).dot(delta)

def _eig_values(m):
	return np.linalg.eig(m)[0]
def _min_abs_eig_values(m):
	return np.min(np.abs(np.linalg.eig(m)[0]))
def _eig_vectors(m):
	return np.linalg.eig(m)[1]
def _det(m):
	return np.linalg.det(m)

def _direction(v):
 	return v[1]/v[0]

def _size(v):
	return np.linalg.norm(v) 

def total():
  global beta
  ml=_ml(beta)
  grad_ml_to_beta=_grad_ml_to_beta(beta)
  hessian_ml_to_beta=_hessian_ml_to_beta(beta)
  global h						# om de hessian te pakken	,
  h=hessian_ml_to_beta
  global e 						# om de eig te pakken	,
  e=np.linalg.eig(hessian_ml_to_beta)
  nwt=-np.linalg.inv(hessian_ml_to_beta).dot(grad_ml_to_beta)
  check_if_grad_ml_lin_is_zero=_grad_ml_to_beta_lin(beta,nwt)
  print(beta)
  print(grad_ml_to_beta)
  print(hessian_ml_to_beta)
  print(nwt)
  print(check_if_grad_ml_lin_is_zero)
  print(_eig_values(hessian_ml_to_beta))
  print(_eig_vectors(hessian_ml_to_beta))
  beta=beta+nwt
  print(beta)
  print("--------------------")


X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([0,0,0]).reshape(-1,1)
beta=p+evc1p
# singular hessian als λ=0, niet singular als λ=-1
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=-1
h=0	# om de hessian te pakken	,
e=0 # om de eig te pakken	,
for i in range(0,10):
  total()

/ start met 0,0,0		, dan laatste frame	,

--------------------
[[-45.2685563 ]
 [  5.75398881]
 [ 10.44602654]]
[[-8.03220288e-05]
 [-3.00670505e-04]
 [-9.12751521e-05]]
[[ -3.01681996 -14.78627212  -4.91471535]
 [-14.78627212 -72.66365494 -24.07153134]
 [ -4.91471535 -24.07153134  -8.07897298]]
[[-0.00378699]
 [ 0.00054344]
 [ 0.00067327]]
[[-1.73472348e-18]
 [ 0.00000000e+00]
 [ 1.73472348e-18]]
[-8.36589650e+01 -5.15069180e-03 -9.53321981e-02]
[[-0.18971096 -0.97647328  0.10251679]
 [-0.93190288  0.14620281 -0.33193638]
 [-0.30913876  0.15850766  0.93771454]]
[[-45.27234329]
 [  5.75453225]
 [ 10.44669981]]
--------------------

/ we zien net zo iets als bij 1 feature: 1 grote eigenwaarde	,

In [14830]: evalp=e[0]
In [14831]: evalp
Out[14831]: array([-8.36589650e+01, -5.15069180e-03, -9.53321981e-02])

In [14822]: evc1p=e[1][:,0].reshape(-1,1)
In [14828]: evc1p
Out[14828]: 
array([[-0.18971096],
       [-0.93190288],
       [-0.30913876]])

In [14833]: p=beta
In [14834]: p
Out[14834]: 
array([[-45.27234329],
       [  5.75453225],
       [ 10.44669981]])

/ als we start met p+evc1p	, dan weer ERR	,
/ neem daarom lmbda=-1	,

--------------------
[[-44.43141175]
 [  5.68535817]
 [ 10.13921646]]
[[-0.00280051]
 [-0.00508894]
 [ 0.01712558]]
[[ -4.10013326 -15.18689482  -5.05320038]
 [-15.18689482 -75.59379477 -24.74024704]
 [ -5.05320038 -24.74024704  -9.31245201]]
[[-0.0029317 ]
 [-0.00460335]
 [ 0.01565946]]
[[ 0.00000000e+00]
 [ 0.00000000e+00]
 [-2.08166817e-17]]
[-86.90369772  -1.00544718  -1.09723514]
[[-0.18978211 -0.97700179  0.09721242]
 [-0.93178126  0.14801614 -0.33147384]
 [-0.30946153  0.15348852  0.93844277]]
[[-44.43434345]
 [  5.68075482]
 [ 10.15487592]]



/ unicodes	,
3bb λ
39b Λ
3bc μ
221a √
b2 ²
b3 ³
b9 ¹
207b ⁻

/ NEWTON 2 CLASSES 2 FEATURES

/ p(1|x)=exp(x.T.dot(alpha))/(exp(x.T.dot(alpha))+exp(x.T.dot(beta))+exp(x.T.dot(gamma)))
/ p(2|x)=exp(x.T.dot(beta))/(exp(x.T.dot(alpha))+exp(x.T.dot(beta))+exp(x.T.dot(gamma)))
/ p(3|x)=exp(x.T.dot(gamma))/(exp(x.T.dot(alpha))+exp(x.T.dot(beta))+exp(x.T.dot(gamma)))
/ maar we delen in alle 3 de kansen de teller en noemer door exp(x.T.dot(alpha))	,
/ dan met
delta=alpha-beta, epsilon=alpha-gamma	, zien we - tekens in exp(-x.T.dot(delta)) en exp(-x.T.dot(epsilon))

/ versie met - in de exponenten	,

def _p1gX(beta,gamma):
	return 1/(1+np.exp(-X.dot(beta))+np.exp(-X.dot(gamma)))
def _p2gX(beta,gamma):
	return np.exp(-X.dot(beta))/(1+np.exp(-X.dot(beta))+np.exp(-X.dot(gamma)))
def _p3gX(beta,gamma):
	return np.exp(-X.dot(gamma))/(1+np.exp(-X.dot(beta))+np.exp(-X.dot(gamma)))

def _ml(alpha):
	beta=alpha[:3]
	gamma=alpha[3:6]
	return (1-s.T-t.T).dot(np.log(_p1gX(beta,gamma)))+s.T.dot(np.log(_p2gX(beta,gamma)))+t.T.dot(np.log(_p3gX(beta,gamma)))+lmbda*(beta.T.dot(beta)+gamma.T.dot(gamma))/2

def _grad_ml_to_beta(beta,gamma):
  return X.T.dot(_p2gX(beta,gamma)-s)+lmbda*beta

def _grad_ml_to_gamma(beta,gamma):
  return X.T.dot(_p3gX(beta,gamma)-t)+lmbda*gamma

def _grad_ml_to_alpha(alpha):
	beta=alpha[:3]
	gamma=alpha[3:6]
  return np.r_[_grad_ml_to_beta(beta,gamma),_grad_ml_to_gamma(beta,gamma)]		# X staat er 2 keer in	, 

def _hessian_ml_to_beta_T_beta(beta,gamma):  
  D=np.diag((-_p2gX(beta,gamma)*(1-_p2gX(beta,gamma))).ravel())
  return X.T.dot(D).dot(X)			+lmbda*np.eye(beta.shape[0])

def _hessian_ml_to_gamma_T_beta(beta,gamma):  
  D=np.diag((_p2gX(beta,gamma)*_p3gX(beta,gamma)).ravel())
  return X.T.dot(D).dot(X)	

def _hessian_ml_to_beta_T_gamma(beta,gamma):  
  D=np.diag((_p3gX(beta,gamma)*_p2gX(beta,gamma)).ravel())
  return X.T.dot(D).dot(X)		

def _hessian_ml_to_gamma_T_gamma(beta,gamma):  
  D=np.diag((-_p3gX(beta,gamma)*(1-_p3gX(beta,gamma))).ravel())
  return X.T.dot(D).dot(X)			+lmbda*np.eye(gamma.shape[0])

def _hessian_ml_to_alpha_T_alpha(alpha):
	beta=alpha[:3]
	gamma=alpha[3:6]
	return np.block([
		[_hessian_ml_to_beta_T_beta(beta,gamma),_hessian_ml_to_gamma_T_beta(beta,gamma)],
		[_hessian_ml_to_beta_T_gamma(beta,gamma),_hessian_ml_to_gamma_T_gamma(beta,gamma)]])

def _grad_ml_to_alpha_lin(alpha,delta):
	beta=alpha[:3]
	gamma=alpha[3:6]
	return _grad_ml_to_alpha(alpha)+_hessian_ml_to_alpha_T_alpha(alpha).dot(delta)

def _eig_values(m):
	return np.linalg.eig(m)[0]
def _min_abs_eig_values(m):
	return np.min(np.abs(np.linalg.eig(m)[0]))
def _eig_vectors(m):
	return np.linalg.eig(m)[1]
def _det(m):
	return np.linalg.det(m)

def _direction(v):
 	return v[1]/v[0]

def _size(v):
	return np.linalg.norm(v) 


def total():
  global alpha 
  ml=_ml(alpha)
  grad_ml_to_alpha=_grad_ml_to_alpha(alpha)
  hessian_ml_to_alpha_T_alpha=_hessian_ml_to_alpha_T_alpha(alpha)
  global h						# om de hessian te pakken	,
  h=hessian_ml_to_alpha_T_alpha
  global e 						# om de eig te pakken	,
  e=np.linalg.eig(hessian_ml_to_alpha_T_alpha)
  nwt=-np.linalg.inv(hessian_ml_to_alpha_T_alpha).dot(grad_ml_to_alpha)
  grad_ml_lin_is_zero=_grad_ml_to_alpha_lin(alpha,nwt)
  print(alpha)
  print(grad_ml_to_alpha)
  print(hessian_ml_to_alpha_T_alpha)
  print(nwt)
  print(grad_ml_lin_is_zero)
  print(_eig_values(hessian_ml_to_alpha_T_alpha))
  print(_eig_vectors(hessian_ml_to_alpha_T_alpha))
  alpha=alpha+nwt
  print(alpha)
  print("--------------------")


X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([0,0,0]).reshape(-1,1)
gamma=np.array([0,0,0]).reshape(-1,1)
alpha=np.r_[beta,gamma]
# beta=alpha[:3]
# gamma=alpha[3:6]
#beta=p+evc1p
# singular hessian als λ=0, niet singular als λ=-1
s=(iris['target']==1).reshape(-1,1).astype(int)
t=(iris['target']==2).reshape(-1,1).astype(int)
# evt np_c[s,t]
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=0
h=0	# om de hessian te pakken	,
e=0 # om de eig te pakken	,
for i in range(0,100):
  total()

ml=_ml(alpha)
grad_ml_to_alpha=_grad_ml_to_alpha(alpha)
hessian_ml_to_alpha_T_alpha=_hessian_ml_to_alpha_T_alpha(alpha)
e=np.linalg.eig(hessian_ml_to_alpha_T_alpha)
nwt=-np.linalg.inv(hessian_ml_to_alpha_T_alpha).dot(grad_ml_to_alpha)
grad_ml_lin_is_zero=_grad_ml_to_alpha_lin(alpha,nwt)
print(alpha)
print(grad_ml_to_alpha)
print(hessian_ml_to_alpha_T_alpha)
print(nwt)
print(grad_ml_lin_is_zero)
print(_eig_values(hessian_ml_to_alpha_T_alpha))
print(_eig_vectors(hessian_ml_to_alpha_T_alpha))
alpha=alpha+nwt
print(alpha)
print("--------------------")

/ unicodes	,
3bb λ
39b Λ
3bc μ
221a √
b2 ²
b3 ³
b9 ¹
207b ⁻


/ Einde NEWTON 2 CLASSES 2 FEATURES

/ NEWTON 2 CLASSES 2 FEATURES

/ p(1|x)=exp(x.T.dot(w1))/(exp(x.T.dot(w1))+exp(x.T.dot(w2))+exp(x.T.dot(w3)))
/ p(2|x)=exp(x.T.dot(w2))/(exp(x.T.dot(w1))+exp(x.T.dot(w2))+exp(x.T.dot(w3)))
/ p(3|x)=exp(x.T.dot(w3))/(exp(x.T.dot(w1))+exp(x.T.dot(w2))+exp(x.T.dot(w3)))
/ maar we delen in alle 3 de kansen de teller en noemer door exp(x.T.dot(w1))	,
/ dan met
v2=w2-w1, v3=w3-w1	, zien we + tekens in exp(x.T.dot(v2)) en exp(x.T.dot(v3))

/ versie met + in exponenten	,

def _p1gX(v2,v3):
	return 1/(1+np.exp(X.dot(v2))+np.exp(X.dot(v3)))
def _p2gX(v2,v3):
	return np.exp(X.dot(v2))/(1+np.exp(X.dot(v2))+np.exp(X.dot(v3)))
def _p3gX(v2,v3):
	return np.exp(X.dot(v3))/(1+np.exp(X.dot(v2))+np.exp(X.dot(v3)))

def _ml(v):
	v2=v[:3]
	v3=v[3:6]
	return (1-t2.T-t3.T).dot(np.log(_p1gX(v2,v3)))+t2.T.dot(np.log(_p2gX(v2,v3)))+t3.T.dot(np.log(_p3gX(v2,v3)))+lmbda*(v2.T.dot(v2)+v3.T.dot(v3))/2

def _grad_ml_to_v2(v2,v3):
  return X.T.dot(t2-_p2gX(v2,v3))+lmbda*v2

def _grad_ml_to_v3(v2,v3):
  return X.T.dot(t3-_p3gX(v2,v3))+lmbda*v3

def _grad_ml_to_v(v):
	v2=v[:3]
	v3=v[3:6]
  return np.r_[_grad_ml_to_v2(v2,v3),_grad_ml_to_v3(v2,v3)]		# X staat er 2 keer in	, 

def _hessian_ml_to_v2_T_v2(v2,v3):  
  D=np.diag((-_p2gX(v2,v3)*(1-_p2gX(v2,v3))).ravel())
  return X.T.dot(D).dot(X)			+lmbda*np.eye(v2.shape[0])

def _hessian_ml_to_v3_T_v2(v2,v3):  
  D=np.diag((_p2gX(v2,v3)*_p3gX(v2,v3)).ravel())
  return X.T.dot(D).dot(X)	

def _hessian_ml_to_v2_T_v3(v2,v3):  
  D=np.diag((_p3gX(v2,v3)*_p2gX(v2,v3)).ravel())
  return X.T.dot(D).dot(X)		

def _hessian_ml_to_v3_T_v3(v2,v3):  
  D=np.diag((-_p3gX(v2,v3)*(1-_p3gX(v2,v3))).ravel())
  return X.T.dot(D).dot(X)			+lmbda*np.eye(v3.shape[0])

def _hessian_ml_to_v_T_v(v):
	v2=v[:3]
	v3=v[3:6]
	return np.block([
		[_hessian_ml_to_v2_T_v2(v2,v3),_hessian_ml_to_v3_T_v2(v2,v3)],
		[_hessian_ml_to_v2_T_v3(v2,v3),_hessian_ml_to_v3_T_v3(v2,v3)]])

def _grad_ml_to_v_lin(v,delta):
	v2=v[:3]
	v3=v[3:6]
	return _grad_ml_to_v(v)+_hessian_ml_to_v_T_v(v).dot(delta)

def _eig_values(m):
	return np.linalg.eig(m)[0]
def _min_abs_eig_values(m):
	return np.min(np.abs(np.linalg.eig(m)[0]))
def _eig_vectors(m):
	return np.linalg.eig(m)[1]
def _det(m):
	return np.linalg.det(m)

def _direction(v):
 	return v[1]/v[0]

def _size(v):
	return np.linalg.norm(v) 

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
v2=np.array([0,0,0]).reshape(-1,1)
v3=np.array([0,0,0]).reshape(-1,1)
v=np.r_[v2,v3]
# beta=alpha[:3]
# gamma=alpha[3:6]
#beta=p+evc1p
# singular hessian als λ=0, niet singular als λ=-1
t2=(iris['target']==1).reshape(-1,1).astype(int)
t3=(iris['target']==2).reshape(-1,1).astype(int)
# evt np_c[s,t]
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=0
h=0	# om de hessian te pakken	,
e=0 # om de eig te pakken	,
for i in range(0,100):
  total()

ml=_ml(v)
grad_ml_to_v=_grad_ml_to_v(v)
hessian_ml_to_v_T_v=_hessian_ml_to_v_T_v(v)
e=np.linalg.eig(hessian_ml_to_v_T_v)
nwt=-np.linalg.inv(hessian_ml_to_v_T_v).dot(grad_ml_to_v)
grad_ml_lin_is_zero=_grad_ml_to_v_lin(v,nwt)
print(v)
print(grad_ml_to_v)
print(hessian_ml_to_v_T_v)
print(nwt)
print(grad_ml_lin_is_zero)
print(_eig_values(hessian_ml_to_v_T_v))
print(_eig_vectors(hessian_ml_to_v_T_v))
v=v+nwt
print(v)
print("--------------------")

/ TODO:
def total():
  global alpha 
  ml=_ml(alpha)
  grad_ml_to_alpha=_grad_ml_to_alpha(alpha)
  hessian_ml_to_alpha_T_alpha=_hessian_ml_to_alpha_T_alpha(alpha)
  global h						# om de hessian te pakken	,
  h=hessian_ml_to_alpha_T_alpha
  global e 						# om de eig te pakken	,
  e=np.linalg.eig(hessian_ml_to_alpha_T_alpha)
  nwt=-np.linalg.inv(hessian_ml_to_alpha_T_alpha).dot(grad_ml_to_alpha)
  grad_ml_lin_is_zero=_grad_ml_to_alpha_lin(alpha,nwt)
  print(alpha)
  print(grad_ml_to_alpha)
  print(hessian_ml_to_alpha_T_alpha)
  print(nwt)
  print(grad_ml_lin_is_zero)
  print(_eig_values(hessian_ml_to_alpha_T_alpha))
  print(_eig_vectors(hessian_ml_to_alpha_T_alpha))
  alpha=alpha+nwt
  print(alpha)
  print("--------------------")



/ unicodes	,
3bb λ
39b Λ
3bc μ
221a √
b2 ²
b3 ³
b9 ¹
207b ⁻


/ Einde NEWTON 2 CLASSES 2 FEATURES

/ NEWTON 3 CLASSES 2 FEATURES


/ p(1|x)=exp(x.T.dot(alpha))/(exp(x.T.dot(alpha))+exp(x.T.dot(beta))+exp(x.T.dot(gamma)))
/ p(2|x)=exp(x.T.dot(beta))/(exp(x.T.dot(alpha))+exp(x.T.dot(beta))+exp(x.T.dot(gamma)))
/ p(3|x)=exp(x.T.dot(gamma))/(exp(x.T.dot(alpha))+exp(x.T.dot(beta))+exp(x.T.dot(gamma)))

/ we laten ze zo staan	,

/ we copy de fcts	,

def _p1gX(alpha,beta,gamma):
	return np.exp(X.dot(alpha))/(np.exp(X.dot(alpha))+np.exp(X.dot(beta))+np.exp(X.dot(gamma)))
def _p2gX(alpha,beta,gamma):
	return np.exp(X.dot(beta))/(np.exp(X.dot(alpha))+np.exp(X.dot(beta))+np.exp(X.dot(gamma)))
def _p3gX(alpha,beta,gamma):
	return np.exp(X.dot(gamma))/(np.exp(X.dot(alpha))+np.exp(X.dot(beta))+np.exp(X.dot(gamma)))

def _ml(epsilon):
	alpha=epsilon[:3]
	beta=epsilon[3:6]
	gamma=epsilon[6:9]
	return (1-s.T-t.T).dot(np.log(_p1gX(alpha,beta,gamma)))+s.T.dot(np.log(_p2gX(alpha,beta,gamma)))+t.T.dot(np.log(_p3gX(alpha,beta,gamma)))         # +lmbda*beta.T.dot(beta)

def _grad_ml_to_alpha(alpha,beta,gamma):
  return X.T.dot(1-s-t-_p1gX(alpha,beta,gamma))		# X staat er 2 keer in	,

def _grad_ml_to_beta(alpha,beta,gamma):
  return X.T.dot(s-_p2gX(alpha,beta,gamma))		# X staat er 2 keer in	,
 																					#+lmbda*beta
def _grad_ml_to_gamma(alpha,beta,gamma):
  return X.T.dot(t-_p3gX(alpha,beta,gamma))		# X staat er 2 keer in	,
 																					#+lmbda*beta
def _grad_ml_to_epsilon(epsilon):
	alpha=epsilon[:3]
	beta=epsilon[3:6]
	gamma=epsilon[6:9]
  return np.r_[_grad_ml_to_alpha(alpha,beta,gamma),_grad_ml_to_beta(alpha,beta,gamma),_grad_ml_to_gamma(alpha,beta,gamma)]		# X staat er 2 keer in	, #+lmbda*beta

def _hessian_ml_to_alpha_T_alpha(alpha,beta,gamma):  
  D=np.diag((-_p1gX(alpha,beta,gamma)*(1-_p1gX(alpha,beta,gamma))).ravel())
  return X.T.dot(D).dot(X)			#+lmbda*np.eye(beta.shape[0])

def _hessian_ml_to_alpha_T_alpha_alt(alpha,beta,gamma):  
	D=np.diag(-_p1gX(alpha,beta,gamma).ravel()).dot(np.diag((1-_p1gX(alpha,beta,gamma)).ravel()))
  return X.T.dot(D).dot(X)			#+lmbda*np.eye(beta.shape[0])

def _hessian_ml_to_beta_T_alpha(alpha,beta,gamma):  
  D=np.diag((_p1gX(alpha,beta,gamma)*_p2gX(alpha,beta,gamma)).ravel())
  return X.T.dot(D).dot(X)			#+lmbda*np.eye(beta.shape[0])

def _hessian_ml_to_gamma_T_alpha(alpha,beta,gamma):  
  D=np.diag((_p1gX(alpha,beta,gamma)*_p3gX(alpha,beta,gamma)).ravel())
  return X.T.dot(D).dot(X)			#+lmbda*np.eye(beta.shape[0])

def _hessian_ml_to_alpha_T_beta(alpha,beta,gamma):  
  D=np.diag((_p2gX(alpha,beta,gamma)*_p1gX(alpha,beta,gamma)).ravel())
  return X.T.dot(D).dot(X)			#+lmbda*np.eye(beta.shape[0])

def _hessian_ml_to_beta_T_beta(alpha,beta,gamma):  
  D=np.diag((-_p2gX(alpha,beta,gamma)*(1-_p2gX(alpha,beta,gamma))).ravel())
  return X.T.dot(D).dot(X)			#+lmbda*np.eye(beta.shape[0])

def _hessian_ml_to_gamma_T_beta(alpha,beta,gamma):  
  D=np.diag((_p2gX(alpha,beta,gamma)*_p3gX(alpha,beta,gamma)).ravel())
  return X.T.dot(D).dot(X)			#+lmbda*np.eye(beta.shape[0])

def _hessian_ml_to_alpha_T_gamma(alpha,beta,gamma):  
  D=np.diag((_p3gX(alpha,beta,gamma)*_p1gX(alpha,beta,gamma)).ravel())
  return X.T.dot(D).dot(X)			#+lmbda*np.eye(beta.shape[0])

def _hessian_ml_to_beta_T_gamma(alpha,beta,gamma):  
  D=np.diag((_p3gX(alpha,beta,gamma)*_p2gX(alpha,beta,gamma)).ravel())
  return X.T.dot(D).dot(X)			#+lmbda*np.eye(beta.shape[0])

def _hessian_ml_to_gamma_T_gamma(alpha,beta,gamma):  
  D=np.diag((-_p3gX(alpha,beta,gamma)*(1-_p3gX(alpha,beta,gamma))).ravel())
  return X.T.dot(D).dot(X)			#+lmbda*np.eye(beta.shape[0])

def _hessian_ml_to_epsilon_T_epsilon(epsilon):
	alpha=epsilon[:3]
	beta=epsilon[3:6]
	gamma=epsilon[6:9]
	return np.block([
		[_hessian_ml_to_alpha_T_alpha(alpha,beta,gamma),_hessian_ml_to_beta_T_alpha(alpha,beta,gamma),_hessian_ml_to_gamma_T_alpha(alpha,beta,gamma)],
		[_hessian_ml_to_alpha_T_beta(alpha,beta,gamma),_hessian_ml_to_beta_T_beta(alpha,beta,gamma),_hessian_ml_to_gamma_T_beta(alpha,beta,gamma)],
		[_hessian_ml_to_alpha_T_gamma(alpha,beta,gamma),_hessian_ml_to_beta_T_gamma(alpha,beta,gamma),_hessian_ml_to_gamma_T_gamma(alpha,beta,gamma)],
	])

def _grad_ml_to_epsilon_lin(epsilon,delta):
	alpha=epsilon[:3]
	beta=epsilon[3:6]
	gamma=epsilon[6:9]
	return _grad_ml_to_epsilon(epsilon)+_hessian_ml_to_epsilon_T_epsilon(epsilon).dot(delta)

def _eig_values(m):
	return np.linalg.eig(m)[0]
def _min_abs_eig_values(m):
	return np.min(np.abs(np.linalg.eig(m)[0]))
def _eig_vectors(m):
	return np.linalg.eig(m)[1]
def _det(m):
	return np.linalg.det(m)

def _direction(v):
 	return v[1]/v[0]

def _size(v):
	return np.linalg.norm(v) 

def total():
  global alpha 
  ml=_ml(epsilon)
  grad_ml_to_epsilon=_grad_ml_to_epsilon(epsilon)
  hessian_ml_to_epsilon_T_epsilon=_hessian_ml_to_epsilon_T_epsilon(epsilon)
  global h						# om de hessian te pakken	,
  h=hessian_ml_to_epsilon_T_epsilon
  global e 						# om de eig te pakken	,
  e=np.linalg.eig(hessian_ml_to_epsilon_T_epsilon)
  nwt=-np.linalg.inv(hessian_ml_to_epsilon_T_epsilon).dot(grad_ml_to_epsilon)
  grad_ml_lin_is_zero=_grad_ml_to_epsilon_lin(epsilon,nwt)
  print(epsilon)
  print(grad_ml_to_epsilon)
  print(hessian_ml_to_epsilon_T_epsilon)
  print(nwt)
  print(grad_ml_lin_is_zero)
  print(_eig_values(hessian_ml_to_epsilon_T_epsilon))
  print(_eig_vectors(hessian_ml_to_epsilon_T_epsilon))
  epsilon=epsilon+nwt
  print(epsilon)
  print("--------------------")


X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
alpha=np.array([0,0,0]).reshape(-1,1)
beta=np.array([0,0,0]).reshape(-1,1)
gamma=np.array([0,0,0]).reshape(-1,1)
epsilon=np.r_[alpha,beta,gamma]
# beta=alpha[:3]
# gamma=alpha[3:6]
#beta=p+evc1p
# singular hessian als λ=0, niet singular als λ=-1
s=(iris['target']==1).reshape(-1,1).astype(int)
t=(iris['target']==2).reshape(-1,1).astype(int)
# evt np_c[s,t]
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=0
#lmbda=-1
h=0	# om de hessian te pakken	,
e=0 # om de eig te pakken	,
for i in range(0,100):
  total()

ml=_ml(epsilon)
grad_ml_to_epsilon=_grad_ml_to_epsilon(epsilon)
hessian_ml_to_epsilon_T_epsilon=_hessian_ml_to_epsilon_T_epsilon(epsilon)
e=np.linalg.eig(hessian_ml_to_epsilon_T_epsilon)
nwt=-np.linalg.inv(hessian_ml_to_epsilon_T_epsilon).dot(grad_ml_to_epsilon)
grad_ml_lin_is_zero=_grad_ml_to_epsilon_lin(epsilon,nwt)
print(epsilon)
print(grad_ml_to_epsilon)
print(hessian_ml_to_epsilon_T_epsilon)
print(nwt)
print(grad_ml_lin_is_zero)
print(_eig_values(hessian_ml_to_epsilon_T_epsilon))
print(_eig_vectors(hessian_ml_to_epsilon_T_epsilon))
epsilon=epsilon+nwt
print(epsilon)
print("--------------------")

/ unicodes	,
3bb λ
39b Λ
3bc μ
221a √
b2 ²
b3 ³
b9 ¹
207b ⁻


/ Einde NEWTON 3 CLASSES 2 FEATURES

/ CG

/ 7	. 

/ 13	. 

N=3
A=np.array([[4,3,0],[3,4,-1],[0,-1,4]])
#b=np.array([1,0,-3]).reshape(-1,1)
b=np.array([24,20,16]).reshape(-1,1)

r=np.zeros((N,N+1))
t=np.zeros((1,N+1))
x=np.zeros((N,N+1))
s=np.zeros((1,N+1))
v=np.zeros((N,N+1))

def cg():
  x[:,[0]]=np.zeros(N).reshape(-1,1)
  r[:,[0]]=b-A.dot(x[:,[0]])
  v[:,[1]]=r[:,[0]]

  for i in range(1,N+1):
    t[:,[i]]=r[:,[i-1]].T.dot(r[:,[i-1]])/v[:,[i]].T.dot(A).dot(v[:,[i]])
    x[:,[i]]=x[:,[i-1]]+t[:,[i]]*v[:,[i]]
    r[:,[i]]=r[:,[i-1]]-t[:,[i]]*A.dot(v[:,[i]])
		if i==N:
      break
    s[:,[i]]=r[:,[i]].T.dot(r[:,[i]])/r[:,[i-1]].T.dot(r[:,[i-1]])
    v[:,[i+1]]=r[:,[i]]+s[:,[i]]*v[:,[i]]

 
In [15853]: cg()
In [15858]: x
Out[15858]: 
array([[ 0.        ,  0.25      ,  0.32258065,  1.        ],
       [ 0.        ,  0.        , -0.48387097, -1.        ],
       [ 0.        , -0.75      , -0.96774194, -1.        ]])

In [15862]: v[:,[3]].T.dot(A).dot(v[:,[2]])
Out[15862]: array([[-1.11022302e-16]])

In [15863]: v[:,[3]].T.dot(A).dot(v[:,[1]])
Out[15863]: array([[-1.33226763e-15]])

/ 13	. 

In [15878]: A
Out[15878]: 
array([[ 4,  3,  0],
       [ 3,  4, -1],
       [ 0, -1,  4]])

In [15864]: A.dot(np.array([3,4,5]).reshape(-1,1))
Out[15864]: 
array([[24],
       [20],
       [16]])

In [15868]: cg()

In [15869]: x
Out[15869]: 
array([[0.        , 4.125     , 3.90774042, 3.        ],
       [0.        , 3.4375    , 3.04300235, 4.        ],
       [0.        , 2.75      , 4.73964034, 5.        ]])
/ klopt	,


/ 7	. 

/ andere A

/ lees	,
https://stackoverflow.com/questions/38426349/how-to-create-random-orthonormal-matrix-in-python-numpy

/ lees	,
https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_spd_matrix.html

/ 13	. 

In [15884]: O=np.array([[1,1,0]/np.sqrt(2),[1,-1,0]/np.sqrt(2),[0,0,1]])
In [15887]: A=O.dot(np.diag([4,7,9]).np.linalg.inv(O))
In [15889]: A
Out[15889]: 
array([[ 5.5, -1.5,  0. ],
       [-1.5,  5.5,  0. ],
       [ 0. ,  0. ,  9. ]])

/ check	,
In [15890]: np.linalg.eigvals(A)
Out[15890]: array([7., 4., 9.])

In [15893]: b=A.dot(np.array([3,4,5]).reshape(-1,1))
In [15894]: b
Out[15894]: 
array([[10.5],
       [17.5],
       [45. ]])

/ 13	. 

N=3
O=np.array([[1,1,0]/np.sqrt(2),[1,-1,0]/np.sqrt(2),[0,0,1]])
A=O.dot(np.diag([4,7,9]).dot(np.linalg.inv(O)))
b=A.dot(np.array([3,4,5]).reshape(-1,1))

r=np.zeros((N,N+1))
t=np.zeros((1,N+1))
x=np.zeros((N,N+1))
s=np.zeros((1,N+1))
v=np.zeros((N,N+1))

def cg():
  x[:,[0]]=np.zeros(N).reshape(-1,1)
  r[:,[0]]=b-A.dot(x[:,[0]])
  v[:,[1]]=r[:,[0]]

  for i in range(1,N+1):
    t[:,[i]]=r[:,[i-1]].T.dot(r[:,[i-1]])/v[:,[i]].T.dot(A).dot(v[:,[i]])
    x[:,[i]]=x[:,[i-1]]+t[:,[i]]*v[:,[i]]
    r[:,[i]]=r[:,[i-1]]-t[:,[i]]*A.dot(v[:,[i]])
		if i==N:
      break
    s[:,[i]]=r[:,[i]].T.dot(r[:,[i]])/r[:,[i-1]].T.dot(r[:,[i-1]])
    v[:,[i+1]]=r[:,[i]]+s[:,[i]]*v[:,[i]]
   	
 
cg()
In [15899]: x
Out[15899]: 
array([[0.        , 1.28406672, 2.90408674, 3.        ],
       [0.        , 2.1401112 , 4.06734336, 4.        ],
       [0.        , 5.50314308, 4.99407439, 5.        ]])

/ hieronder allen 0	,
v[:,[3]].T.dot(A).dot(v[:,[1]])
Out[15906]: array([[6.28830321e-13]])

v[:,[3]].T.dot(A).dot(v[:,[2]])

v[:,[2]].T.dot(A).dot(v[:,[1]])

r[:,[2]].T.dot(v[:,[1]])

r[:,[2]].T.dot(v[:,[2]])

r[:,[2]].T.dot(v[:,[0]])

r[:,[1]].T.dot(v[:,[0]])

r[:,[1]].T.dot(v[:,[1]])


/ Einde GC

/ GD 2 CLASSES 2 FEATURES

/ p(1|x)=exp(x.T.dot(w1))/(exp(x.T.dot(w1))+exp(x.T.dot(w2))+exp(x.T.dot(w3)))
/ p(2|x)=exp(x.T.dot(w2))/(exp(x.T.dot(w1))+exp(x.T.dot(w2))+exp(x.T.dot(w3)))
/ p(3|x)=exp(x.T.dot(w3))/(exp(x.T.dot(w1))+exp(x.T.dot(w2))+exp(x.T.dot(w3)))

/ deel in alle 3 de kansen de teller en noemer door exp(x.T.dot(w1))	,
/ dan met
v2=w1-w2, v3=w1-w3  ,zien we - in exp(-x.T.dot(v2)) en exp(-x.T.dot(v3))
v=(v2,v3).T

/ versie met - in de exponenten	,

/ lr=learning rate	,
/ doe gd met lr=.001, .003, .01, .03, .1 .3 1 
/ plot voor iedere lr de ml	, voor 100, 200, ... steps	, 

def _p1gX(v2,v3):
	return 1/(1+np.exp(-X.dot(v2))+np.exp(-X.dot(v3)))
def _p2gX(v2,v3):
	return np.exp(-X.dot(v2))/(1+np.exp(-X.dot(v2))+np.exp(-X.dot(v3)))
def _p3gX(v2,v3):
	return np.exp(-X.dot(v3))/(1+np.exp(-X.dot(v2))+np.exp(-X.dot(v3)))

def _ml(v):
	v2=v[:3]
	v3=v[3:6]
	return (1-t2.T-t3.T).dot(np.log(_p1gX(v2,v3)))+t2.T.dot(np.log(_p2gX(v2,v3)))+t3.T.dot(np.log(_p3gX(v2,v3)))#+lmbda*(v2.T.dot(v2)+v3.T.dot(v3))/2

def _grad_ml_to_v2(v2,v3):
  return X.T.dot(_p2gX(v2,v3)-t2)#+lmbda*v2

def _grad_ml_to_v3(v2,v3):
  return X.T.dot(_p3gX(v2,v3)-t3)#+lmbda*v3

def _grad_ml_to_v(v):
	v2=v[:3]
	v3=v[3:6]
  return np.r_[_grad_ml_to_v2(v2,v3),_grad_ml_to_v3(v2,v3)]		# X staat er 2 keer in	, 

def _gd(v):
	return v+lr*_grad_ml_to_v(v) 

def _check(v):
	return _ml(_gd(v))>_ml(v)

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
v2=np.array([0,0,0]).reshape(-1,1)
v3=np.array([0,0,0]).reshape(-1,1)
v=np.r_[v2,v3]
t2=(iris['target']==1).reshape(-1,1).astype(int)
t3=(iris['target']==2).reshape(-1,1).astype(int)
lr_list=(.001,.003,.01,.03,.1,.3,1)


# evt np_c[s,t]
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=0
h=0	# om de hessian te pakken	,
e=0 # om de eig te pakken	,
for i in range(0,100):
  total()

ml=_ml(v)
grad_ml_to_v=_grad_ml_to_v(v)
gd=_gd(v)
ch=_check(v)
v=v+grad_ml_to_v


/ 13	. 

/ als tr=1	,

In [15242]: _ml(g)
/home/eric/miniconda3/bin/ipython:4: RuntimeWarning: divide by zero encountered in log

/ want	,
In [15239]:  v
Out[15239]: 
array([[0],
       [0],
       [0],
       [0],
       [0],
       [0]])
In [15232]: g=v+_grad_ml_to_v(v)
In [15240]: g
Out[15240]: 
array([[-2.02060590e-14],
       [-2.50666667e+01],
       [-6.36666667e+00],
       [ 2.26485497e-14],
       [-8.96666667e+01],
       [-4.13666667e+01]])
In [15233]: g2=g[:3]
In [15234]: g3=g[3:6]
In [15235]: X.dot(g2)
Out[15235]: 
array([[ -36.36666667],
       [ -36.36666667],
       [ -33.86      ],
       [ -38.87333333],
...
In [15236]: X.dot(g3)
Out[15236]: 
array([[-133.80666667],
       [-133.80666667],
       [-124.84      ],
...
In [15237]: np.exp(-X.dot(g2))
Out[15237]: 
array([[6.22074872e+15],
       [6.22074872e+15],
...
In [15241]:   np.exp(-X.dot(g3))
Out[15241]: 
array([[1.29269770e+058],
       [1.29269770e+058],
...
/ Dan omdat -X.dot(g3) zo groot	,
In [15246]: _p2gX(g2,g3)
Out[15246]: 
array([[4.81222232e-043],
       [4.81222232e-043],
/ en is np.log is oneindig	,





def total():
  global alpha 
  ml=_ml(alpha)
  grad_ml_to_alpha=_grad_ml_to_alpha(alpha)
  hessian_ml_to_alpha_T_alpha=_hessian_ml_to_alpha_T_alpha(alpha)
  global h						# om de hessian te pakken	,
  h=hessian_ml_to_alpha_T_alpha
  global e 						# om de eig te pakken	,
  e=np.linalg.eig(hessian_ml_to_alpha_T_alpha)
  nwt=-np.linalg.inv(hessian_ml_to_alpha_T_alpha).dot(grad_ml_to_alpha)
  grad_ml_lin_is_zero=_grad_ml_to_alpha_lin(alpha,nwt)
  print(alpha)
  print(grad_ml_to_alpha)
  print(hessian_ml_to_alpha_T_alpha)
  print(nwt)
  print(grad_ml_lin_is_zero)
  print(_eig_values(hessian_ml_to_alpha_T_alpha))
  print(_eig_vectors(hessian_ml_to_alpha_T_alpha))
  alpha=alpha+nwt
  print(alpha)
  print("--------------------")









print(alpha)
print(grad_ml_to_alpha)
print(hessian_ml_to_alpha_T_alpha)
print(nwt)
print(grad_ml_lin_is_zero)
print(_eig_values(hessian_ml_to_alpha_T_alpha))
print(_eig_vectors(hessian_ml_to_alpha_T_alpha))
alpha=alpha+nwt
print(alpha)
print("--------------------")

/ unicodes	,
3bb λ
39b Λ
3bc μ
221a √
b2 ²
b3 ³
b9 ¹
207b ⁻


/ Einde GD 2 CLASSES 2 FEATURES

/ GD 2 CLASSES 2 FEATURES

p(x)=exp(x.T.dot(w1))+exp(x.T.dot(w2))+exp(x.T.dot(w3))
p(1|x)p(1)=exp(x.T.dot(w1))/p(x)
p(2|x)p(2)=exp(x.T.dot(w2))/p(x)
p(3|x)p(3)=exp(x.T.dot(w3))/p(x)

/ deel in alle 3 de kansen de teller en noemer door exp(x.T.dot(w1))	,
/ dan met
v2=w1-w2, v3=w1-w3  ,zien we - in exp(-x.T.dot(v2)) en exp(-x.T.dot(v3))
v=(v2,v3).T

/ versie met - in de exponenten	,

/ lr=learning rate	,
/ doe gd met lr=.001, .003, .01, .03, .1 .3 1 
/ plot voor iedere lr de ml	, voor 100, 200, ... steps	, 

def pX(v2,v3):
	return 1+np.exp(-X.dot(v2))+np.exp(-X.dot(v3))
def _p1gXp1(v2,v3):
	return 1/pX(v2,v3)
def _p2gXp2(v2,v3):
	return np.exp(-X.dot(v2))/pX(v2,v3)
def _p3gXp3(v2,v3):
	return np.exp(-X.dot(v3))/pX(v2,v3)

def _ml(v):
	v2=v[:3]
	v3=v[3:6]
	return (1-t2.T-t3.T).dot(np.log(_p1gXp1(v2,v3)))+t2.T.dot(np.log(_p2gXp2(v2,v3)))+t3.T.dot(np.log(_p3gXp3(v2,v3)))#+lmbda*(v2.T.dot(v2)+v3.T.dot(v3))/2

def _grad_ml_to_v2(v2,v3):
  return X.T.dot(_p2gXp2(v2,v3)-t2)#+lmbda*v2

def _grad_ml_to_v3(v2,v3):
  return X.T.dot(_p3gXp3(v2,v3)-t3)#+lmbda*v3

def _grad_ml_to_v(v):
	v2=v[:3]
	v3=v[3:6]
  return np.r_[_grad_ml_to_v2(v2,v3),_grad_ml_to_v3(v2,v3)]		# X staat er 2 keer in	, 

def _gd(v):
	return v+lr*_grad_ml_to_v(v) 

def _check(v):
	return _ml(_gd(v))>_ml(v)

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
v2=np.array([0,0,0]).reshape(-1,1)
v3=np.array([0,0,0]).reshape(-1,1)
v=np.r_[v2,v3]
t2=(iris['target']==1).reshape(-1,1).astype(int)
t3=(iris['target']==2).reshape(-1,1).astype(int)
lr_list=(.001,.003,.01,.03,.1,.3,1)


# evt np_c[s,t]
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=0
h=0	# om de hessian te pakken	,
e=0 # om de eig te pakken	,
for i in range(0,100):
  total()

ml=_ml(v)
grad_ml_to_v=_grad_ml_to_v(v)
gd=_gd(v)
ch=_check(v)
v=v+_gd(v)


/ 13	. 

/ als tr=1	,

In [15242]: _ml(g)
/home/eric/miniconda3/bin/ipython:4: RuntimeWarning: divide by zero encountered in log

/ want	,
In [15239]:  v
Out[15239]: 
array([[0],
       [0],
       [0],
       [0],
       [0],
       [0]])
In [15232]: g=v+_grad_ml_to_v(v)
In [15240]: g
Out[15240]: 
array([[-2.02060590e-14],
       [-2.50666667e+01],
       [-6.36666667e+00],
       [ 2.26485497e-14],
       [-8.96666667e+01],
       [-4.13666667e+01]])
In [15233]: g2=g[:3]
In [15234]: g3=g[3:6]
In [15235]: X.dot(g2)
Out[15235]: 
array([[ -36.36666667],
       [ -36.36666667],
       [ -33.86      ],
       [ -38.87333333],
...
In [15236]: X.dot(g3)
Out[15236]: 
array([[-133.80666667],
       [-133.80666667],
       [-124.84      ],
...
In [15237]: np.exp(-X.dot(g2))
Out[15237]: 
array([[6.22074872e+15],
       [6.22074872e+15],
...
In [15241]:   np.exp(-X.dot(g3))
Out[15241]: 
array([[1.29269770e+058],
       [1.29269770e+058],
...
/ Dan omdat -X.dot(g3) zo groot	,
In [15246]: _p2gX(g2,g3)
Out[15246]: 
array([[4.81222232e-043],
       [4.81222232e-043],
/ en is np.log is oneindig	,


/ unicodes	,
3bb λ
39b Λ
3bc μ
221a √
b2 ²
b3 ³
b9 ¹
207b ⁻


/ Einde GD 2 CLASSES 2 FEATURES

/ GD 2 CLASSES 2 FEATURES

p(x)=exp(x.T.dot(w1))+exp(x.T.dot(w2))+exp(x.T.dot(w3))
p(1|x)p(1)=exp(x.T.dot(w1))/p(x)
p(2|x)p(2)=exp(x.T.dot(w2))/p(x)
p(3|x)p(3)=exp(x.T.dot(w3))/p(x)

/ deel in alle 3 de kansen de teller en noemer door exp(x.T.dot(w1))	,
/ dan met
v2=w2-w1, v3=w3-w1  ,zien we + in exp(x.T.dot(v2)) en exp(x.T.dot(v3))
v=(v2,v3).T

/ versie met + in de exponenten	,

/ lr=learning rate	,
/ doe gd met lr=.001, .003, .01, .03, .1 .3 1 
/ plot voor iedere lr de ml	, voor 100, 200, ... steps	, 

def pX(v2,v3):
	return 1+np.exp(X.dot(v2))+np.exp(X.dot(v3))
def _p1gXp1(v2,v3):
	return 1/pX(v2,v3)
def _p2gXp2(v2,v3):
	return np.exp(X.dot(v2))/pX(v2,v3)
def _p3gXp3(v2,v3):
	return np.exp(X.dot(v3))/pX(v2,v3)


def _ml(v):
	v2=v[:3]
	v3=v[3:6]
	return (1-t2.T-t3.T).dot(np.log(_p1gXp1(v2,v3)))+t2.T.dot(np.log(_p2gXp2(v2,v3)))+t3.T.dot(np.log(_p3gXp3(v2,v3)))#+lmbda*(v2.T.dot(v2)+v3.T.dot(v3))/2

def _grad_ml_to_v2(v2,v3):
  return X.T.dot(t2-_p2gXp2(v2,v3))#+lmbda*v2

def _grad_ml_to_v3(v2,v3):
  return X.T.dot(t3-_p3gXp3(v2,v3))#+lmbda*v3

def _grad_ml_to_v(v):
	v2=v[:3]
	v3=v[3:6]
  return np.r_[_grad_ml_to_v2(v2,v3),_grad_ml_to_v3(v2,v3)]		# X staat er 2 keer in	, 

def _gd(v):
	return v+lr*_grad_ml_to_v(v) 

def _check(v):
	return _ml(_gd(v))>_ml(v)

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
v2=np.array([0,0,0]).reshape(-1,1)
v3=np.array([0,0,0]).reshape(-1,1)
v=np.r_[v2,v3]
t2=(iris['target']==1).reshape(-1,1).astype(int)
t3=(iris['target']==2).reshape(-1,1).astype(int)
lr_list=(.001,.003,.01,.03,.1,.3,1)


# evt np_c[s,t]
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
lmbda=0
h=0	# om de hessian te pakken	,
e=0 # om de eig te pakken	,
for i in range(0,100):
  total()

ml=_ml(v)
grad_ml_to_v=_grad_ml_to_v(v)
gd=_gd(v)
ch=_check(v)
v=v+_gd(v)



/ unicodes	,
3bb λ
39b Λ
3bc μ
221a √
b2 ²
b3 ³
b9 ¹
207b ⁻


/ Einde GD 2 CLASSES 2 FEATURES

/ GD 3 CLASSES 2 FEATURES


p(x)=exp(x.T.dot(w1))+exp(x.T.dot(w2))+exp(x.T.dot(w3))
p(1|x)p(1)=exp(x.T.dot(w1))/(exp(x.T.dot(w1))+exp(x.T.dot(w2))+exp(x.T.dot(w3)))
p(2|x)p(2)=exp(x.T.dot(w2))/(exp(x.T.dot(w1))+exp(x.T.dot(w2))+exp(x.T.dot(w3)))
p(3|x)p(3)=exp(x.T.dot(w3))/(exp(x.T.dot(w1))+exp(x.T.dot(w2))+exp(x.T.dot(w3)))

/ we laten ze zo staan	,

/ we copy de fcts	,

def pX(w1,w2,w3):
	return np.exp(X.dot(w1))+np.exp(X.dot(w2))+np.exp(X.dot(w3))
def _p1gXp1(w1,w2,w3):
	return np.exp(X.dot(w1))/pX(w1,w2,w3)
def _p2gXp2(w1,w2,w3):
	return np.exp(X.dot(w2))/pX(w1,w2,w3)
def _p3gXp3(w1,w2,w3):
	return np.exp(X.dot(w3))/pX(w1,w2,w3)

def _ml(w):
	w1=w[:3]
	w2=w[3:6]
	w3=w[6:9]
	return t1.T.dot(np.log(_p1gXp1(w1,w2,w3)))+t2.T.dot(np.log(_p2gXp2(w1,w2,w3)))+t3.T.dot(np.log(_p3gXp3(w1,w2,w3)))         # +lmbda*beta.T.dot(beta)

def _grad_ml_to_w1(w1,w2,w3):
  return X.T.dot(t1-_p1gXp1(w1,w2,w3))		# X staat er 2 keer in	,

def _grad_ml_to_w2(w1,w2,w3):
  return X.T.dot(t2-_p2gXp2(w1,w2,w3))		# X staat er 2 keer in	,
 																					#+lmbda*beta
def _grad_ml_to_w3(w1,w2,w3):
  return X.T.dot(t3-_p3gXp3(w1,w2,w3))		# X staat er 2 keer in	,
 																					#+lmbda*beta
def _grad_ml_to_w(w):
	w1=w[:3]
	w2=w[3:6]
	w3=w[6:9]
  return np.r_[_grad_ml_to_w1(w1,w2,w3),_grad_ml_to_w2(w1,w2,w3),_grad_ml_to_w3(w1,w2,w3)]		# X staat er 2 keer in	, #+lmbda*w2

def _gd(w):
	return w+lr*_grad_ml_to_w(w) 

def _check_gd(w):
	return _ml(_gd(w))>_ml(w)

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
t1=(iris['target']==0).reshape(-1,1).astype(int)
t2=(iris['target']==1).reshape(-1,1).astype(int)
t3=(iris['target']==2).reshape(-1,1).astype(int)
lr_list=(.001,.003,.01,.03,.1,.3,1)

w1=np.array([0,0,0]).reshape(-1,1)
w2=np.array([0,0,0]).reshape(-1,1)
w3=np.array([0,0,0]).reshape(-1,1)
w=np.r_[w1,w2,w3]
lmbda=0
lr=lr_list[1]
lr=.001
for i in range(0,10000):
  if _check_gd(w)==False:
		display(i)
		break
  w=_gd(w)
coef_=w.reshape(-1,3)[:,1:]
intercept_=w.reshape(-1,3)[:,0].ravel()
display(coef_)
display(intercept_)

array([[-2.1439229 , -4.72848933],
       [ 0.31672102, -0.75817666],
       [ 1.82720189,  5.48666599]])
array([ 11.94735624,   2.83853809, -14.78589433])
/ Hij komt er niet echt	,
/ TODO
/ w2 klopt WH niet	,
/ TODO


ml=_ml(w)
grad_ml_to_w=_grad_ml_to_w(w)
gd=_gd(w)
ch=_check_gd(w)
w=_gd(w)


/ unicodes	,
3bb λ
39b Λ
3bc μ
221a √
b2 ²
b3 ³
b9 ¹
207b ⁻

/ 13	. 

X=iris['data'][:,[2,3]]
y = iris["target"]

/ 1313	. 

lg=LogisticRegression(C=10**10)
lg.fit(X,y)
display(lg.coef_)
display(lg.intercept_)

array([[-6.67454643, -9.82307197],
       [ 1.54644801, -3.10696268],
       [ 5.7528683 , 10.44455633]])
array([ 24.05565958,  -2.85895422, -45.26062435])

/ 1313	. 

lg=LogisticRegression(multi_class="multinomial",solver="lbfgs",C=10**10) 
lg.fit(X,y)
display(lg.coef_)
display(lg.intercept_)

array([[ -8.08440785, -18.01963296],
       [  1.16488946,   3.78620864],
       [  6.91951839,  14.23342432]])
array([ 41.08973527,   2.09196084, -43.18169611])


lg=LogisticRegression(multi_class="multinomial",solver="newton-cg",C=10**10) 
lg.fit(X,y)
display(lg.coef_)
display(lg.intercept_)

array([[ -7.96438255, -12.71560017],
       [  1.10492054,   1.13445454],
       [  6.859462  ,  11.58114562]])
array([ 36.95518541,   4.15859445, -41.11377986])




/ Einde GD 3 CLASSES 2 FEATURES

/ lees over np.newaxis,
https://stackoverflow.com/questions/29241056/how-does-numpy-newaxis-work-and-when-to-use-it




/ 131313	. 
	
X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([-21,12]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
#resML=np.zeros((10,2+2+2+1+1+2))
lmbda=0
i=0
for i in range(0,10):
  #m=X.dot(beta)
  p1gX=expit(X.dot(beta))
	mlvect=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)
	ml=np.ones((1,mlvect.shape[0])).dot(mlvect)+lmbda*beta.T.dot(beta)
  grad_ml_to_beta=X.T.dot(t-p1gX)+lmbda*beta 		# X staat er 2 keer in	,
  D=np.diag((-p1gX*(1-p1gX)).ravel())
  hessian_ml_to_beta=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
	nwt=-np.linalg.inv(hessian_ml_to_beta).dot(grad_ml_to_beta)
	grad_ml_to_beta_lin_in_nwt=grad_ml_to_beta+hessian_ml_to_beta.dot(nwt)
	print(beta)
	print(grad_ml_to_beta)
	print(nwt)
	print(grad_ml_to_beta_lin_in_nwt)
	print("--------------------")
  if (grad_ml_to_beta == np.zeros(beta.shape[0])).all():
    break
	beta=beta+nwt

p1gX=expit(X.dot(beta))
mlvect=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)
ml=np.ones((1,mlvect.shape[0])).dot(mlvect)+lmbda*beta.T.dot(beta)
grad_ml_to_beta=X.T.dot(t-p1gX)+lmbda*beta 		# X staat er 2 keer in	,
D=np.diag((-p1gX*(1-p1gX)).ravel())
hessian_ml_to_beta=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
nwt=-np.linalg.inv(hessian_ml_to_beta).dot(grad_ml_to_beta)
grad_ml_to_beta_lin_in_nwt=grad_ml_to_beta+hessian_ml_to_beta.dot(nwt)
print(beta)
print(grad_ml_to_beta)
print(nwt)
print(grad_ml_to_beta_lin_in_nwt)
print("--------------------")
beta=beta+nwt

/ HIER HIER HIER

	
	resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
  beta=beta+nwt

m=X.dot(beta)
p1gX=expit(m)
mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)
ml=np.ones((1,mlv.shape[0])).dot(mlv)+lmbda*beta.T.dot(beta)
grad=X.T.dot(t-p1gX)+lmbda*beta
D=np.diag((-p1gX*(1-p1gX)).ravel())
H=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
nwt=-np.linalg.inv(H).dot(grad)
resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),np.linalg.norm(grad).ravel(),np.linalg.eig(H)[0]]
beta=beta+nwt
i=i+1

# in de 1ste stap ga je de verkeerde kant op	, daarna goed:
6.49116244e+00, -2.92355732e+00, 	# verkeerd	,
-4.03932128e+00,  2.33840635e+00	# ok	,
...
In [13358]: resML
Out[13358]: 
array([[-2.10000000e+01,  1.20000000e+01,  7.32980154e+00,
         1.24971138e+01,  6.49116244e+00, -2.92355732e+00,
        -2.18780514e+01,  1.44880587e+01, -3.16189918e-02,
        -2.21976998e+01],
       [-1.45088376e+01,  9.07644268e+00, -2.85554275e+00,
        -3.77084753e+00, -4.03932128e+00,  2.33840635e+00,
        -1.85144814e+01,  4.73005449e+00, -8.72161555e-02,
        -2.71034126e+01],
       [-1.85481588e+01,  1.14148490e+01, -6.23277201e-01,
        -7.75108084e-01, -2.16067493e+00,  1.28278799e+00,
        -1.69074812e+01,  9.94619028e-01, -4.83104324e-02,
        -2.05265739e+01],
       [-2.07088338e+01,  1.26976370e+01, -7.26514138e-02,
        -8.51030288e-02, -4.05525346e-01,  2.43065060e-01,
        -1.67148719e+01,  1.11896173e-01, -3.61946798e-02,
        -1.77947749e+01],
       [-2.11143591e+01,  1.29407021e+01, -1.59553397e-03,
        -1.72443592e-03, -1.12725670e-02,  6.80005759e-03,
        -1.67104073e+01,  2.34934201e-03, -3.43512445e-02,
        -1.73327510e+01],
       [-2.11256317e+01,  1.29475021e+01, -9.42868266e-07,
        -8.95445294e-07, -8.27835686e-06,  5.02327785e-06,
        -1.67104041e+01,  1.30031644e-06, -3.43014348e-02,
        -1.73200771e+01],
       [-2.11256400e+01,  1.29475072e+01, -3.89910326e-13,
        -2.90434343e-13, -4.46189873e-12,  2.72277538e-12,
        -1.67104041e+01,  4.86191495e-13, -3.43013980e-02,
        -1.73200677e+01],
       [-2.11256400e+01,  1.29475072e+01, -3.10862447e-15,
        -3.55271368e-15, -1.94860360e-14,  1.17089474e-14,
        -1.67104041e+01,  4.72073305e-15, -3.43013980e-02,
        -1.73200677e+01],
       [-2.11256400e+01,  1.29475072e+01, -1.46549439e-14,
        -2.22044605e-14, -2.09182591e-14,  1.11044995e-14,
        -1.67104041e+01,  2.66046133e-14, -3.43013980e-02,
        -1.73200677e+01],
       [-2.11256400e+01,  1.29475072e+01,  3.55271368e-15,
         7.10542736e-15, -1.73271648e-14,  1.12294104e-14,
        -1.67104041e+01,  7.94410929e-15, -3.43013980e-02,
        -1.73200677e+01]])

/ 1313	. 

/ 1 feature	,
/ newton	,
/ print 

beta0=np.linspace(-22,-20,11)
beta1=np.linspace(11,13,11)
beta0c,beta1c=np.meshgrid(beta0,beta1,indexing='ij')
betag=np.c_[beta0c.ravel(),beta1c.ravel()]

X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
t=(iris['target']==2).reshape(-1,1).astype(int)
#resML=np.zeros((g.shape[0],g.shape[1]+2+1))
# :2		2:4					 4     5:7 7	 8:10	
# beta, normed grad,|grad|,nwt,ml,eigv(hessian)
resML=np.zeros((g.shape[0],g.shape[1]+2+1+2+1+2))
lmbda=0

for i in np.arange(0,g.shape[0]):
	beta=g[i].reshape(-1,1)
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)
	ml=np.ones((1,mlv.shape[0])).dot(mlv)+lmbda*beta.T.dot(beta)
  grad=X.T.dot(t-p1gX)+lmbda*beta
  D=np.diag((-p1gX*(1-p1gX)).ravel())
  H=X.T.dot(D).dot(X)+lmbda*np.eye(beta.shape[0])
	nwt=-np.linalg.inv(H).dot(grad)
	#resML[i]=np.r_[beta.ravel(),(grad/np.linalg.norm(grad)).ravel(),ml.ravel()]
	resML[i]=np.r_[beta.ravel(),(grad/np.linalg.norm(grad)).ravel(),np.linalg.norm(grad).ravel(),nwt.ravel(),ml.ravel(),np.linalg.eig(H)[0]]
  beta=beta+nwt

df=pd.DataFrame(resML,columns=['b1','b2','g1','g2','ml'])
In [13397]: df.sort_values(by='ml',ascending=False)
Out[13397]: 
       b1    b2        g1        g2         ml
54  -21.2  13.0 -0.497811 -0.867286 -16.710817
85  -20.6  12.6  0.469843  0.882750 -16.720871
75  -20.8  12.8 -0.539241 -0.842151 -16.730836
106 -20.2  12.4 -0.612557 -0.790426 -16.734740
64  -21.0  12.8  0.518454  0.855106 -16.742067
116 -20.0  12.2  0.472626  0.881263 -16.762515
96  -20.4  12.6 -0.542745 -0.839898 -16.787476
95  -20.4  12.4  0.504827  0.863221 -16.790921
43  -21.4  13.0  0.527250  0.849710 -16.796113
65  -21.0  13.0 -0.528959 -0.848647 -16.816883
74  -20.8  12.6  0.515805  0.856706 -16.852889
117 -20.0  12.4 -0.544585 -0.838706 -16.883189
86  -20.6  12.8 -0.534463 -0.845192 -16.916849
105 -20.2  12.2  0.507916  0.861406 -16.945443
53  -21.2  12.8  0.521030  0.853539 -16.946280
84  -20.6  12.4  0.513824  0.857896 -17.047804
107 -20.2  12.6 -0.538024 -0.842929 -17.057554
32  -21.6  13.0  0.523882  0.851791 -17.069108
76  -20.8  13.0 -0.532087 -0.846690 -17.119464
63  -21.0  12.6  0.517380  0.855756 -17.180220
115 -20.0  12.0  0.508105  0.861295 -17.189431
97  -20.4  12.8 -0.535557 -0.844499 -17.305801
94  -20.4  12.2  0.511990  0.858991 -17.331875
42  -21.4  12.8  0.519636  0.854388 -17.340786
73  -20.8  12.4  0.514597  0.857432 -17.503062
21  -21.8  13.0  0.521100  0.853496 -17.527741
118 -20.0  12.6 -0.538328 -0.842735 -17.537314
87  -20.6  13.0 -0.534666 -0.845063 -17.625011
52  -21.2  12.6  0.516388  0.856355 -17.701171
104 -20.2  12.0  0.510213  0.860048 -17.710710
...

In [13416]: resML[(resML[:,0]==-21)&(resML[:,1]==12)]
Out[13416]: 
array([[-21.        ,  12.        ,   0.5059202 ,   0.86258029,
         14.48805871,   6.49116244,  -2.92355732, -21.87805139,
         -0.03161899, -22.19769975]])
/ Hoe met df?
/ TODO

In [13422]: df=pd.DataFrame(resML,columns=['b1','b2','g1','g2','norm grad','nwt1','nwt2','ml','eigv1','eigv2'])


/ Einde NEWTON LOGISTIC REGRESSION

/ GD LOGISTIC REGRESSION

/ 13	.

/ gd,
/ sigmoid	,
/ 1 feature	,
/ log	, = ml
/ zoek max, dus volg grad	,

/ newton,
In [13606]: beta
Out[13606]: 
array([[-21.12563996],
       [ 12.94750716]])

X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([0,0]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :2		2:4 	4:6  6		
# beta, grad,gd,ml
resML=np.zeros((200,2+2+2+1))
alpha=.1
i=0
for i in range(0,200):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX) 	# log ipv -log
	ml=np.ones((1,mlv.shape[0])).dot(mlv)
  grad=X.T.dot(t-p1gX)									# t-p1gX ipv p1gX-t 
	gd=alpha*grad
	resML[i]=np.r_[beta.ravel(),grad.ravel(),gd.ravel(),ml.ravel(),]
  beta=beta+gd

m=X.dot(beta)
p1gX=expit(m)
mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX) 	# log ipv -log
ml=np.ones((1,mlv.shape[0])).dot(mlv)
grad=X.T.dot(t-p1gX)									# t-p1gX ipv p1gX-t 
gd=alpha*grad
resML[i]=np.r_[beta.ravel(),grad.ravel(),gd.ravel(),ml.ravel(),]
beta=beta+gd
i=i+1

def compute_ml(beta):
  m=X.dot(beta)
  p1gX=expit(m)
  mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX)   # log ipv -log
  ml=np.ones((1,mlv.shape[0])).dot(mlv)
	return ml

/ je moet niet de hele gradient nemen: δc/δβ, maar een factor:
In [13570]: compute_ml(np.array([[0],[0]]))
Out[13570]: array([[-103.97207708]])
In [13571]: compute_ml(np.array([[-25],[11.4]]))
Out[13571]: array([[-129.89382314]])

/ we kijken in de buurt, en we nemen factor=.1	,
/ dan zie je dat .1*δc/δβ het beste is	,
In [13573]: compute_ml(np.array([[-25/10],[11.4/10]]))
Out[13573]: array([[-61.47709908]])
In [13574]: compute_ml(np.array([[-25/10],[-11.4/10]]))
Out[13574]: array([[-244.86112075]])
In [13575]: compute_ml(np.array([[25/10],[-11.4/10]]))
Out[13575]: array([[-212.46909908]])
In [13576]: compute_ml(np.array([[25/10],[11.4/10]]))
Out[13576]: array([[-343.86912075]])
/ en	,
In [13577]: compute_ml(np.array([[-25/10],[0]]))
Out[13577]: array([[-136.83346014]])
In [13580]: compute_ml(np.array([[0],[11.4/10]]))
Out[13580]: array([[-132.84899647]])
/ en	,
In [13584]: compute_ml(np.array([[-25*.2],[11.4*.2]]))			<-
Out[13584]: array([[-54.93961992]])
In [13586]: compute_ml(np.array([[-25*.3],[11.4*.3]]))
Out[13586]: array([[-58.75440841]])


/ 13	. 

/ gd,
/ 1 feature	,
/ sigmoid	,
/ -log, 
/ zoek min cost	, dus volg -grad,	

X=iris['data'][:,[3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([0,0]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :2	  2:4		4:6 6	
# beta, grad,gd,ml
resML=np.zeros((10,2+2+2+1))
alpha=1
i=0
for i in range(0,10):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*-np.log(p1gX)+(1-t)*-np.log(1-p1gX) 	
	ml=np.ones((1,mlv.shape[0])).dot(mlv)
  grad=X.T.dot(p1gX-t)								
	gd=alpha*-grad
	resML[i]=np.r_[beta.ravel(),grad.ravel(),gd.ravel(),ml.ravel(),]
  beta=beta+gd

m=X.dot(beta)
p1gX=expit(m)
mlv=t*-np.log(p1gX)+(1-t)*-np.log(1-p1gX)
ml=np.ones((1,mlv.shape[0])).dot(mlv)
grad=X.T.dot(p1gX-t)			
gd=alpha*-grad
resML[i]=np.r_[beta.ravel(),grad.ravel(),gd.ravel(),ml.ravel(),]
beta=beta+gd
i=i+1

/ 13. 

/ gd,
/ sigmoid	,
/ -log, 
/ zoek min cost	, dus volg -grad,	

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([-45,5,10]).reshape(-1,1)
beta=np.array([0,0,0]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,nwt,ml,|grad|,eigv(hessian)
resML=np.zeros((1000,3+3+3+1))
alpha=1
i=0
for i in range(0,10):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*-np.log(p1gX)+(1-t)*-np.log(1-p1gX) 	
	ml=np.ones((1,mlv.shape[0])).dot(mlv)
  grad=X.T.dot(p1gX-t)								
	gd=alpha*-grad
	resML[i]=np.r_[beta.ravel(),grad.ravel(),gd.ravel(),ml.ravel(),]
  beta=beta+gd

m=X.dot(beta)
p1gX=expit(m)
mlv=t*-np.log(p1gX)+(1-t)*-np.log(1-p1gX)
ml=np.ones((1,mlv.shape[0])).dot(mlv)
grad=X.T.dot(p1gX-t)			
nwt=alpha*-grad
resML[i]=np.r_[beta.ravel(),grad.ravel(),nwt.ravel(),ml.ravel(),]
beta=beta+gd
i=i+1

/ 13	.

/ gd,
/ sigmoid	,
/ log	, = ml
/ zoek max, dus volg grad	,

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((X.shape[0],1)),X]
beta=np.array([0,0,0]).reshape(-1,1)
beta=np.array([-45,5,10]).reshape(-1,1)
t=(iris['target']==2).reshape(-1,1).astype(int)
# :3		3:6	6:9  9		10		11:14
# beta, grad,gd,ml,|grad|,eigv(hessian)
resML=np.zeros((10000,3+3+3+1))
alpha=.1
i=0
for i in range(0,10000):
  m=X.dot(beta)
  p1gX=expit(m)
	mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX) 	# log ipv -log
	ml=np.ones((1,mlv.shape[0])).dot(mlv)
  grad=X.T.dot(t-p1gX)									# t-p1gX ipv p1gX-t 
	gd=alpha*grad
	resML[i]=np.r_[beta.ravel(),grad.ravel(),gd.ravel(),ml.ravel(),]
  beta=beta+gd

m=X.dot(beta)
p1gX=expit(m)
mlv=t*np.log(p1gX)+(1-t)*np.log(1-p1gX) 	# log ipv -log
ml=np.ones((1,mlv.shape[0])).dot(mlv)
grad=X.T.dot(t-p1gX)									# t-p1gX ipv p1gX-t 
gd=alpha*grad
resML[i]=np.r_[beta.ravel(),grad.ravel(),gd.ravel(),ml.ravel(),]
beta=beta+gd
i=i+1

/ Einde GD LOGISTIC REGRESSION

/  7	. 

/ NEWTON 

/ 13	.  

/ zonder demping	,

In [13439]: np.sqrt(1/3)
Out[13439]: 0.5773502691896257
/ opl + en -	,

def f(x):
	return x**3-x
def gradf(x):
	return 3*x**2-1
def nwt(s):
	return -f(s)/gradf(s)	

x=1/np.sqrt(3)
res=np.zeros((10,2))
for i in np.arange(0,10):
	d=nwt(x)
	res[i]=(x,d)
	x=x+d

d=nwt(x)
res[i]=(x,d)
x=x+d

/ links en vlakbij 1/√3 -> -1 en verder naar links naar 0 -> 0
/ rechts van 1/√3 -> 1
/ rechts vlakbij -1/√3 -> 1
/ links van -1/√3 -> -1
      ->-1     | ->1               0<-           -1<- |          ->1
-------------------------------------------------------------------
	   -1				-1/√3								0            1/√3              1

/ 1/√3 = 0.5773502691896257
/ we zien dat .45 -> -1 omdat dicht bij 1/√3	, en .44 -> 0 omdat dicht bij 0	

/ bij .44 < 1/√5  <.45 springt hij heen en weer	, 
/ TODO

/ .5 -> -1	, en dus niet naar 0	, die dichterbij ligt	,
/ maar het schiet over 0 heen	,
In [13458]: res
Out[13458]: 
array([[ 0.5, -1.5],
       [-1. , -0. ],
       [-1. , -0. ],
       [-1. , -0. ],
       [-1. , -0. ],
       [-1. , -0. ],
       [-1. , -0. ],
       [-1. , -0. ],
       [-1. , -0. ],
       [-1. , -0. ]])

/ .4 -> 0 wel	,
In [13460]: res
Out[13460]: 
array([[ 4.00000000e-01, -6.46153846e-01],
       [-2.46153846e-01,  2.82610534e-01],
       [ 3.64566877e-02, -3.65539840e-02],
       [-9.72963905e-05,  9.72963923e-05],
       [ 1.84212965e-12, -1.84212965e-12],
       [ 0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00]])

/ 0 -> 0
In [13462]: res
Out[13462]: 
array([[ 0., -0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.]])

/ .57 -> -1
In [13464]: res
Out[13464]: 
array([[  0.57      , -15.20976285],
       [-14.63976285,   4.86471794],
       [ -9.7750449 ,   3.23553509],
       [ -6.53950981,   2.14558817],
       [ -4.39392164,   1.4131771 ],
       [ -2.98074454,   0.9161229 ],
       [ -2.06462164,   0.57144306],
       [ -1.49317858,   0.32273996],
       [ -1.17043862,   0.13923055],
       [ -1.03120807,   0.02984625]])

/ .49 -> -1
In [13466]: res
Out[13466]: 
array([[ 4.90000000e-01, -1.33125134e+00],
       [-8.41251341e-01, -2.18940474e-01],
       [-1.06019182e+00,  5.54257002e-02],
       [-1.00476611e+00,  4.73241592e-03],
       [-1.00003370e+00,  3.36973520e-05],
       [-1.00000000e+00,  1.70330549e-09],
       [-1.00000000e+00, -0.00000000e+00],
       [-1.00000000e+00, -0.00000000e+00],
       [-1.00000000e+00, -0.00000000e+00],
       [-1.00000000e+00, -0.00000000e+00]])

/ .58 -> 1
In [13468]: res
Out[13468]: 
array([[  0.58      ,  41.83565217],
       [ 42.41565217, -14.1333106 ],
       [ 28.28234158,  -9.41958664],
       [ 18.86275494,  -6.27579293],
       [ 12.58696201,  -4.17796183],
       [  8.40900018,  -2.77644818],
       [  5.632552  ,  -1.8376452 ],
       [  3.7949068 ,  -1.20502341],
       [  2.58988338,  -0.77300344],
       [  1.81687994,  -0.46957905]])


/ 13	. 

/ met demping	, 

/ zonder demping zagen we dat .45 -> -1 omdat dichtbij 1/√3	, en .44 -> 0
/ maar met demping gaat .45 -> 0	,

def f(x):
  return x**3-x
def gradf(x):
  return 3*x**2-1

def nwt(s):
  return -f(s)/gradf(s)

x=.45
alpha=.1
res=np.zeros((100,2))
for i in np.arange(0,100):
  d=alpha*nwt(x)
  res[i]=(x,d)
  x=x+d

In [13499]: res
Out[13499]: 
array([[ 4.40000000e-01, -8.46412214e-01],
       [-4.06412214e-01,  6.72533724e-01],
       [ 2.66121510e-01, -3.13984343e-01],
...
       [ 1.22894507e-05, -1.22894507e-06],
       [ 1.10605056e-05, -1.10605056e-06],
       [ 9.95445506e-06, -9.95445506e-07]])

/ 7	. 

/  nulpunten afgeleiden	,

# (x³-x)(x²-4)	 		
/ x³= x ctrl+shift+u b3
/ x²= x ctrl+shift+u b2
def f(x):
  return x**5-5*x**3+4*x 
def gradf(x):
  return 5*x**4-15*x**2+4 
def hessianf(x):
	return 20*x**3-30*x 

def nwt(s):
  return -gradf(s)/hessianf(s)

plt.figure()
x=np.linspace(-2,2,21)
plt.plot(x,f(x))
/ we zien de max/min bij ongeveer -1.6, -.6	, .6, 1.6

x=.45
alpha=.1
res=np.zeros((100,2))
for i in np.arange(0,100):
  d=alpha*nwt(x)
  res[i]=(x,d)
  x=x+d

/ 	7 .

/ met newton op zoek naar gradient =0

def r2(x):
	return x[0,0]**2+x[1,0]**2

def gradr2(x):
	return 2*x

def hessianr2(x):
	return 2*np.eye(x.shape[0])

def nwtr2(x):
	return -np.linalg.inv(hessianr2(x)).dot(gradr2(x))

x=np.array([[3],[4]])
d=nwtr2(x)
x=x+d

/ je bent er in 1 stap	,  

/ 7	. 

/ met newton op zoek naar gradient =0

/ 13	. 

def r2(x):
	return x[0,0]**2+x[1,0]**2
def gradr2(x):
	return 2*x
def hessianr2(x):
	return 2*np.eye(x.shape[0])
def nwtr2(x):
	return -np.linalg.inv(hessianr2(x)).dot(gradr2(x))

# in 1 stap gradr2(x)=0
x=np.array([[3],[4]])
d=nwtr2(x)
x=x+d
gradr2(x)

/ 13	. 

def r(x):
	return np.sqrt(r2(x))
def gradr(x):
	return gradr2(x)/2/r(x)
def hessianr_alt(x):
	return hessianr2(x)/2/r(x)-gradr2(x).dot(gradr(x).T)/2/r(x)**2
def hessianr(x):
	return np.eye(x.shape[0])/r(x)-1/(r(x)**3)*x.dot(x.T)
def hessianr_alt2(x):
	return np.eye(x.shape[0])/r(x)-x.dot(x.T)/r(x)**3
def hessianr_alt3(x):
	return (r(x)**2-x.dot(x.T))/r(x)**3
def nwtr(x):
	return -np.linalg.inv(hessianr(x)).dot(gradr(x))

x=np.array([[3],[4]])
d=nwtr(x)
x=x+d
gradr(x)
# Singular matrix	,

/ 13	. 

/////////////////////////////////////////

def r3(x):
	return r(x)**3
# vv
def gradr3(x):
	return 3*r(x)**2*gradr(x)
def gradr3_alt(x):
	return 3*r(x)*x
def hessianr3(x):
	return 6*r(x)*gradr(x).dot(gradr(x).T)+3*r(x)**2*hessianr(x)
def hessianr3_alt(x):
	return 3*x.dot(x.T)/r(x)+3*r(x)*np.eye(x.shape[0])
# linear vv
def gradr3_lin(x,d):
	return gradr3(x)+hessianr3(x).dot(d)
def nwtr3(x):
	return -np.linalg.inv(hessianr3(x)).dot(gradr3(x))
def nwtr3_alt(x):
	return -np.linalg.inv(hessianr3_alt(x)).dot(gradr3_alt(x))

x=np.array([[1],[2]])
for iter in np.arange(10):
  d=nwtr3(x)
  print(x)
	print(d)
  print(gradr3_lin(x,d))
  print(gradr3(x))
	print("-----------")
  x=x+d
	if (gradr3(x)==np.array([[0],[0]])).all():
		break

d=nwtr3(x)
print(x)
print(d)
print(gradr3_lin(x,d))
print(gradr3(x))
print("-----------")
x=x+d

/ 13	. 

def c(x):
	return r(x)**3 -r(x)
def gradc(x):
	return (3*r(x)**2-1)*gradr(x)
def hessianc(x):
	return 6*r(x)*gradr(x).dot(gradr(x).T)+(3*r(x)**2-1)*hessianr(x)
def nwtc(x):
	return -np.linalg.inv(hessianc(x)).dot(gradc(x))

x=np.array([[3],[4]])
for iter in np.arange(10):
  d=nwtc(x)
  x=x+d
  print(gradc(x))

/ 13	. 

def c(x):
	return r(x)**3 -r(x)**2
def gradc(x):
	return (3*r(x)**2-2*r(x))*gradr(x)
def hessianc(x):
	return (6*r(x)-2)*gradr(x).dot(gradr(x).T)+(3*r(x)**2-2*r(x))*hessianr(x)
def nwtc(x):
	return -np.linalg.inv(hessianc(x)).dot(gradc(x))

x=np.array([[3],[4]])
for iter in np.arange(10):
  d=nwtc(x)
  x=x+d
  display(gradc(x))
	if (gradc(x)==np.zeros(2)).all():
		break
# 
In [13835]: x
Out[13835]: 
array([[0.4       ],
       [0.53333333]])
In [13834]: gradc(x)
Out[13834]: 
array([[0.],
       [0.]])
In [13833]: hessianc(x)
Out[13833]: 
array([[0.72, 0.96],
       [0.96, 1.28]])
# is singular	,

x=np.array([[3],[4]])
d=nwtc(x)
x=x+d
gradc(x)


/ 7	 




/ Einde NEWTON
