[eric@almond python]$ pwd
/home/eric/Devel/Eclipse/python
[eric@almond python]$ tar -xvzf ~/Downloads/liclipse_5.2.4_linux.gtk.x86_64.tar.gz 
[eric@almond python]$ mkdir workspace 
[eric@almond python]$ ls
liclipse  workspace
/ bij start eclipse hebben we deze workspace gekozen	,

/ lees	,
https://stackoverflow.com/questions/13298630/how-do-i-import-a-pre-existing-python-project-into-eclipse

/ lees	,
https://scikit-learn.org/stable/developers/advanced_installation.html

/ Window, Preferences, PyDev, Interpreters, Python interpreter	,
/ click: Choose from list
/home/eric/miniconda3/bin/python
/ onderin zien we ook allemaal packages	,

File, Import
Git
	projects from git 
/ kies uit 
Local
Clone URI
/ we hadden sklearn al checkout, maar we kiezen toch Clone URI en we moeten de clone uri opgeven: https://github.com/scikit-learn/scikit-learn.git 
/ hij gaat checkout, we gaven op /home/eric/git_codebase/scikit-learn

/ we komen in window: Import projects from Git	,
/ bovenin zien we in het grijs: Wizard for project import, hier komen we later , maar kies: Import as general project	,
Next	,
/home/eric/Devel/python/scikit-learn

/ 7	. 


/ lees	,
https://www.pydev.org/manual_adv_debugger.html
https://www.pydev.org/manual_101_root.html			<-
->
https://www.pydev.org/manual_101_first_module.html 	<-

/ lees,
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
->
https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm
/ 7	. 

/ start LiClipse	,

[eric@almond liclipse]$ pwd
/home/eric/Devel/Eclipse/python/liclipse
[eric@almond liclipse]$ ./LiClipse 
/ ws, 
/home/eric/Devel/Eclipse/python/workspace

/ we hebben de manual gevolgd, en toegevoegd in root.nested package,
log_reg_sklearn.py	, 

/ we hebben ook scikit-learn 0.19.1 in liclipse	, 
/ TODO nodig?

/ log_reg_sklearn.py, right click, Debug as, Python run	,




/ 7	. 

/ lees,
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit
https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
 

/ 7	. 

/ pas op bij zoeken, want er zijn 2 versies van logistic.py	,
/ 1 is in miniconda's sklearn, 																<-
/ en de andere is WH die we hebben import in eclipse	, 

/ 7		.

/ newton-cg
/ lees,
https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method

/ 13	. 

In [15406]: from scipy.sparse.linalg import cg

In [15404]: A=np.array([[3,-1],[-1,3]])/2
In [15405]: A
Out[15405]: 
array([[ 1.5, -0.5],
       [-0.5,  1.5]])

In [15409]: b=np.array([3,2]).reshape(-1,1)
In [15410]: b
Out[15410]: 
array([[3],
       [2]])

In [15411]: cg(A,b)
Out[15411]: (array([2.75, 2.25]), 0)

0 : successful exit >0 : convergence to tolerance not achieved, number of iterations <0 : illegal input or breakdown

/ check	, 

In [15418]: x=x[0].reshape(-1,1)

In [15419]: x
Out[15419]: 
array([[2.75],
       [2.25]])

In [15420]: A.dot(x)
Out[15420]: 
array([[3.],
       [2.]])

/ NEWTON-CG DEBUG


/ 7	 .

/ we komen in 	,

$ less logistic.py

def _multinomial_loss(w, X, Y, alpha, sample_weight):


/ 13	. 

In [16272]: from scipy.special import logsumexp

/ axis is de richting waarin je sommeert	,

In [16280]: a=np.array([1,2,1,2]).reshape(-1,2)
In [16281]: a
Out[16281]: 
array([[1, 2],
       [1, 2]])

In [16273]: logsumexp(a,axis=1)
Out[16273]: array([2.31326169, 2.31326169])

In [16274]: logsumexp(a,axis=0)
Out[16274]: array([1.69314718, 2.69314718])

/ lees	,
https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.misc.logsumexp.html

>>> from scipy.special import logsumexp
>>> a = np.arange(10)
>>> np.log(np.sum(np.exp(a)))
9.4586297444267107
>>> logsumexp(a)
9.4586297444267107

/ 13	. 

/ een andere manier om van een array een vector te maken	, dan reshape(-1,1)	,
In [16277]: logsumexp(a,axis=1).shape
Out[16277]: (2,)
In [16278]: logsumexp(a,axis=1)[:,np.newaxis].shape
Out[16278]: (2, 1)

    p = safe_sparse_dot(X, w.T)
    p += intercept
    p -= logsumexp(p, axis=1)[:, np.newaxis]
    loss = -(sample_weight * Y * p).sum()
    loss += 0.5 * alpha * squared_norm(w)
    p = np.exp(p, p)


/ w=(3,2), de rijen zijn de 2 coeff (niet de intercept) voor class 1,2,3
/ bij ons heten ze w1,w2,w3	,
/ X.dot(w.T), wij doen altijd X.T.dot(w)
/ zij hebben X ongemoeid laten, en de intercept apart	, wij hadden een 1 kolom toegevoegd	, zij tellen de intercept er bij op 	, hetzelfde	,
/ dan bereken we altijd noemer pX=e^x.T.dot(w1)+ e^x.T.dot(w2)+e^x.T.dot(w3)	, en deze komt in de ml of cost in de log terecht	, en dat bereken we hier,

/ de loss (=cost):

/ de 1ste 3 regels hierboven berekent, voor elke waarneming en bijbehorende class die er bij hoort (supervised learning)	, dus p is 150 lang array (wi0=intercept)	: 
x.T.dot(w1)+w10-log(som e^...), x.T.dot(w2)+w20-log(som e^...), x.T.dot(w3)+w30-log(som e^...)

/ dan de 4de regel berekent:
t1*(x.T.dot(w1)+w10-log(som e^...))t2*(x.T.dot(w2)+w20-log(som e^...))t3*(x.T.dot(w3)+w30-log(som e^...))
/ het is gelijk aan	, maar dat doen zij niet	, wij wel	,
/ (waarin hier de intercept in w1 zit en x een extra 1 heeft)	,
t1*x.T.dot(w1)+t2*x.T.dot(w2)+t3*(x.T.dot(w3)-log(som e^...)

/ regularisatie doen we niet , we hadden C=10**10, waardoor alpha=10**-10	,

/ dan de laatste stap: np.exp(p,p) (moeten 2 keer p geven met floats TODO):
/ pi = x.T.dot(wi)-log(e^x.T.dot(w1)+e^x.T.dot(w2)+e^x.T.dot(w3)), en p= matrix met column p1,p2,p3 en rij = elke waarneming	,
/ dan is e^pi = e^x.T.dot(wi)/(e^x.T.dot(w1)+e^x.T.dot(w2)+e^x.T.dot(w3))	, en dat precies kans op i gegeven x	, wat we altijd doen	,

/ 13	. 

/ het lijkt erop dat als de dtype van een array float is	, we np.exp(a,a) moeten doen, als dtype=int	, dan moeten we np.exp(a) doen	,

In [16289]: a
Out[16289]: 
array([[1],
       [2]])

In [16290]: np.exp(a)
Out[16290]: 
array([[2.71828183],
       [7.3890561 ]])

In [16298]: a=np.array([1,2],dtype=float).reshape(-1,1)

In [16299]: a
Out[16299]: 
array([[1.],
       [2.]])

In [16300]: np.exp(a,a)
Out[16300]: 
array([[2.71828183],
       [7.3890561 ]])

In [16301]: np.exp(a)
Out[16301]: 
array([[  15.15426224],
       [1618.17799191]])

/ 7	. 

/ we komen in

$ less logistic.py

def _multinomial_loss_grad(w, X, Y, alpha, sample_weight):

    loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
    sample_weight = sample_weight[:, np.newaxis]
    diff = sample_weight * (p - Y)
/ klopt	, 
    grad[:, :n_features] = safe_sparse_dot(diff.T, X)

    if fit_intercept:
        grad[:, -1] = diff.sum(axis=0)

ndarray: [[ 1.14733333e+02  4.77333333e+01 -5.27355937e-14]
 [-2.50666667e+01 -6.36666667e+00  2.65343303e-14]
 [-8.96666667e+01 -4.13666667e+01 -6.21724894e-14]]

/ de 3 gradienten zijn rijen	,

/ Einde NEWTON-CG DEBUG



