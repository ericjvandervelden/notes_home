/ CHROME

/ Lees	,
http://www.if-not-true-then-false.com/2010/install-google-chrome-with-yum-on-fedora-red-hat-rhel/

[root@localhost ~]# cat /etc/yum.repos.d/google-chrome.repo 
[google-chrome]
name=google-chrome
baseurl=http://dl.google.com/linux/chrome/rpm/stable/x86_64
enabled=1
gpgcheck=1

[root@localhost ~]# yum install google-chrome-stable
...
warning: /var/cache/yum/x86_64/21/google-chrome/packages/google-chrome-stable-40.0.2214.94-1.x86_64.rpm: Header V4 DSA/SHA1 Signature, key ID 7fac5991: NOKEY
Public key for google-chrome-stable-40.0.2214.94-1.x86_64.rpm is not installed
(85/85): google-chrome-stable-40.0.2214.94-1.x86_64.rpm     |  59 MB  01:47     
--------------------------------------------------------------------------------
Total                                              824 kB/s |  89 MB  01:50     
Retrieving key from https://dl-ssl.google.com/linux/linux_signing_key.pub
Importing GPG key 0x7FAC5991:
 Userid     : "Google, Inc. Linux Package Signing Key <linux-packages-keymaster@google.com>"
 Fingerprint: 4cca 1eaf 950c ee4a b839 76dc a040 830f 7fac 5991
 From       : https://dl-ssl.google.com/linux/linux_signing_key.pub
Is this ok [y/N]: y
...

/ Einde CHROME

/ DOCKER

/ 7	.	

/ Lees	,
http://unix.stackexchange.com/questions/4405/how-to-make-fedora-user-a-sudoer
/ Eerst eric sudoer	,
[eric@localhost ~]$ su -c "usermod -g wheel eric"
Password: 
/ Maar root verwisselt	, 
[root@localhost ~]# visudo 
#%wheel ALL=(ALL)       ALL
%wheel  ALL=(ALL)       NOPASSWD: ALL


/ Ga naar	,
https://docs.docker.com/
/ click Installation , Fedora	,
[eric@localhost ~]$ sudo yum install docker-io
/ OK
[eric@localhost ~]$ sudo systemctl restart docker
/ OK

[eric@localhost ~]$ sudo systemctl enable docker
Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.
/ OK

/ 7	.

/ User guide,

/ Using docker hub	,

https://hub.docker.com/account/signup/
ericjvandervelden
vl$Walnoot27

/ ga via profile naar	,
https://registry.hub.docker.com/

[eric@localhost ~]$ sudo docker run fedora /bin/echo 'Hello World'
Hello World
/ Duurde lang, omdat hij een docker image van fedora moet download	, 125MB	,

/////////////////////

/ Als we een op onze repo nog niet aanwezig image run	, wordt deze download, en saved in list van images	, de download hoeven we dus maar 1 keer te doen	,
[eric@localhost Docker]$ sudo docker run -i -t  --name fd fedora:20 /usr/bin/bash
Unable to find image 'fedora:20' locally
fedora:20: The image you are pulling has been verified
782cf93a8f16: Pull complete 								/ <none> image	, 
/ TODO
6cece30db4f9: Pull complete  								/ fedora:20 image	, zojuist download
511136ea3c5a: Already exists 								/ <none> image	,
/ TODO
Status: Downloaded newer image for fedora:20
[root@e94e5fde3949 /]# 
/ In ander window	,
[eric@localhost own]$ sudo docker images
fedora                20                  6cece30db4f9        10 weeks ago        374.1 MB




[eric@localhost ~]$ sudo docker run -t -i fedora /bin/bash
bash-4.3# pwd
/
bash-4.3# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?            7 Ss       0   0:00 /bin/bash
    1     7     7     1 ?            7 R+       0   0:00 ps ajx
bash-4.3# ls
bin   dev  home  lib64	     media  opt   root	sbin  sys  usr
boot  etc  lib	 lost+found  mnt    proc  run	srv   tmp  var
bash-4.3# exit
exit
[eric@localhost ~]$ 

/ 13	. 

/ we hebben per ongeluk in de fg	, niet -d dus	,
[eric@localhost ~]$ sudo docker run -t -i fedora /bin/bash -c "while true;do echo Foo bar;sleep 1;done"
Foo bar
Foo bar
...
/ om de seconde	, 
/ ctrl+c heeft geen zin	, 

/ 13	. 

[eric@localhost ~]$ sudo docker run -d fedora /bin/bash -c "while true;do echo Foo bar;sleep 1;done"
98c0ce0397fc7bf1d1859876a0666c2eefd58196010088e243dd56589aed2e43

[eric@localhost ~]$ sudo docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES
98c0ce0397fc        fedora:latest       "/bin/bash -c 'while   31 seconds ago      Up 30 seconds                           serene_davinci      

[eric@localhost ~]$ sudo docker stop naughty_engelbart trusting_pasteur
naughty_engelbart
trusting_pasteur
/ je kunt er 2 tegelijk stop	,

[eric@localhost ~]$ sudo docker logs naughty_engelbart
Foo bar
Foo bar
...

/ 13	 .

/ Working with containers	,

/ 7	.

/ Lees	,
https://github.com/docker-training/webapp/blob/master/Dockerfile
$ vi Dockerfile

FROM ubuntu:12.04
MAINTAINER Docker Education Team <education@docker.com>
RUN apt-get update
RUN DEBIAN_FRONTEND=noninteractive apt-get install -y -q curl python-all python-pip wget
ADD ./webapp /opt/webapp/
WORKDIR /opt/webapp
RUN pip install -r requirements.txt
EXPOSE 5000
CMD ["python", "app.py"]

[eric@localhost own]$ sudo docker run --rm --name py -P training/webapp
 * Running on http://0.0.0.0:5000/
/ In ander window	,
[eric@localhost hadoop]$ sudo docker port py
5000/tcp -> 0.0.0.0:49156
/ Geef in chrome	,
http://localhost:49156
/ OK
[eric@localhost hadoop]$ sudo docker stop py
py

/ Ipv --rm doen we -d	,
/ dan kun je verder op de cmd line	,
/ we zien nog steeds py bij 'docker ps'	, 
/ we doen 'docker stop py'	, en we hoeven GEEN 'docker rm py'	, dus -d hetzelfde als --rm	,

/ 7	.



[eric@localhost ~]$ sudo docker run -d -P training/webapp
Unable to find image 'training/webapp:latest' locally
Pulling repository training/webapp
31fa814ba25a: Download complete 
511136ea3c5a: Download complete 
f10ebce2c0e1: Download complete 
82cdea7ab5b5: Download complete 
5dbd9cb5a02f: Download complete 
74fe38d11401: Download complete 
64523f641a05: Download complete 
0e2afc9aad6e: Download complete 
e8fc7643ceb1: Download complete 
733b0e3dbcee: Download complete 
a1feb043c441: Download complete 
e12923494f6a: Download complete 
a15f98c46748: Download complete 
Status: Downloaded newer image for training/webapp:latest
f79ae4ec615e9df5ad5f0e5f3e15ed680e5dc7328e2ade05c40d61775bfb68e7


/ Ga naar	,
https://registry.hub.docker.com/
/ Inderdaad, training/webapp is er,

[eric@localhost ~]$ sudo docker ps -l
CONTAINER ID        IMAGE                    COMMAND             CREATED             STATUS              PORTS                     NAMES
f79ae4ec615e        training/webapp:latest   "python app.py"     2 minutes ago       Up 2 minutes        0.0.0.0:49153->5000/tcp   berserk_wozniak     

/ Geef in chrome	,
http://localhost:49153/
/ Inderdaad, we zien 	,
Hello world!

[eric@localhost ~]$ sudo docker port berserk_wozniak
5000/tcp -> 0.0.0.0:49153
[eric@localhost ~]$ sudo docker port berserk_wozniak 5000
0.0.0.0:49153

[eric@localhost ~]$ sudo docker logs -f berserk_wozniak
 * Running on http://0.0.0.0:5000/
172.17.42.1 - - [01/Feb/2015 11:05:46] "GET / HTTP/1.1" 200 -
172.17.42.1 - - [01/Feb/2015 11:05:46] "GET /favicon.ico HTTP/1.1" 404 -

[eric@localhost ~]$ sudo docker top berserk_wozniak
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                29166               3995                0                   12:00               ?                   00:00:01            python app.py


[eric@localhost ~]$ sudo docker inspect berserk_wozniak
[{
    "Created": "2015-02-01T11:00:20.977057077Z",
    "Driver": "devicemapper",

   "HostsPath": "/var/lib/docker/containers/f79ae4ec615e9df5ad5f0e5f3e15ed680e5dc7328e2ade05c40d61775bfb68e7/hosts",
    "Id": "f79ae4ec615e9df5ad5f0e5f3e15ed680e5dc7328e2ade05c40d61775bfb68e7",
    "Image": "31fa814ba25ae3426f8710df7a48d567d4022527ef2c14964bb8bc45e653417c",

[eric@localhost ~]$ sudo ls /var/lib/docker/containers/f79ae4ec615e9df5ad5f0e5f3e15ed680e5dc7328e2ade05c40d61775bfb68e7
config.json
f79ae4ec615e9df5ad5f0e5f3e15ed680e5dc7328e2ade05c40d61775bfb68e7-json.log
hostconfig.json
hostname
hosts
resolv.conf

[eric@localhost ~]$ sudo locate 31fa814ba25ae3426f8710df7a48d567d4022527ef2c14964bb8bc45e653417c
/var/lib/docker/devicemapper/metadata/31fa814ba25ae3426f8710df7a48d567d4022527ef2c14964bb8bc45e653417c
/var/lib/docker/devicemapper/mnt/31fa814ba25ae3426f8710df7a48d567d4022527ef2c14964bb8bc45e653417c
/var/lib/docker/graph/31fa814ba25ae3426f8710df7a48d567d4022527ef2c14964bb8bc45e653417c
/var/lib/docker/graph/31fa814ba25ae3426f8710df7a48d567d4022527ef2c14964bb8bc45e653417c/json
/var/lib/docker/graph/31fa814ba25ae3426f8710df7a48d567d4022527ef2c14964bb8bc45e653417c/layersize

[eric@localhost ~]$ sudo docker stop berserk_wozniak

[eric@localhost ~]$ sudo docker start berserk_wozniak
berserk_wozniak
[eric@localhost ~]$ sudo docker ps 
CONTAINER ID        IMAGE                    COMMAND             CREATED             STATUS              PORTS                     NAMES
f79ae4ec615e        training/webapp:latest   "python app.py"     7 hours ago         Up 14 seconds       0.0.0.0:49154->5000/tcp   berserk_wozniak     

/ de port is 1 verhoogt	, 49154

/ 7	.

[eric@localhost ~]$ sudo docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
fedora              latest              834629358fe2        4 weeks ago         250.2 MB
training/webapp     latest              31fa814ba25a        8 months ago        278.6 MB

/ als we een container rm, is de image er nog steeds	, 
/ een image is create met pull, een container met run	, 
/ maar als we niet eerst pull hebben gedaan, calls run implicit pull	,

[eric@localhost ~]$ sudo docker ps -a
CONTAINER ID        IMAGE                    COMMAND                CREATED             STATUS                       PORTS               NAMES
f79ae4ec615e        training/webapp:latest   "python app.py"        8 hours ago         Exited (-1) 36 minutes ago                       berserk_wozniak      
aa88a4dcea81        fedora:latest            "/bin/bash -c 'while   8 hours ago         Exited (-1) 8 hours ago                          naughty_engelbart    
...

[eric@localhost ~]$ sudo docker rm berserk_wozniak
berserk_wozniak

[eric@localhost ~]$ sudo docker ps -a
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS                     PORTS               NAMES
aa88a4dcea81        fedora:latest       "/bin/bash -c 'while   8 hours ago         Exited (-1) 8 hours ago                        naughty_engelbart    
...

/ 13	. 

[eric@localhost ~]$ sudo docker run -i -t training/webapp /bin/bash -c "echo Foo bar"
Foo bar

/ Als we naar	,
https://registry.hub.docker.com/u/training/webapp/
/ dan zien we dat hij build is op ubuntu:12.04

/ we rm deze image	, en gaan eerst ubuntu:12.04 pull, en dan deze weer	,

/ we rm alle containers	,
[eric@localhost ~]$ sudo docker rm $(sudo docker ps -a -q)
e30371f59c10
c9c456e8e818
e268c554a290
aa88a4dcea81
ae17b266d317
98c0ce0397fc
be43367a6a29
c5494042d984
bc8ea660eafb
cc459d83cf32

[eric@localhost ~]$ sudo docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
fedora              latest              834629358fe2        4 weeks ago         250.2 MB
training/webapp     latest              31fa814ba25a        8 months ago        278.6 MB

 /we zien ook de immediate layers	,
[eric@localhost ~]$ sudo docker images -a
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
fedora              latest              834629358fe2        4 weeks ago         250.2 MB
<none>              <none>              00a0c78eeb6d        11 weeks ago        0 B
training/webapp     latest              31fa814ba25a        8 months ago        278.6 MB
<none>              <none>              a15f98c46748        8 months ago        278.6 MB
<none>              <none>              e12923494f6a        8 months ago        278.6 MB
<none>              <none>              a1feb043c441        8 months ago        273.2 MB
<none>              <none>              733b0e3dbcee        8 months ago        273.2 MB
<none>              <none>              e8fc7643ceb1        8 months ago        273.2 MB
<none>              <none>              0e2afc9aad6e        8 months ago        234.5 MB
<none>              <none>              64523f641a05        8 months ago        209.4 MB
<none>              <none>              74fe38d11401        9 months ago        209.4 MB
<none>              <none>              82cdea7ab5b5        9 months ago        103.8 MB
<none>              <none>              5dbd9cb5a02f        9 months ago        103.8 MB
<none>              <none>              f10ebce2c0e1        9 months ago        103.7 MB
<none>              <none>              511136ea3c5a        19 months ago       0 B

[eric@localhost ~]$ sudo docker rmi training/webapp
Untagged: training/webapp:latest
Deleted: 31fa814ba25ae3426f8710df7a48d567d4022527ef2c14964bb8bc45e653417c
Deleted: a15f98c467482e15cae7bede2eccf45b698ee0ca5fd18ea3120c7c35e4f5e842
Deleted: e12923494f6a48e4e6297a0a179c5d2b4ef0c21b4f9acc6c9f9b03b79cff9933
Deleted: a1feb043c441ce616cdca4b45643e8c17daa7489d546f04d98e2f1c78f06e123
Deleted: 733b0e3dbceedc3aa8f9b8c62ffa713be237176f84773a6a090a9ed6febc10cb
Deleted: e8fc7643ceb189cd2fd07da12ca8dce6a206f861727c3d6f7a77d2abf4214fa9
Deleted: 0e2afc9aad6ebae553530467e8b4a950cf6ae0db53e45f39690729abb208b5fa
Deleted: 64523f641a05a8be728b2c71e8316e9ede2e81bd735a687965ecd87cf57a0b45
Deleted: 74fe38d114018aac73c5997b95263090048ec9a1f58f33a1b53f55e92156d53b
Deleted: 5dbd9cb5a02fb27734e3dbaef8d6abf0997c137f49dd433bf3f27c8036d3348e
Deleted: 82cdea7ab5b555f53c2adf8df75b0d2ad1e49dbfc11da50df3e7ea38454ed606
Deleted: f10ebce2c0e158af1eb0dc08c9e917cc0976e7d57319defbb06ea61191d29e76

[eric@localhost ~]$ sudo docker images -a
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
fedora              latest              834629358fe2        4 weeks ago         250.2 MB
<none>              <none>              00a0c78eeb6d        11 weeks ago        0 B
<none>              <none>              511136ea3c5a        19 months ago       0 B

/ 13	. 

/ we download eerst ubuntu:12.04.

[eric@localhost ~]$ sudo docker pull ubuntu:12.04
ubuntu:12.04: The image you are pulling has been verified
bc1f0427b833: Pull complete 
0cf29d6c76b4: Pull complete 
b560c37f1efa: Pull complete 
69c02692b0c1: Pull complete 
511136ea3c5a: Already exists 
Status: Downloaded newer image for ubuntu:12.04

/ Klopt, 	511136ea3c5a zagen wel al bij fedora	,

[eric@localhost ~]$ sudo docker images -a
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
ubuntu              12.04               69c02692b0c1        5 days ago          131.5 MB
<none>              <none>              b560c37f1efa        5 days ago          131.5 MB
<none>              <none>              0cf29d6c76b4        5 days ago          131.5 MB
<none>              <none>              bc1f0427b833        5 days ago          131.3 MB
fedora              latest              834629358fe2        4 weeks ago         250.2 MB
<none>              <none>              00a0c78eeb6d        11 weeks ago        0 B
<none>              <none>              511136ea3c5a        19 months ago       0 B

Pulling repository training/webapp
31fa814ba25a: Download complete 
511136ea3c5a: Download complete 
f10ebce2c0e1: Download complete 
82cdea7ab5b5: Download complete 
5dbd9cb5a02f: Download complete 
74fe38d11401: Download complete 
64523f641a05: Download complete 
0e2afc9aad6e: Download complete 
e8fc7643ceb1: Download complete 
733b0e3dbcee: Download complete 
a1feb043c441: Download complete 
e12923494f6a: Download complete 
a15f98c46748: Download complete 
Status: Downloaded newer image for training/webapp:latest

[eric@localhost ~]$ sudo docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
ubuntu              12.04               69c02692b0c1        5 days ago          131.5 MB
fedora              latest              834629358fe2        4 weeks ago         250.2 MB
training/webapp     latest              31fa814ba25a        8 months ago        278.6 MB

[eric@localhost ~]$ sudo docker images -a
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
ubuntu              12.04               69c02692b0c1        5 days ago          131.5 MB
<none>              <none>              b560c37f1efa        5 days ago          131.5 MB
<none>              <none>              0cf29d6c76b4        5 days ago          131.5 MB
<none>              <none>              bc1f0427b833        5 days ago          131.3 MB

fedora              latest              834629358fe2        4 weeks ago         250.2 MB
<none>              <none>              00a0c78eeb6d        11 weeks ago        0 B

training/webapp     latest              31fa814ba25a        8 months ago        278.6 MB
<none>              <none>              a15f98c46748        8 months ago        278.6 MB
<none>              <none>              e12923494f6a        8 months ago        278.6 MB
<none>              <none>              a1feb043c441        8 months ago        273.2 MB
<none>              <none>              733b0e3dbcee        8 months ago        273.2 MB
<none>              <none>              e8fc7643ceb1        8 months ago        273.2 MB
<none>              <none>              0e2afc9aad6e        8 months ago        234.5 MB
<none>              <none>              64523f641a05        8 months ago        209.4 MB
<none>              <none>              74fe38d11401        9 months ago        209.4 MB
<none>              <none>              82cdea7ab5b5        9 months ago        103.8 MB
<none>              <none>              5dbd9cb5a02f        9 months ago        103.8 MB
<none>              <none>              f10ebce2c0e1        9 months ago        103.7 MB
<none>              <none>              511136ea3c5a        19 months ago       0 B

[eric@localhost ~]$ sudo docker run -i -t training/webapp /bin/bash
root@18f23729d75b:/opt/webapp# uname -a
Linux 18f23729d75b 3.18.3-201.fc21.x86_64 #1 SMP Mon Jan 19 15:59:31 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

[eric@localhost ~]$ sudo docker run -i -t ubuntu:12.04 /bin/bash
root@fab615504dfc:/# uname -a
Linux fab615504dfc 3.18.3-201.fc21.x86_64 #1 SMP Mon Jan 19 15:59:31 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
/ dit is altijd zo	,
/ TODO

/ 7	. 

/ we gaan nog een keer terug naar	,
https://docs.docker.com/userguide/dockerizing/

[eric@localhost ~]$ sudo docker pull ubuntu
ubuntu:latest: The image you are pulling has been verified
27d47432a69b: Pull complete 
5f92234dcf1e: Pull complete 
51a9c7c1f8bb: Pull complete 
5ba9dab47459: Pull complete 
511136ea3c5a: Already exists 

[eric@localhost ~]$ sudo docker pull ubuntu:14.04
ubuntu:14.04: The image you are pulling has been verified
511136ea3c5a: Already exists 
27d47432a69b: Already exists 
5f92234dcf1e: Already exists 
51a9c7c1f8bb: Already exists 
5ba9dab47459: Already exists 
Status: Image is up to date for ubuntu:14.04

/ dus ubuntu:latest=ubuntu:14.04

[eric@localhost ~]$ sudo docker search sinatra
...
[eric@localhost ~]$ sudo  docker pull training/sinatra
Pulling repository training/sinatra
f0f4ab557f95: Download complete 
511136ea3c5a: Download complete 
3e76c0a80540: Download complete 
be88c4c27e80: Download complete 
bfab314f3b76: Download complete 
e809f156dc98: Download complete 
ce80548340bb: Download complete 
79e6bf39f993: Download complete 
Status: Downloaded newer image for training/sinatra:latest

[eric@localhost ~]$ sudo docker run -t -i training/sinatra /bin/bash
root@d947eb45cf47:/#  gem install json
Fetching: json-1.8.2.gem (100%)
Building native extensions.  This could take a while...
Successfully installed json-1.8.2
1 gem installed
Installing ri documentation for json-1.8.2...
Installing RDoc documentation for json-1.8.2...

/ we rm alle containers	,
[eric@localhost ~]$ sudo docker rm $(sudo docker ps -a -q)

/ In root@d947eb45cf47 is d947eb45cf47 de ID van de container	,

[eric@localhost ~]$ sudo docker start -i d947eb45cf47
d947eb45cf47
            root@d947eb45cf47:/# gem list
*** LOCAL GEMS ***
json (1.8.2)
...
root@d947eb45cf47:/# exit
exit
[eric@localhost ~]$ sudo docker ps -a
CONTAINER ID        IMAGE                     COMMAND             CREATED             STATUS                      PORTS               NAMES
d947eb45cf47        training/sinatra:latest   "/bin/bash"         52 minutes ago      Exited (0) 34 seconds ago                       sick_pare           

[eric@localhost ~]$ sudo docker commit -m "Added json gem" -a "Eric J." d947eb45cf47 ouruser/sinatra:v2
2e7e0bb2a0fd1013a3ae612859385620fb3a4618a34d67bfab40e28e2453fa0c

/ commit a copy of this container to an image	,

/ run :  image -> container	,
/ commit: image <- container

[eric@localhost ~]$ sudo docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
ouruser/sinatra     v2                  2e7e0bb2a0fd        5 minutes ago       451.7 MB
...

[eric@localhost ~]$ sudo docker run -t -i ouruser/sinatra:v2 /bin/bash
root@b0ae53d405d2:/# gem list
*** LOCAL GEMS ***
json (1.8.2)
...
root@e8c8fec9aa2e:/# exit
exit
[eric@localhost ~]$ sudo docker ps -a
CONTAINER ID        IMAGE                     COMMAND             CREATED             STATUS                      PORTS               NAMES
b0ae53d405d2        ouruser/sinatra:v2        "/bin/bash"         5 seconds ago       Exited (0) 1 seconds ago                        elated_archimedes   
d947eb45cf47        training/sinatra:latest   "/bin/bash"         About an hour ago   Exited (0) 13 minutes ago                       sick_pare           

[eric@localhost sinatra]$ sudo docker rmi 2e7e0bb2a0fd
Error response from daemon: Conflict, cannot delete 2e7e0bb2a0fd because the container e8c8fec9aa2e is using it, use -f to force

/ we moeten eerst de container rm	,

[eric@localhost ~]$ sudo docker rm b0ae53d405d2
b0ae53d405d2

[eric@localhost sinatra]$ sudo docker rmi 2e7e0bb2a0fd
Untagged: ouruser/sinatra:v2
Deleted: 2e7e0bb2a0fd1013a3ae612859385620fb3a4618a34d67bfab40e28e2453fa0c

/ 7	.

/ Docker files

/ we gaan zelf ouruser/sinatra:v2 maken	,

[eric@localhost sinatra]$ pwd
/home/eric/Devel/Docker/sinatra
$ vi Dockerfile
FROM ubuntu:14.04
MAINTAINER Eric J. Van der Velden ericjvandervelden@gmail.com
RUN apt-get update && apt-get install -y ruby ruby-dev
RUN gem install sinatra

/ build -rm=true: rm intermediate containers na build van image	,
/ inderdaad : intermediate CONTAINERs na build IMAGE	,

/ build -t : repository name van image na build 

[eric@localhost sinatra]$ sudo docker build -t ouruser/sinatra:v2 .
Sending build context to Docker daemon 2.048 kB
Sending build context to Docker daemon 
Step 0 : FROM ubuntu:14.04
 ---> 5ba9dab47459
Step 1 : MAINTAINER Eric J. Van der Velden ericjvandervelden@gmail.com
 ---> Running in 16b24a10cb37
 ---> cab82b1927dd
Removing intermediate container 16b24a10cb37
Step 2 : RUN apt-get update && apt-get install -y ruby ruby-dev
 ---> Running in 684762db7711
Ign http://archive.ubuntu.com trusty InRelease
Ign http://archive.ubuntu.com trusty-updates InRelease
Ign http://archive.ubuntu.com trusty-security InRelease
Hit http://archive.ubuntu.com trusty Release.gpg
Get:1 http://archive.ubuntu.com trusty-updates Release.gpg [933 B]
Get:2 http://archive.ubuntu.com trusty-security Release.gpg [933 B]
Hit http://archive.ubuntu.com trusty Release
Get:3 http://archive.ubuntu.com trusty-updates Release [62.0 kB]
Get:4 http://archive.ubuntu.com trusty-security Release [62.0 kB]
Get:5 http://archive.ubuntu.com trusty/main Sources [1335 kB]
Get:6 http://archive.ubuntu.com trusty/restricted Sources [5335 B]
Get:7 http://archive.ubuntu.com trusty/universe Sources [7926 kB]
Get:8 http://archive.ubuntu.com trusty/main amd64 Packages [1743 kB]
Get:9 http://archive.ubuntu.com trusty/restricted amd64 Packages [16.0 kB]
Get:10 http://archive.ubuntu.com trusty/universe amd64 Packages [7589 kB]
Get:11 http://archive.ubuntu.com trusty-updates/main Sources [207 kB]
Get:12 http://archive.ubuntu.com trusty-updates/restricted Sources [1874 B]
Get:13 http://archive.ubuntu.com trusty-updates/universe Sources [124 kB]
Get:14 http://archive.ubuntu.com trusty-updates/main amd64 Packages [524 kB]
Get:15 http://archive.ubuntu.com trusty-updates/restricted amd64 Packages [14.8 kB]
Get:16 http://archive.ubuntu.com trusty-updates/universe amd64 Packages [318 kB]
Get:17 http://archive.ubuntu.com trusty-security/main Sources [79.8 kB]
Get:18 http://archive.ubuntu.com trusty-security/restricted Sources [1874 B]
Get:19 http://archive.ubuntu.com trusty-security/universe Sources [19.1 kB]
Get:20 http://archive.ubuntu.com trusty-security/main amd64 Packages [251 kB]
Get:21 http://archive.ubuntu.com trusty-security/restricted amd64 Packages [14.8 kB]
Get:22 http://archive.ubuntu.com trusty-security/universe amd64 Packages [110 kB]
Fetched 20.4 MB in 1min 1s (334 kB/s)
Reading package lists...
Reading package lists...
Building dependency tree...
Reading state information...
The following extra packages will be installed:
  binutils ca-certificates cpp cpp-4.8 gcc gcc-4.8 libasan0 libatomic1
  libc-dev-bin libc6-dev libcloog-isl4 libgcc-4.8-dev libgmp10 libgomp1
  libisl10 libitm1 libmpc3 libmpfr4 libquadmath0 libruby1.9.1 libtsan0
  libyaml-0-2 linux-libc-dev manpages manpages-dev openssl ruby1.9.1
  ruby1.9.1-dev
Suggested packages:
  binutils-doc cpp-doc gcc-4.8-locales gcc-multilib make autoconf automake1.9
  libtool flex bison gdb gcc-doc gcc-4.8-multilib gcc-4.8-doc libgcc1-dbg
  libgomp1-dbg libitm1-dbg libatomic1-dbg libasan0-dbg libtsan0-dbg
  libbacktrace1-dbg libquadmath0-dbg binutils-gold glibc-doc man-browser ri
  ruby1.9.1-examples ri1.9.1 graphviz ruby-switch
The following NEW packages will be installed:
  binutils ca-certificates cpp cpp-4.8 gcc gcc-4.8 libasan0 libatomic1
  libc-dev-bin libc6-dev libcloog-isl4 libgcc-4.8-dev libgmp10 libgomp1
  libisl10 libitm1 libmpc3 libmpfr4 libquadmath0 libruby1.9.1 libtsan0
  libyaml-0-2 linux-libc-dev manpages manpages-dev openssl ruby ruby-dev
  ruby1.9.1 ruby1.9.1-dev
0 upgraded, 30 newly installed, 0 to remove and 3 not upgraded.
Need to get 24.0 MB of archives.
After this operation, 89.9 MB of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu/ trusty/main libasan0 amd64 4.8.2-19ubuntu1 [63.0 kB]
Get:2 http://archive.ubuntu.com/ubuntu/ trusty/main libatomic1 amd64 4.8.2-19ubuntu1 [8626 B]
Get:3 http://archive.ubuntu.com/ubuntu/ trusty/main libgmp10 amd64 2:5.1.3+dfsg-1ubuntu1 [218 kB]
Get:4 http://archive.ubuntu.com/ubuntu/ trusty/main libisl10 amd64 0.12.2-1 [419 kB]
Get:5 http://archive.ubuntu.com/ubuntu/ trusty/main libcloog-isl4 amd64 0.18.2-1 [57.5 kB]
Get:6 http://archive.ubuntu.com/ubuntu/ trusty/main libgomp1 amd64 4.8.2-19ubuntu1 [23.2 kB]
Get:7 http://archive.ubuntu.com/ubuntu/ trusty/main libitm1 amd64 4.8.2-19ubuntu1 [28.5 kB]
Get:8 http://archive.ubuntu.com/ubuntu/ trusty/main libmpfr4 amd64 3.1.2-1 [203 kB]
Get:9 http://archive.ubuntu.com/ubuntu/ trusty/main libquadmath0 amd64 4.8.2-19ubuntu1 [126 kB]
Get:10 http://archive.ubuntu.com/ubuntu/ trusty/main libtsan0 amd64 4.8.2-19ubuntu1 [94.7 kB]
Get:11 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libyaml-0-2 amd64 0.1.4-3ubuntu3.1 [48.1 kB]
Get:12 http://archive.ubuntu.com/ubuntu/ trusty/main libmpc3 amd64 1.0.1-1ubuntu1 [38.4 kB]
Get:13 http://archive.ubuntu.com/ubuntu/ trusty-updates/main openssl amd64 1.0.1f-1ubuntu2.8 [489 kB]
Get:14 http://archive.ubuntu.com/ubuntu/ trusty/main ca-certificates all 20130906ubuntu2 [175 kB]
Get:15 http://archive.ubuntu.com/ubuntu/ trusty/main manpages all 3.54-1ubuntu1 [627 kB]
Get:16 http://archive.ubuntu.com/ubuntu/ trusty/main binutils amd64 2.24-5ubuntu3 [2071 kB]
Get:17 http://archive.ubuntu.com/ubuntu/ trusty/main cpp-4.8 amd64 4.8.2-19ubuntu1 [4439 kB]
Get:18 http://archive.ubuntu.com/ubuntu/ trusty/main cpp amd64 4:4.8.2-1ubuntu6 [27.5 kB]
Get:19 http://archive.ubuntu.com/ubuntu/ trusty/main libgcc-4.8-dev amd64 4.8.2-19ubuntu1 [1688 kB]
Get:20 http://archive.ubuntu.com/ubuntu/ trusty/main gcc-4.8 amd64 4.8.2-19ubuntu1 [5012 kB]
Get:21 http://archive.ubuntu.com/ubuntu/ trusty/main gcc amd64 4:4.8.2-1ubuntu6 [5098 B]
Get:22 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libc-dev-bin amd64 2.19-0ubuntu6.5 [69.0 kB]
Get:23 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-libc-dev amd64 3.13.0-45.74 [790 kB]
Get:24 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libc6-dev amd64 2.19-0ubuntu6.5 [1912 kB]
Get:25 http://archive.ubuntu.com/ubuntu/ trusty/main ruby all 1:1.9.3.4 [5334 B]
Get:26 http://archive.ubuntu.com/ubuntu/ trusty-updates/main ruby1.9.1 amd64 1.9.3.484-2ubuntu1.2 [35.6 kB]
Get:27 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libruby1.9.1 amd64 1.9.3.484-2ubuntu1.2 [2645 kB]
Get:28 http://archive.ubuntu.com/ubuntu/ trusty/main manpages-dev all 3.54-1ubuntu1 [1820 kB]
Get:29 http://archive.ubuntu.com/ubuntu/ trusty-updates/main ruby1.9.1-dev amd64 1.9.3.484-2ubuntu1.2 [871 kB]
Get:30 http://archive.ubuntu.com/ubuntu/ trusty/main ruby-dev all 1:1.9.3.4 [4660 B]
debconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
debconf: unable to initialize frontend: Readline
debconf: (This frontend requires a controlling tty.)
debconf: falling back to frontend: Teletype
dpkg-preconfigure: unable to re-open stdin: 
Fetched 24.0 MB in 1min 17s (312 kB/s)
Selecting previously unselected package libasan0:amd64.
(Reading database ... 11527 files and directories currently installed.)
Preparing to unpack .../libasan0_4.8.2-19ubuntu1_amd64.deb ...
Unpacking libasan0:amd64 (4.8.2-19ubuntu1) ...
Selecting previously unselected package libatomic1:amd64.
Preparing to unpack .../libatomic1_4.8.2-19ubuntu1_amd64.deb ...
Unpacking libatomic1:amd64 (4.8.2-19ubuntu1) ...
Selecting previously unselected package libgmp10:amd64.
Preparing to unpack .../libgmp10_2%3a5.1.3+dfsg-1ubuntu1_amd64.deb ...
Unpacking libgmp10:amd64 (2:5.1.3+dfsg-1ubuntu1) ...
Selecting previously unselected package libisl10:amd64.
Preparing to unpack .../libisl10_0.12.2-1_amd64.deb ...
Unpacking libisl10:amd64 (0.12.2-1) ...
Selecting previously unselected package libcloog-isl4:amd64.
Preparing to unpack .../libcloog-isl4_0.18.2-1_amd64.deb ...
Unpacking libcloog-isl4:amd64 (0.18.2-1) ...
Selecting previously unselected package libgomp1:amd64.
Preparing to unpack .../libgomp1_4.8.2-19ubuntu1_amd64.deb ...
Unpacking libgomp1:amd64 (4.8.2-19ubuntu1) ...
Selecting previously unselected package libitm1:amd64.
Preparing to unpack .../libitm1_4.8.2-19ubuntu1_amd64.deb ...
Unpacking libitm1:amd64 (4.8.2-19ubuntu1) ...
Selecting previously unselected package libmpfr4:amd64.
Preparing to unpack .../libmpfr4_3.1.2-1_amd64.deb ...
Unpacking libmpfr4:amd64 (3.1.2-1) ...
Selecting previously unselected package libquadmath0:amd64.
Preparing to unpack .../libquadmath0_4.8.2-19ubuntu1_amd64.deb ...
Unpacking libquadmath0:amd64 (4.8.2-19ubuntu1) ...
Selecting previously unselected package libtsan0:amd64.
Preparing to unpack .../libtsan0_4.8.2-19ubuntu1_amd64.deb ...
Unpacking libtsan0:amd64 (4.8.2-19ubuntu1) ...
Selecting previously unselected package libyaml-0-2:amd64.
Preparing to unpack .../libyaml-0-2_0.1.4-3ubuntu3.1_amd64.deb ...
Unpacking libyaml-0-2:amd64 (0.1.4-3ubuntu3.1) ...
Selecting previously unselected package libmpc3:amd64.
Preparing to unpack .../libmpc3_1.0.1-1ubuntu1_amd64.deb ...
Unpacking libmpc3:amd64 (1.0.1-1ubuntu1) ...
Selecting previously unselected package openssl.
Preparing to unpack .../openssl_1.0.1f-1ubuntu2.8_amd64.deb ...
Unpacking openssl (1.0.1f-1ubuntu2.8) ...
Selecting previously unselected package ca-certificates.
Preparing to unpack .../ca-certificates_20130906ubuntu2_all.deb ...
Unpacking ca-certificates (20130906ubuntu2) ...
Selecting previously unselected package manpages.
Preparing to unpack .../manpages_3.54-1ubuntu1_all.deb ...
Unpacking manpages (3.54-1ubuntu1) ...
Selecting previously unselected package binutils.
Preparing to unpack .../binutils_2.24-5ubuntu3_amd64.deb ...
Unpacking binutils (2.24-5ubuntu3) ...
Selecting previously unselected package cpp-4.8.
Preparing to unpack .../cpp-4.8_4.8.2-19ubuntu1_amd64.deb ...
Unpacking cpp-4.8 (4.8.2-19ubuntu1) ...
Selecting previously unselected package cpp.
Preparing to unpack .../cpp_4%3a4.8.2-1ubuntu6_amd64.deb ...
Unpacking cpp (4:4.8.2-1ubuntu6) ...
Selecting previously unselected package libgcc-4.8-dev:amd64.
Preparing to unpack .../libgcc-4.8-dev_4.8.2-19ubuntu1_amd64.deb ...
Unpacking libgcc-4.8-dev:amd64 (4.8.2-19ubuntu1) ...
Selecting previously unselected package gcc-4.8.
Preparing to unpack .../gcc-4.8_4.8.2-19ubuntu1_amd64.deb ...
Unpacking gcc-4.8 (4.8.2-19ubuntu1) ...
Selecting previously unselected package gcc.
Preparing to unpack .../gcc_4%3a4.8.2-1ubuntu6_amd64.deb ...
Unpacking gcc (4:4.8.2-1ubuntu6) ...
Selecting previously unselected package libc-dev-bin.
Preparing to unpack .../libc-dev-bin_2.19-0ubuntu6.5_amd64.deb ...
Unpacking libc-dev-bin (2.19-0ubuntu6.5) ...
Selecting previously unselected package linux-libc-dev:amd64.
Preparing to unpack .../linux-libc-dev_3.13.0-45.74_amd64.deb ...
Unpacking linux-libc-dev:amd64 (3.13.0-45.74) ...
Selecting previously unselected package libc6-dev:amd64.
Preparing to unpack .../libc6-dev_2.19-0ubuntu6.5_amd64.deb ...
Unpacking libc6-dev:amd64 (2.19-0ubuntu6.5) ...
Selecting previously unselected package ruby.
Preparing to unpack .../ruby_1%3a1.9.3.4_all.deb ...
Unpacking ruby (1:1.9.3.4) ...
Selecting previously unselected package ruby1.9.1.
Preparing to unpack .../ruby1.9.1_1.9.3.484-2ubuntu1.2_amd64.deb ...
Unpacking ruby1.9.1 (1.9.3.484-2ubuntu1.2) ...
Selecting previously unselected package libruby1.9.1.
Preparing to unpack .../libruby1.9.1_1.9.3.484-2ubuntu1.2_amd64.deb ...
Unpacking libruby1.9.1 (1.9.3.484-2ubuntu1.2) ...
Selecting previously unselected package manpages-dev.
Preparing to unpack .../manpages-dev_3.54-1ubuntu1_all.deb ...
Unpacking manpages-dev (3.54-1ubuntu1) ...
Selecting previously unselected package ruby1.9.1-dev.
Preparing to unpack .../ruby1.9.1-dev_1.9.3.484-2ubuntu1.2_amd64.deb ...
Unpacking ruby1.9.1-dev (1.9.3.484-2ubuntu1.2) ...
Selecting previously unselected package ruby-dev.
Preparing to unpack .../ruby-dev_1%3a1.9.3.4_all.deb ...
Unpacking ruby-dev (1:1.9.3.4) ...
Setting up libasan0:amd64 (4.8.2-19ubuntu1) ...
Setting up libatomic1:amd64 (4.8.2-19ubuntu1) ...
Setting up libgmp10:amd64 (2:5.1.3+dfsg-1ubuntu1) ...
Setting up libisl10:amd64 (0.12.2-1) ...
Setting up libcloog-isl4:amd64 (0.18.2-1) ...
Setting up libgomp1:amd64 (4.8.2-19ubuntu1) ...
Setting up libitm1:amd64 (4.8.2-19ubuntu1) ...
Setting up libmpfr4:amd64 (3.1.2-1) ...
Setting up libquadmath0:amd64 (4.8.2-19ubuntu1) ...
Setting up libtsan0:amd64 (4.8.2-19ubuntu1) ...
Setting up libyaml-0-2:amd64 (0.1.4-3ubuntu3.1) ...
Setting up libmpc3:amd64 (1.0.1-1ubuntu1) ...
Setting up openssl (1.0.1f-1ubuntu2.8) ...
Setting up ca-certificates (20130906ubuntu2) ...
debconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
debconf: unable to initialize frontend: Readline
debconf: (This frontend requires a controlling tty.)
debconf: falling back to frontend: Teletype
Setting up manpages (3.54-1ubuntu1) ...
Setting up binutils (2.24-5ubuntu3) ...
Setting up cpp-4.8 (4.8.2-19ubuntu1) ...
Setting up cpp (4:4.8.2-1ubuntu6) ...
Setting up libgcc-4.8-dev:amd64 (4.8.2-19ubuntu1) ...
Setting up gcc-4.8 (4.8.2-19ubuntu1) ...
Setting up gcc (4:4.8.2-1ubuntu6) ...
Setting up libc-dev-bin (2.19-0ubuntu6.5) ...
Setting up linux-libc-dev:amd64 (3.13.0-45.74) ...
Setting up libc6-dev:amd64 (2.19-0ubuntu6.5) ...
Setting up manpages-dev (3.54-1ubuntu1) ...
Setting up libruby1.9.1 (1.9.3.484-2ubuntu1.2) ...
Setting up ruby1.9.1-dev (1.9.3.484-2ubuntu1.2) ...
Setting up ruby-dev (1:1.9.3.4) ...
Setting up ruby (1:1.9.3.4) ...
Setting up ruby1.9.1 (1.9.3.484-2ubuntu1.2) ...
Processing triggers for libc-bin (2.19-0ubuntu6.5) ...
Processing triggers for ca-certificates (20130906ubuntu2) ...
Updating certificates in /etc/ssl/certs... 164 added, 0 removed; done.
Running hooks in /etc/ca-certificates/update.d....done.
 ---> 3fc5618801d1
Removing intermediate container 684762db7711
Step 3 : RUN gem install sinatra
 ---> Running in d0e5bd192489
unable to convert "\xC3" to UTF-8 in conversion from ASCII-8BIT to UTF-8 to US-ASCII for README.rdoc, skipping
unable to convert "\xC3" to UTF-8 in conversion from ASCII-8BIT to UTF-8 to US-ASCII for README.rdoc, skipping
Successfully installed rack-1.6.0
Successfully installed tilt-1.4.1
Successfully installed rack-protection-1.5.3
Successfully installed sinatra-1.4.5
4 gems installed
Installing ri documentation for rack-1.6.0...
Installing ri documentation for tilt-1.4.1...
Installing ri documentation for rack-protection-1.5.3...
Installing ri documentation for sinatra-1.4.5...
Installing RDoc documentation for rack-1.6.0...
Installing RDoc documentation for tilt-1.4.1...
Installing RDoc documentation for rack-protection-1.5.3...
Installing RDoc documentation for sinatra-1.4.5...
 ---> 66d64dc5a36c
Removing intermediate container d0e5bd192489
Successfully built 66d64dc5a36c
[eric@localhost sinatra]$ sudo docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
ouruser/sinatra     v2                  66d64dc5a36c        51 minutes ago      321 MB
training/sinatra    latest              f0f4ab557f95        8 months ago        446.9 MB
/ Deze is kleiner als die we hadden download	,
/ Daar zit WH meer in	,

[eric@localhost sinatra]$ sudo docker run -t -i ouruser/sinatra:v2 /bin/bash
root@ab523363add1:/# exit
exit
[eric@localhost sinatra]$ sudo docker ps -a
CONTAINER ID        IMAGE                     COMMAND             CREATED             STATUS                         PORTS               NAMES
ab523363add1        ouruser/sinatra:v2        "/bin/bash"         25 seconds ago      Exited (0) 9 seconds ago                           silly_stallman      
d947eb45cf47        training/sinatra:latest   "/bin/bash"         2 hours ago         Exited (0) About an hour ago                       sick_pare           

/ 7	.

/ linking	,

[eric@localhost sinatra]$ sudo docker start -i 6447314d5501 python app.py
FATA[0000] You cannot start and attach multiple containers at once. 

[eric@localhost sinatra]$ sudo docker start -i 6447314d5501
root@6447314d5501:/opt/webapp# python app.py 
 * Running on http://0.0.0.0:5000/
/ Maar localhost:5000 runs NIET	,

So now you can run any command in running container just knowing its ID: 
docker exec <container_id> echo "Hello from container!"
/ TODO

$ sudo docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py
/ chrome	?
/ TODO

[eric@localhost sinatra]$ sudo docker rm $(sudo docker stop $(sudo docker ps -a -q))
7eb3e99c21b0
40d63a7d59dd
59470b4c02bf



/ 13	.

[eric@localhost sinatra]$ sudo docker run -d -P --name web training/webapp python app.py
[eric@localhost sinatra]$ sudo docker port web
5000/tcp -> 0.0.0.0:49155
[eric@localhost sinatra]$ sudo docker rm $(sudo docker stop $(sudo docker ps -a -q))

/ we kunnen ook deze container in de fg start, zonder -d dus, en met --rm	, als we dan ctrl+c doen om te cancel, dan is de container ook rm	,

/ --rm kan niet met -d	,

[eric@localhost sinatra]$ sudo docker run -P --name web --rm training/webapp pythoapp.py
 * Running on http://0.0.0.0:5000/
/ In ander window	,
[eric@localhost ~]$ sudo docker port web
5000/tcp -> 0.0.0.0:49158

/ doe ctrl+c op 1ste window	,


/ 7	.

[eric@localhost ~]$ sudo docker pull training/postgres
Pulling repository training/postgres
258105bea10d: Download complete 
511136ea3c5a: Download complete 
35f6dd4dd141: Download complete 
7baf0ef6f14a: Download complete 
e497c7c1bfbb: Download complete 
5cf8fd909c6c: Download complete 
8726e050fbc9: Download complete 
043c01407567: Download complete 
65a89e6a06f8: Download complete 
6af9ddfabfd3: Download complete 
316f4525806b: Download complete 
bfbc096044e3: Download complete 
444db2eae2c3: Download complete 
e06e512105c3: Download complete 
Status: Downloaded newer image for training/postgres:latest

/ we kunnen ook de Dockerfile copy	, 
[eric@localhost ubuntu]$ pwd
/home/eric/Devel/Docker/postgresql/ubuntu
[eric@localhost ubuntu]$ cat Dockerfile 

from ubuntu:14.04
MAINTAINER Docker Education Team <education@docker.com>

ENV PG_VERSION 9.3
RUN apt-get -y install postgresql postgresql-client postgresql-contrib

RUN echo "host    all             all             0.0.0.0/0 trust" >> /etc/postgresql/$PG_VERSION/main/pg_hba.conf
RUN echo "listen_addresses='*'" >> /etc/postgresql/$PG_VERSION/main/postgresql.conf

RUN service postgresql start && \
 su postgres sh -c "createuser -d -r -s docker" && \
 su postgres sh -c "createdb -O docker docker" && \
 su postgres sh -c "psql -c \"GRANT ALL PRIVILEGES ON DATABASE docker to docker;\""

EXPOSE 5432
CMD ["su", "postgres", "-c", "/usr/lib/postgresql/$PG_VERSION/bin/postgres -D /var/lib/postgresql/$PG_VERSION/main/ -c config_file=/etc/postgresql/$PG_VERSION/main/postgresql.conf"]

/ OK, als we de container run, dan wordt postgres start	, en verder niets	, we kunnen er psql naar	,
 als we 
cmd service postgresql start
/ dan returns de container 	, en verdwijnt	,



[eric@localhost sinatra]$ sudo docker run --rm --name db training/postgres
2015-02-07 08:46:32 UTC LOG:  database system was interrupted; last known up at 2014-06-02 23:51:25 UTC
2015-02-07 08:46:32 UTC LOG:  database system was not properly shut down; automatic recovery in progress
2015-02-07 08:46:32 UTC LOG:  redo starts at 0/1782F38
2015-02-07 08:46:32 UTC LOG:  record with zero length at 0/17835C8
2015-02-07 08:46:32 UTC LOG:  redo done at 0/1783598
2015-02-07 08:46:32 UTC LOG:  last completed transaction was at log time 2014-06-02 23:51:25.978993+00
2015-02-07 08:46:32 UTC LOG:  autovacuum launcher started
2015-02-07 08:46:32 UTC LOG:  database system is ready to accept connections

/ Intermezzo 

/ we kunnen connect 	,

[eric@localhost ubuntu]$ sudo docker port db
5432/tcp -> 0.0.0.0:49153

[eric@localhost ubuntu]$ sudo su - postgres -c "psql -p 49153"
/ NEE

[eric@localhost ubuntu]$ sudo su - postgres -c "psql -h localhost -p 49153"
postgres=# \q

[eric@localhost ubuntu]$ sudo su - postgres -c "psql -U docker -h localhost -p 49153"
docker=# \q

[eric@localhost ubuntu]$ sudo su  -c "psql -U docker -h localhost -p 49153"
docker=# \q

[eric@localhost ubuntu]$ psql -U docker -h localhost -p 49153
docker=# \q

[eric@localhost ubuntu]$ psql -U postgres -h localhost -p 49153
postgres=#  

/ als we op onze machine	,
[eric@localhost ubuntu]$ psql -U postgres  -h localhost 
Password for user postgres: 
/ Dat komt omdat in de conf file van training/postgres	,
RUN echo "host    all             all             0.0.0.0/0 trust" >> /etc/postgresql/$PG_VERSION/main/pg_hba.conf

/ de pg role postgres bestaat, net als eric, 
postgres=> \dg
                             List of roles
 Role name |                   Attributes                   | Member of 
-----------+------------------------------------------------+-----------
 eric      | Create role, Create DB                         | {}
 foo       | Create role, Create DB                         | {}
 postgres  | Superuser, Create role, Create DB, Replication | {}
 root      |                                                | {}
/ alleen de auth gaat via peer met linux accounts	, en met md5 met foo bijv	, 
////////////////////////
/ maar als we -h localhost geven, gaat dus de auth van postgres, eric ook via md5!

/ Einde Intermezzo 


[eric@localhost sinatra]$ sudo docker run -P --name web --link db:db --rm training/webapp python app.py
 * Running on http://0.0.0.0:5000/
[eric@localhost ~]$ sudo docker ps -a
CONTAINER ID        IMAGE                      COMMAND                CREATED             STATUS              PORTS                     NAMES
b8e1d16ba792        training/webapp:latest     "python app.py"        19 seconds ago      Up 17 seconds       0.0.0.0:49159->5000/tcp   web                 
c48825caa06c        training/postgres:latest   "su postgres -c '/us   2 minutes ago       Up 2 minutes        5432/tcp                  db                  

/ Hoe created?
/ Ga naar docker hub, search training/postgres, click Dockerfile 	,
https://registry.hub.docker.com/u/training/postgres/dockerfile/
/ of click rechts onder Build Details, % Source Project Page	,
https://github.com/docker-training/postgres
/ en lees daar de Dockerfile	,

/ we kunnen ook	,
[eric@localhost hadoop]$ sudo docker run -P --name web --link db:mydb --rm -it training/webapp /bin/bash
root@7ae814b519bd:/opt/webapp# 


/ 13	. 

/ clone een image: als we in local repo al ubuntu:14.04 hebben, gaat hij deze niet download	,

/ Lees	,
https://groups.google.com/forum/#!topic/docker-user/MR5zW0toJns

[eric@localhost ubuntu]$ pwd
/home/eric/Devel/Docker/postgresql/ubuntu
[eric@localhost ubuntu]$ cat Dockerfile 
FROM ubuntu:14.04
MAINTAINER Eric J. van der Velden <ericjvandervelden@gmail.com>

[eric@localhost ubuntu]$ sudo docker build -t eric/test .
Sending build context to Docker daemon 2.048 kB
Sending build context to Docker daemon 
Step 0 : FROM ubuntu:14.04
 ---> 5ba9dab47459
Step 1 : MAINTAINER Eric J. van der Velden <ericjvandervelden@gmail.com>
 ---> Running in f73db031f655
 ---> 338a861adba4
Removing intermediate container f73db031f655
Successfully built 338a861adba4

[eric@localhost ]$ sudo docker rmi 338a861adba4
Untagged: ericj/test:latest
Deleted: 338a861adba4cc03aee28bd200649d4e889e4211fb9e5700523e2f62e5ceed44

/ 7	.

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/postgresql/fedora
[eric@localhost fedora]$ cat Dockerfile 
from fedora:21 
maintainer Eric J. van der Velden <ericjvandervelden@gmail.com>

[eric@localhost fedora]$ sudo docker build -t eric/fedora:21 .

[eric@localhost fedora]$ cat Dockerfile 
from fedora:21 
maintainer Eric J. van der Velden <ericjvandervelden@gmail.com>
run yum -y update

/ we moeten -y doen	, anders ...

/ we krijgen SELinux ERRs
/ TODO

/ we kunnen meerder keren achter elkaar	,
[eric@localhost fedora]$ sudo docker build -t eric/fedora:21 .
/ TODO

[eric@localhost sinatra]$ sudo docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
eric/fedora         21                  66a541a8a457        14 seconds ago      658.8 MB
/ Is dit de bedoeling?
/ TODO

/ we zagen een ERR over	...	, we moeten	,
delta rpms disabled because /usr/bin/applydeltarpm not installed
yum provides '*/applydeltarpm'

/ we doen	,
[eric@localhost fedora]$  sudo docker run -i -t fedora:21 /bin/bash
# yum -y update
...
/ OK
/ we hebben een container create	, nu image maken	,
[eric@localhost fedora]$ sudo docker commit -m "Updated fedora:21" -a "Eric J." 24bfa3ab61b4 eric/fedora:v2
50cbf024d8f4f37d416057f0461fbac960a3e3c22556898f68b5dd1b115304ea
[eric@localhost fedora]$ sudo docker images 
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
eric/fedora         v2                  50cbf024d8f4        16 seconds ago      658.8 MB
...
/ Weer zo groot	,

/ 13	. 

/ We doen geen yum -y update	, maar 

[eric@localhost fedora]$  sudo docker run -i -t fedora:21 /bin/bash
bash-4.3# yum install postgres-server
Installing:
 postgresql-server            x86_64            9.3.6-1.fc21              updates            3.8 M
Installing for dependencies:
 postgresql                   x86_64            9.3.6-1.fc21              updates            1.1 M
 postgresql-libs              x86_64            9.3.6-1.fc21              updates            236 k
...
  Installing : postgresql-libs-9.3.6-1.fc21.x86_64                                             1/3 
  Installing : postgresql-9.3.6-1.fc21.x86_64                                                  2/3 
  Installing : postgresql-server-9.3.6-1.fc21.x86_64                                           3/3 


/ Dus hetzelfde als bij 9.4	,
/ We lezen	,
https://fedoraproject.org/wiki/PostgreSQL
$ sudo yum install postgresql-server postgresql-contrib

/ Wat is postgresql-contrib	?
bash-4.3# repoquery -l postgresql-contrib
bash: repoquery: command not found
/ De deden het op eigen machine	, WH niet echt nodig	,

bash-4.3# which pg_ctl                          
/usr/bin/pg_ctl

/ systemctl in docker container	,
/ lees	,
https://bugzilla.redhat.com/show_bug.cgi?id=1033604
/ refs	,
https://rhatdan.wordpress.com/2014/04/30/running-systemd-within-a-docker-container/
/ en	,
https://maci0.wordpress.com/2014/07/23/run-systemd-in-an-unprivileged-docker-container/
/ en	,
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Security-Enhanced_Linux/sect-Security-Enhanced_Linux-SELinux_Contexts_Labeling_Files-Persistent_Changes_semanage_fcontext.html

/ we kunnen	,
bash-4.3# su - postgres -c "pg_ctl initdb"
The files belonging to this database system will be owned by user "postgres".
This user must also own the server process.

The database cluster will be initialized with locale "C".
The default database encoding has accordingly been set to "SQL_ASCII".
The default text search configuration will be set to "english".

Data page checksums are disabled.

fixing permissions on existing directory /var/lib/pgsql/data ... ok
creating subdirectories ... ok
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
creating configuration files ... ok
creating template1 database in /var/lib/pgsql/data/base/1 ... ok
initializing pg_authid ... ok
initializing dependencies ... ok
creating system views ... ok
loading system objects' descriptions ... ok
creating collations ... ok
No usable system locales were found.
Use the option "--debug" to see details.
creating conversions ... ok
creating dictionaries ... ok
setting privileges on built-in objects ... ok
creating information schema ... ok
loading PL/pgSQL server-side language ... ok
vacuuming database template1 ... ok
copying template1 to template0 ... ok
copying template1 to postgres ... ok
syncing data to disk ... ok

WARNING: enabling "trust" authentication for local connections
You can change this by editing pg_hba.conf or using the option -A, or
--auth-local and --auth-host, the next time you run initdb.

Success. You can now start the database server using:

    /usr/bin/postgres -D /var/lib/pgsql/data
or
    /usr/bin/pg_ctl -D /var/lib/pgsql/data -l logfile start

/ 7	. 

/ -D /var/lib/pgsql/data hoeft NIET	, want pakt default PGDATA en deze is WH /var/lib/pgsql/data
/ TODO

/ postgres is fg, pg_ctl background	,

/ Lees	,
http://www.postgresql.org/docs/9.4/static/app-postgres.html

/ Lees	,
http://superuser.com/questions/363444/how-do-i-get-the-output-and-exit-value-of-a-subshell-when-using-bash-e

/ Als we bovenstaande container 
# exit
/ doe dan	,
[eric@localhost fedora]$ sudo docker start -i db78f2181c63


bash-4.3#  su postgres -c "pg_ctl start"
pg_ctl: no database directory specified and environment variable PGDATA unset
Try "pg_ctl --help" for more information.
bash-4.3#  su - postgres -c "pg_ctl start"
server starting
bash-4.3# LOG:  redirecting log output to logging collector process
HINT:  Future log output will appear in directory "pg_log".
/ Geef Enter
bash-4.3# su - postgres
Last login: Mon Feb  9 15:38:01 EST 2015 on console
-bash-4.3$ echo $PGDATA
/var/lib/pgsql/data
-bash-4.3$ exit
logout
bash-4.3# su - postgres -c "echo $PGDATA"

bash-4.3# su - postgres -c "pg_ctl start"
server starting
...
bash-4.3# su - postgres -c psql
psql (9.3.6)
Type "help" for help.

postgres=#  exit
[eric@localhost fedora]$ sudo docker start -i db78f2181c63
bash-4.3# su - postgres -c "pg_ctl stop" 
waiting for server to shut down.... done
server stopped
bash-4.3# su - postgres -c postgres
LOG:  redirecting log output to logging collector process
HINT:  Future log output will appear in directory "pg_log".
/ In fg	,

/ 7	. 

/ doe in een window	,
[eric@localhost fedora]$ sudo docker start -i db78f2181c63
db78f2181c63
            bash-4.3# 
bash-4.3# 

/ doe in een ander window	,
[eric@localhost fedora]$ sudo docker attach db78f2181c63
/ geef enter	,
bash-4.3# 
/ Je zit in dezelfde bash	,

/7	.

/ doe 	, -s = statistics	,
[eric@localhost fedora]$ sudo su - postgres -c "/usr/pgsql-9.4/bin/postgres -s"

/ doe in ander window	,
[eric@localhost fedora]$ sudo su - postgres -c psql
psql (9.4.1)
Type "help" for help.

postgres=# create table foo(s text);
CREATE TABLE
/ OK, maar we zien geen statistics	,
/ TODO

/ 7	. 

[eric@localhost fedora]$ sudo docker run -i -t fedora:21 /bin/bash
bash-4.3# yum install deltarpm  
/ OK
/ Maar	,
[eric@localhost fedora]$ sudo docker commit -m "yum update with deltarpm" -a "Eric J." ec09572f93e4 test/fedora:21_update
test/fedora         21_update           8d6f71b2e893        About a minute ago   961 MB
/ Veel te groot	,
[eric@localhost fedora]$ sudo docker rmi 8d6f71b2e893

[eric@localhost fedora]$ sudo docker commit -m "with postgresql" -a "Eric J." db78f2181c63 test/postgresql
test/postgresql     latest              1d7a81471d66        13 seconds ago       468.4 MB

/ 7	.

eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/postgresql/fedora
[eric@localhost fedora]$ cat Dockerfile 
from fedora:21 
maintainer Eric J. van der Velden <ericjvandervelden@gmail.com>
#run yum -y update
#run yum clean all
run yum -y install postgresql-server
run yum clean all
run su - postgres -c "pg_ctl initdb"
run echo "listen_addresses='*'" >>/var/lib/pgsql/data/postgresql.conf
run echo "host all all 127.0.0.1/32 md5">>/var/lib/pgsql/data/pg_hba.conf
run echo "host all all 192.168.123.182/24 md5">>/var/lib/pgsql/data/pg_hba.conf

[eric@localhost fedora]$ sudo docker build -t test/postgresql:v1 .
Sending build context to Docker daemon 2.048 kB
Sending build context to Docker daemon 
Step 0 : FROM fedora:21
 ---> 834629358fe2
Step 1 : MAINTAINER Eric J. van der Velden <ericjvandervelden@gmail.com>
 ---> Running in 78b80f9b537f
 ---> 5742a804a83a
Removing intermediate container 78b80f9b537f
Step 2 : RUN yum -y install postgresql-server
 ---> Running in daac555f00da
Dependencies Resolved

================================================================================
 Package                 Arch         Version               Repository     Size
================================================================================
Installing:
 postgresql-server       x86_64       9.3.6-1.fc21          updates       3.8 M
Installing for dependencies:
 postgresql              x86_64       9.3.6-1.fc21          updates       1.1 M
 postgresql-libs         x86_64       9.3.6-1.fc21          updates       236 k

Transaction Summary
================================================================================
Install  1 Package (+2 Dependent packages)

Total download size: 5.1 M
Installed size: 21 M
...
Complete!
 ---> 318b5ce1b8d8
Removing intermediate container daac555f00da
Step 3 : RUN yum clean all
 ---> Running in 46852d156531
Cleaning repos: fedora updates
Cleaning up everything
 ---> c519c7d32d7e
Removing intermediate container 46852d156531
Step 4 : RUN su - postgres -c "pg_ctl initdb"
 ---> Running in d438eae49a5b
The files belonging to this database system will be owned by user "postgres".
This user must also own the server process.

The database cluster will be initialized with locale "C".
The default database encoding has accordingly been set to "SQL_ASCII".
The default text search configuration will be set to "english".

Data page checksums are disabled.

fixing permissions on existing directory /var/lib/pgsql/data ... ok
creating subdirectories ... ok
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
creating configuration files ... ok
creating template1 database in /var/lib/pgsql/data/base/1 ... ok
initializing pg_authid ... ok
initializing dependencies ... ok
creating system views ... ok
loading system objects' descriptions ... ok
creating collations ... ok
No usable system locales were found.
Use the option "--debug" to see details.
creating conversions ... ok
creating dictionaries ... ok
setting privileges on built-in objects ... ok
creating information schema ... ok
loading PL/pgSQL server-side language ... ok
vacuuming database template1 ... ok
copying template1 to template0 ... ok
copying template1 to postgres ... ok
syncing data to disk ... ok


WARNING: enabling "trust" authentication for local connections
Success. You can now start the database server using:
You can change this by editing pg_hba.conf or using the option -A, or

--auth-local and --auth-host, the next time you run initdb.
    /usr/bin/postgres -D /var/lib/pgsql/data
or
    /usr/bin/pg_ctl -D /var/lib/pgsql/data -l logfile start

 ---> 113d09e72dce
Removing intermediate container d438eae49a5b
Step 5 : RUN echo "listen_addresses='*'" >>/var/lib/pgsql/data/postgresql.conf
 ---> Running in 88b9dff03cc0
 ---> 3e364a7fe823
Removing intermediate container 88b9dff03cc0
Step 6 : RUN echo "host all all 127.0.0.1/32 md5">>/var/lib/pgsql/data/pg_hba.conf
 ---> Running in 15e1e374bfe1
 ---> ac61127d0bea
Removing intermediate container 15e1e374bfe1
Step 7 : RUN echo "host all all 192.168.123.182/24 md5">>/var/lib/pgsql/data/pg_hba.conf
 ---> Running in d449befe170b
 ---> 453e9101714a
Removing intermediate container d449befe170b
Successfully built 453e9101714a

/ Als we een regel add aan de Dockerfile	, en we doen nog een keer	,
[eric@localhost fedora]$ sudo docker build -t test/postgresql:v1 .
/ dan gaat hij verder met de nieuwe regel	,

[eric@localhost fedora]$ sudo docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
test/postgresql     v1                  453e9101714a        4 minutes ago       470.5 MB

/ 7	.

[eric@localhost fedora]$ sudo docker run -it --name postgresql test/postgresql:v1 /bin/bash
bash-4.3# su - postgres -c psql 
psql: could not connect to server: No such file or directory

/ we zijn vergeten de postgresql server te start	,

$ vi Dockerfile
...
run su - postgres -c "pg_ctl start"

/ Dit heeft WH geen zin	,

[eric@localhost fedora]$ sudo docker build -t test/postgresql:v1 .
Sending build context to Docker daemon 2.048 kB
Sending build context to Docker daemon 
Step 0 : FROM fedora:21
 ---> 834629358fe2
Step 1 : MAINTAINER Eric J. van der Velden <ericjvandervelden@gmail.com>
 ---> Using cache
 ---> 5742a804a83a
Step 2 : RUN yum -y install postgresql-server
 ---> Using cache
 ---> 318b5ce1b8d8
Step 3 : RUN yum clean all
 ---> Using cache
 ---> c519c7d32d7e
Step 4 : RUN su - postgres -c "pg_ctl initdb"
 ---> Using cache
 ---> 113d09e72dce
Step 5 : RUN echo "listen_addresses='*'" >>/var/lib/pgsql/data/postgresql.conf
 ---> Using cache
 ---> 3e364a7fe823
Step 6 : RUN echo "host all all 127.0.0.1/32 md5">>/var/lib/pgsql/data/pg_hba.conf
 ---> Using cache
 ---> ac61127d0bea
Step 7 : RUN echo "host all all 192.168.123.182/24 md5">>/var/lib/pgsql/data/pg_hba.conf
 ---> Using cache
 ---> 453e9101714a
Step 8 : RUN su - postgres -c "pg_ctl start"
 ---> Running in ccb97d9e2d77
server starting
 ---> 94bb5156ff8c
Removing intermediate container ccb97d9e2d77
Successfully built 94bb5156ff8c
[eric@localhost fedora]$ sudo docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
test/postgresql     v1                  94bb5156ff8c        38 seconds ago      470.5 MB

/ nieuw images	, de oude is rmi	,
/ TODO

/ 7	.

[eric@localhost fedora]$ sudo docker run --name=web -d -P training/webapp python app.py
8cf45768071f8f67c2d1fbcabe32ddb79eb9389ba01dbfa6cf60e1b3b554c4e4
[eric@localhost fedora]$ sudo docker port web
5000/tcp -> 0.0.0.0:49162
/ OK

[eric@localhost fedora]$  sudo docker stop web
web
[eric@localhost fedora]$ sudo docker start web
web
[eric@localhost fedora]$ sudo docker port web
5000/tcp -> 0.0.0.0:49163
/ OK

[eric@localhost fedora]$  sudo docker stop web
web
[eric@localhost fedora]$ sudo docker start -i  web
web
 * Running on http://0.0.0.0:5000/
/ Geen prompt	, dat is WH omdat container niet is create met /bin/bash	, maar met python	,
/ Dus heeft geen zin	,	
/ in ander window	,
[eric@localhost fedora]$  sudo docker port web
5000/tcp -> 0.0.0.0:49164


/ 7	.

[eric@localhost fedora]$  sudo docker stop web
web

[eric@localhost fedora]$  sudo docker run -d  --name mydb training/postgres
d09194224596a0fa74734440dcf68221e90a880a2005877a35f4b9fb2f4c5538

[eric@localhost fedora]$ ifconfig
...
vethba0836f: flags=67<UP,BROADCAST,RUNNING>  mtu 1500
        inet6 fe80::30e7:2cff:fe24:d348  prefixlen 64  scopeid 0x20<link>
        ether 32:e7:2c:24:d3:48  txqueuelen 0  (Ethernet)
        RX packets 6  bytes 508 (508.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 6  bytes 508 (508.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
/ TODO

[eric@localhost fedora]$  sudo docker run -d -P --name web --link mydb:aliasdb training/webapp python app.py 
c4ad13a771ef6a5620109b6b32a7145d8cd5ed850b2e8aebe5c89cab950f0c88
[eric@localhost fedora]$  sudo docker port web
5000/tcp -> 0.0.0.0:49166

$ ifconfig
...
vethba0836f: flags=67<UP,BROADCAST,RUNNING>  mtu 1500
        inet6 fe80::30e7:2cff:fe24:d348  prefixlen 64  scopeid 0x20<link>
        ether 32:e7:2c:24:d3:48  txqueuelen 0  (Ethernet)
        RX packets 8  bytes 648 (648.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 16  bytes 1296 (1.2 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

/ we kunnen	,
[eric@localhost fedora]$  sudo docker inspect -f "{{ .HostConfig.Links }}" web
[/mydb:/web/aliasdb]
[eric@localhost fedora]$  sudo docker inspect -f "{{ .Name }}" web
/web

/ maar ook,	
[eric@localhost fedora]$  sudo docker inspect -f "{{ .HostConfig }}" web
map[
	CapDrop:<nil>
	Devices:[]
	Dns:<nil>
	DnsSearch:<nil>
	ExtraHosts:<nil>
	Links:[/mydb:/web/aliasdb]
	ContainerIDFile:
	NetworkMode:bridge
	PublishAllPorts:true
	Binds:<nil>
	CapAdd:<nil>
	IpcMode:
	PortBindings:map[]
	RestartPolicy:map[MaximumRetryCount:0
		Name:]
	SecurityOpt:<nil>
	LxcConf:[]
Privileged:false
VolumesFrom:<nil>]

/ of	,
[eric@localhost fedora]$  sudo docker inspect -f "{{ . }}" web
map[
Created:2015-02-11T20:52:23.160480674Z

HostConfig:map[Devices:[]
ExtraHosts:<nil>
IpcMode:
PortBindings:map[]
Dns:<nil>
Links:[/mydb:/web/aliasdb]
LxcConf:[]
NetworkMode:bridge
PublishAllPorts:true
VolumesFrom:<nil>
Binds:<nil>
CapDrop:<nil>
ContainerIDFile:
DnsSearch:<nil>
RestartPolicy:map[MaximumRetryCount:0
Name:]
CapAdd:<nil>
Privileged:false
SecurityOpt:<nil>]

HostnamePath:/var/lib/docker/containers/8166984002a114930d7d4b59e892dc049b6dbe28809320a6feae8b85d1e51e05/hostname
HostsPath:/var/lib/docker/containers/8166984002a114930d7d4b59e892dc049b6dbe28809320a6feae8b85d1e51e05/hosts
Path:python
Volumes:map[]
ExecDriver:native-0.2
Name:/web
NetworkSettings:map[Ports:map[5000/tcp:[map[HostIp:0.0.0.0
HostPort:49167]]]
Bridge:docker0
Gateway:172.17.42.1
IPAddress:172.17.0.95
IPPrefixLen:16
MacAddress:02:42:ac:11:00:5f
PortMapping:<nil>]
State:map[Pid:23881
Running:true
ExitCode:0
OOMKilled:false
Paused:false
StartedAt:2015-02-11T20:52:23.61873926Z
Error:
FinishedAt:0001-01-01T00:00:00Z
Restarting:false]
VolumesRW:map[]
AppArmorProfile:
Args:[app.py]
Driver:devicemapper
Image:31fa814ba25ae3426f8710df7a48d567d4022527ef2c14964bb8bc45e653417c
MountLabel:system_u:object_r:svirt_sandbox_file_t:s0:c346,c667
ResolvConfPath:/var/lib/docker/containers/8166984002a114930d7d4b59e892dc049b6dbe28809320a6feae8b85d1e51e05/resolv.conf
Config:map[CpuShares:0
Hostname:8166984002a1
OnBuild:<nil>
WorkingDir:/opt/webapp
AttachStdout:false
Cpuset:
Entrypoint:<nil>
Env:[HOME=/
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin]
Cmd:[python
app.py]
Image:training/webapp
PortSpecs:<nil>
Tty:false
User:
MemorySwap:0
NetworkDisabled:false
AttachStderr:false
AttachStdin:false
Domainname:
ExposedPorts:map[5000/tcp:map[]]
MacAddress:
Memory:0
OpenStdin:false
StdinOnce:false
Volumes:<nil>]
Id:8166984002a114930d7d4b59e892dc049b6dbe28809320a6feae8b85d1e51e05
ProcessLabel:system_u:system_r:svirt_lxc_net_t:s0:c346,c667]

/ we deden dus	,
[eric@localhost fedora]$  sudo docker run -d -P --name web --link mydb:aliasdb training/webapp python app.py 

[eric@localhost fedora]$  sudo docker run --rm --name web --link mydb:aliasdb training/webapp env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=0bf4928db462
ALIASDB_PORT=tcp://172.17.0.93:5432
ALIASDB_PORT_5432_TCP=tcp://172.17.0.93:5432
ALIASDB_PORT_5432_TCP_ADDR=172.17.0.93
ALIASDB_PORT_5432_TCP_PORT=5432
ALIASDB_PORT_5432_TCP_PROTO=tcp
ALIASDB_NAME=/web/aliasdb
ALIASDB_ENV_PG_VERSION=9.3
HOME=/

/ of doe,
[eric@localhost hadoop]$ sudo docker run -P --name web --link db:mydb --rm -it training/webapp /bin/bash
root@7ae814b519bd:/opt/webapp# env

MYDB_PORT_5432_TCP=tcp://172.17.0.4:5432
HOSTNAME=7ae814b519bd
TERM=xterm
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
MYDB_PORT_5432_TCP_PROTO=tcp
PWD=/opt/webapp
MYDB_NAME=/web/mydb
MYDB_PORT=tcp://172.17.0.4:5432
SHLVL=1
HOME=/
MYDB_PORT_5432_TCP_ADDR=172.17.0.4
MYDB_ENV_PG_VERSION=9.3
MYDB_PORT_5432_TCP_PORT=5432
_=/usr/bin/env

root@7ae814b519bd:/opt/webapp# apt-get install net-tools
root@7ae814b519bd:/opt/webapp# apt-get install nmap
root@7ae814b519bd:/opt/webapp# ifconfig 
eth0      Link encap:Ethernet  HWaddr 02:42:ac:11:00:0a  
          inet addr:172.17.0.10  Bcast:0.0.0.0  Mask:255.255.0.0
...
root@7ae814b519bd:/opt/webapp# nmap 172.17.0.10/24

Nmap scan report for mydb (172.17.0.4)
Host is up (0.000022s latency).
PORT     STATE SERVICE
5432/tcp open  postgresql

Nmap scan report for 7ae814b519bd (172.17.0.10)
Host is up (0.000018s latency).

/ op de laptop	,
[eric@localhost fedora_tmp]$ ifconfig
docker0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 172.17.42.1  netmask 255.255.0.0  broadcast 0.0.0.0
...



/ we hadden	, 
[eric@localhost fedora]$  sudo docker run -d  --name mydb training/postgres

[eric@localhost fedora]$  sudo docker run  --rm --name web  --link mydb:aliasdb -it training/webapp  /bin/bash
root@6dfca9d81265:/opt/webapp# cat /etc/hosts
172.17.0.101	6dfca9d81265
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters
172.17.0.93	aliasdb

/in ander window	,
[eric@localhost sinatra]$ sudo docker ps -a
CONTAINER ID        IMAGE                      COMMAND                CREATED              STATUS                    PORTS               NAMES
6dfca9d81265        training/webapp:latest     "/bin/bash"            About a minute ago   Up About a minute         5000/tcp            web                 
d09194224596        training/postgres:latest   "su postgres -c '/us   23 hours ago         Up 23 hours               5432/tcp            mydb                

/ Dus 6dfca9d81265 is web zelf	,

/ Lees	,
https://help.ubuntu.com/community/AptGet/Howto
/ over $ apt-cache search	,

root@6dfca9d81265:/opt/webapp# apt-get install inetutils-ping
/ OK
root@6dfca9d81265:/opt/webapp#  ping aliasdb
PING aliasdb (172.17.0.93): 48 data bytes
56 bytes from 172.17.0.93: icmp_seq=0 ttl=64 time=0.249 ms
...
/ OK

/ We doen in een ander window	, 
[eric@localhost sinatra]$ sudo su - postgres -c "psql -h 172.17.0.93"
psql (9.4.1, server 9.3.4)
SSL connection (protocol: TLSv1.2, cipher: AES128-GCM-SHA256, bits: 128, compression: off)
Type "help" for help.

postgres=# \l
                             List of databases
   Name    |  Owner   | Encoding  | Collate | Ctype |   Access privileges   
-----------+----------+-----------+---------+-------+-----------------------
 docker    | docker   | SQL_ASCII | C       | C     | =Tc/docker           +
           |          |           |         |       | docker=CTc/docker
 postgres  | postgres | SQL_ASCII | C       | C     | 
 template0 | postgres | SQL_ASCII | C       | C     | =c/postgres          +
           |          |           |         |       | postgres=CTc/postgres
 template1 | postgres | SQL_ASCII | C       | C     | =c/postgres          +

/ we keren weer terug naar de web container	,
 
/ doe in ander window	,
[eric@localhost fedora]$ sudo docker restart mydb
mydb

/ als we in web # cat /etc/hosts ,dan zien we dezelfde 	,
root@6dfca9d81265:/opt/webapp#  ping aliasdb
/ ERR 
/ geen contact meer	,

/ maar als we opnieuw een container start	, 
web# exit
[eric@localhost fedora]$  sudo docker run  --rm --name web  --link mydb:aliasdb -it training/webapp  /bin/bash
root@97b2ec16ffd9:/opt/webapp# cat /etc/hosts
172.17.0.106	97b2ec16ffd9
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters
172.17.0.105	aliasdb

/ dan zien we dus de nieuwe ip addresses	,

root@97b2ec16ffd9:/opt/webapp# apt-get install inetutils-ping
root@97b2ec16ffd9:/opt/webapp#  ping aliasdb
PING aliasdb (172.17.0.105): 48 data bytes
56 bytes from 172.17.0.105: icmp_seq=0 ttl=64 time=0.150 ms
...
/ OK

/ Dus /etc/hosts is NIET automatically updated	,
/ TODO

/ 7	.

/ psql in in andere container naar andere	, 

[eric@localhost ubuntu]$ sudo docker run --rm --name postgres --link mydb:aliasdb -it training/postgres /bin/bash

root@ef4fd7ba7350:/# cat /etc/hosts
172.17.0.108	ef4fd7ba7350
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters
172.17.0.107	aliasdb

root@ef4fd7ba7350:/# su - postgres -c "psql -h aliasdb"
psql (9.3.4)
SSL connection (cipher: DHE-RSA-AES256-SHA, bits: 256)
Type "help" for help.

postgres=# \l
                             List of databases
   Name    |  Owner   | Encoding  | Collate | Ctype |   Access privileges   
-----------+----------+-----------+---------+-------+-----------------------
 docker    | docker   | SQL_ASCII | C       | C     | =Tc/docker           +
           |          |           |         |       | docker=CTc/docker
 postgres  | postgres | SQL_ASCII | C       | C     | 
 template0 | postgres | SQL_ASCII | C       | C     | =c/postgres          +
           |          |           |         |       | postgres=CTc/postgres
 template1 | postgres | SQL_ASCII | C       | C     | =c/postgres          +
           |          |           |         |       | postgres=CTc/postgres
(4 rows)

postgres=# \c docker
docker=# create table first(i int);
CREATE TABLE
docker=# insert into first values(7),(13);
INSERT 0 2
docker=# \q
could not save history to file "/var/lib/postgresql/.psql_history": No such file or directory
/ TODO

root@5ee6fef92c2b:/# exit
exit
[eric@localhost ubuntu]$ sudo docker run --rm --name postgres --link mydb:aliasdb -it training/postgres /bin/bash

postgres=# \c docker
docker=# select*from first;
 i  
----
  7
 13
(2 rows)
/ OK	, dat is er nog	,

/ Nu container met server herstart	,

[eric@localhost ubuntu]$ sudo docker restart mydb
mydb

/ start client container	,
[eric@localhost ubuntu]$ sudo docker run --rm --name postgres --link mydb:aliasdb -it training/postgres /bin/bash
postgres=# \c docker
docker=# select*from first;
 i  
----
  7
 13
(2 rows)

/ als we de server container rm, is de db rm	,
/ we rm de container,	
[eric@localhost ubuntu]$ sudo docker stop mydb
mydb
[eric@localhost ubuntu]$ sudo docker rm mydb
mydb

/ start opnieuw	,
[eric@localhost ubuntu]$ sudo docker run -d --name mydb   training/postgres

[eric@localhost ubuntu]$ sudo docker run --rm --link mydb:aliasdb -it training/postgres /bin/bash
root@a4c082386fe7:/# su - postgres -c "psql -h aliasdb"
postgres=# \c docker
docker=# select*from first;
ERROR:  relation "first" does not exist


/ 7	.

/ we maken een nieuwe image van de server container	, 
/ als we van dit image een container start, zal hij de first tbl wel hebben	,

/ we stop de server container	,
[eric@localhost fedora]$ sudo docker stop mydb
mydb
[eric@localhost fedora]$ sudo docker commit -m "table first in docker db" -a "Eric J." mydb training/postgres:v1
/ OK
[eric@localhost fedora]$ sudo docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
training/postgres   v1                  11cd9e0cdf84        30 seconds ago      383.3 MB
training/postgres   latest              258105bea10d        8 months ago        364.5 MB

/ latest zouden we moeten andere version	,
/ TODO

/ dan rm server container, en start nieuwe server container van nieuwe image	,
[eric@localhost fedora]$ sudo docker rm mydb
mydb
[eric@localhost fedora]$ sudo docker run -d --name mydb training/postgres:v1

/ start client	,
[eric@localhost fedora]$ sudo docker run --rm --link mydb:aliasdb -it training/postgres:v1 /bin/bash
root@313d80b0a800:/# su - postgres -c "psql -h aliasdb"
postgres=# \c docker 
docker=# select*from first;
 i  
----
  7
 13
(2 rows)

/ Nu zien we dat als we een container create van het nieuwe image de first tbl er is	,

/ 7	. 

/ we deden	,
[eric@localhost fedora]$ sudo docker run --rm --link mydb:aliasdb -it training/postgres:v1 /bin/bash

/ Omdat we /bin/bash deden, werd de postgres server niet start	,
/ als we doen	,
[eric@localhost fedora]$ sudo docker run --rm --link mydb:aliasdb -it training/postgres:v1
2015-02-14 08:47:28 UTC LOG:  database system was shut down at 2015-02-14 08:17:52 UTC
2015-02-14 08:47:28 UTC LOG:  autovacuum launcher started
2015-02-14 08:47:28 UTC LOG:  database system is ready to accept connections

/ Als we de Dockerfile lezen	,
https://registry.hub.docker.com/u/training/postgres/dockerfile/

CMD ["su", "postgres", "-c", "/usr/lib/postgresql/$PG_VERSION/bin/postgres -D /var/lib/postgresql/$PG_VERSION/main/ -c config_file=/etc/postgresql/$PG_VERSION/main/postgresql.conf"]

/ WH als we geen command zoals /bin/bash geven, wordt deze gedaan	, en hier staat dat hij de server start	,

/ we kunnen deze container stop , door 
ctrl-c

Session terminated, terminating shell...2015-02-14 08:50:02 UTC LOG:  received fast shutdown request
2015-02-14 08:50:02 UTC LOG:  aborting any active transactions
2015-02-14 08:50:02 UTC LOG:  autovacuum launcher shutting down
2015-02-14 08:50:02 UTC LOG:  shutting down
2015-02-14 08:50:02 UTC LOG:  database system is shut down
 ...terminated.

/ 7	.

/ kunnen we /bin/bash doen op de server container?

/ Dit is niet slim	,

[eric@localhost fedora]$ sudo docker attach mydb
^C
Session terminated, terminating shell...2015-02-14 08:56:54 UTC LOG:  received fast shutdown request
2015-02-14 08:56:54 UTC LOG:  aborting any active transactions
2015-02-14 08:56:54 UTC LOG:  autovacuum launcher shutting down
2015-02-14 08:56:54 UTC LOG:  shutting down
2015-02-14 08:56:54 UTC LOG:  database system is shut down
 ...terminated.

[eric@localhost fedora]$ sudo docker ps -a
CONTAINER ID        IMAGE                  COMMAND                CREATED             STATUS                      PORTS               NAMES
ae9d7ac83617        training/postgres:v1   "su postgres -c '/us   30 minutes ago      Exited (0) 46 seconds ago                       mydb       

/ we doen	,

[eric@localhost fedora]$ sudo docker start mydb
mydb
[eric@localhost fedora]$ sudo docker ps -a
CONTAINER ID        IMAGE                  COMMAND                CREATED             STATUS                  PORTS               NAMES
ae9d7ac83617        training/postgres:v1   "su postgres -c '/us   32 minutes ago      Up 3 seconds            5432/tcp            mydb                

/ OK, 
/ de postgres server runs	, en we kunnen in client container link naar deze	,


/ 7	.

/ In de Dockerfile van training/postgres doen ze 	,

RUN service postgresql start && \
 su postgres sh -c "createuser -d -r -s docker" && \
 su postgres sh -c "createdb -O docker docker" && \
 su postgres sh -c "psql -c \"GRANT ALL PRIVILEGES ON DATABASE docker to docker;\""

/ ze start postgresql server om de de 3 cmds daaronder te kunnen doen	,

/ 7	.

/ we start een 2de server container	,

[eric@localhost fedora]$ sudo docker run -d --name mydb2 training/postgres:v1

[eric@localhost fedora]$ sudo docker ps -a
CONTAINER ID        IMAGE                  COMMAND                CREATED             STATUS                  PORTS               NAMES
470b30e0ff77        training/postgres:v1   "su postgres -c '/us   41 seconds ago      Up 39 seconds           5432/tcp            mydb2               
ae9d7ac83617        training/postgres:v1   "su postgres -c '/us   41 minutes ago      Up 9 minutes            5432/tcp            mydb                

/ link naar beide doe je zo	,

[eric@localhost fedora]$ sudo docker run --rm --link mydb:aliasdb --link mydb2:aliasdb2 -it training/postgres:v1 /bin/bash
root@4eee5f53d0de:/# cat /etc/hosts
172.17.0.126	4eee5f53d0de
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters
172.17.0.123	aliasdb
172.17.0.125	aliasdb2

root@4eee5f53d0de:/# su - postgres -c "psql -h aliasdb"
postgres=# \c docker
docker=# select*from first
docker-# ;
 i  
----
  7
 13
(2 rows)

docker=# \q

root@4eee5f53d0de:/# su - postgres -c "psql -h aliasdb2"
postgres=# \c docker
docker=# select*from first;
 i  
----
  7
 13
(2 rows)

docker=# \q

/ 7	.

[eric@localhost fedora]$ sudo docker rm $(sudo docker stop mydb2)
mydb2
[eric@localhost fedora]$ sudo docker run -d  --name mydb2 --link mydb:aliasdb -it training/postgres:v1 

[eric@localhost fedora]$ sudo docker run --link mydb:aliasdb --name mydb2 -it training/postgres:v1 /bin/bash
root@fdf3502e398e:/# service  postgresql start       
 * Starting PostgreSQL 9.3 database server                                                              [ OK ] 
root@fdf3502e398e:/# ps ajx | grep postgres
    1    37    33    33 ?           -1 S      102   0:00 /usr/lib/postgresql/9.3/bin/postgres -D /var/lib/postgresql/9.3/main -c config_file=/etc/postgresql/9.3/main/postgresql.conf
   37    39    39    39 ?           -1 Ss     102   0:00 postgres: checkpointer process                                                                                              
   37    40    40    40 ?           -1 Ss     102   0:00 postgres: writer process                                                                                                    
   37    41    41    41 ?           -1 Ss     102   0:00 postgres: wal writer process                                                                                                
   37    42    42    42 ?           -1 Ss     102   0:00 postgres: autovacuum launcher process                                                                                       
   37    43    43    43 ?           -1 Ss     102   0:00 postgres: stats collector process                                                                                           
    1    57    56     1 ?           56 S+       0   0:00 grep postgres

root@fdf3502e398e:/# ls /var/lib/postgresql/9.3/main
PG_VERSION  global   pg_multixact  pg_serial	 pg_stat      pg_subtrans  pg_twophase	postmaster.opts
base	    pg_clog  pg_notify	   pg_snapshots  pg_stat_tmp  pg_tblspc    pg_xlog	postmaster.pid

root@fdf3502e398e:/# ls /usr/bin/pg_
pg_basebackup      pg_ctlcluster      pg_dumpall         pg_receivexlog     pg_virtualenv
pg_config          pg_dropcluster     pg_isready         pg_restore         
pg_createcluster   pg_dump            pg_lsclusters      pg_upgradecluster  


/ training/postgres heeft geen pg_ctl
/ maar wel pg_...cluster	,
/ TODO

/ 7	.

/ als we /bin/bash zonder -it	, dan is cmd dus /bin/bash	,  en zonder -it stops meteen	,

[eric@localhost fedora]$ sudo docker run   --link mydb:aliasdb --name mydb2  training/postgres:v1 /bin/bash[eric@localhost fedora]$ sudo docker ps -a
CONTAINER ID        IMAGE                  COMMAND                CREATED             STATUS                     PORTS               NAMES
87e1f72463c1        training/postgres:v1   "/bin/bash"            9 seconds ago       Exited (0) 9 seconds ago                       mydb2               
[eric@localhost fedora]$ sudo docker rm $(sudo docker stop mydb2)
mydb2

/ als we geen cmd geven is het dus die uit de Dockerfile, die postgres start	,
/ maar zonder -d is in fg	,

[eric@localhost fedora]$ sudo docker run   --link mydb:aliasdb --name mydb2  training/postgres:v1
2015-02-14 10:10:59 UTC LOG:  database system was shut down at 2015-02-14 08:17:52 UTC
2015-02-14 10:10:59 UTC LOG:  autovacuum launcher started
2015-02-14 10:10:59 UTC LOG:  database system is ready to accept connections

/ we kunnen 	,
^C
Session terminated, terminating shell...2015-02-14 10:11:05 UTC LOG:  received fast shutdown request
2015-02-14 10:11:05 UTC LOG:  aborting any active transactions
2015-02-14 10:11:05 UTC LOG:  autovacuum launcher shutting down
2015-02-14 10:11:05 UTC LOG:  shutting down
2015-02-14 10:11:05 UTC LOG:  database system is shut down
 ...terminated.

/ 7	.

/ In Docker file	,
run ...
cmd ...
/ Als we build, zien we dat cmd execs in een container, maar deze container wordt meteen weer rm	,

/als we docker run, dan wordt cmd exec, als we geen andere geven	,


/ 7	. 

/op laptop pg 9.4,	
[eric@localhost fedora]$ ls /usr/pgsql-9.4/bin
clusterdb   dropdb    pg_basebackup   pg_dump         pg_resetxlog   postgresql94-check-db-dir  reindexdb
createdb    droplang  pg_config       pg_dumpall      pg_restore     postgresql94-setup         vacuumdb
createlang  dropuser  pg_controldata  pg_isready      pg_test_fsync  postmaster
createuser  initdb    pg_ctl          pg_receivexlog  postgres       psql

[eric@localhost fedora]$ ls /usr/bin/pg_
pg_basebackup  pg_dump        pg_dumpall     pg_restore     

/ 7	.

$ vi Dockerfile
...
[eric@localhost fedora]$ sudo docker build -t test/postgres .

/ start server	,
[eric@localhost fedora]$ sudo docker run --name mydb2 -d test/postgres
/ exits
/ TODO

/ we oef	,

[eric@localhost fedora]$ sudo docker run -it --rm test/postgres /bin/bash
bash-4.3# su - postgres -c postgres&
/ of	,
bash-4.3# su - postgres -c "pg_ctl start"
/ OK

bash-4.3# LOG:  redirecting log output to logging collector process
HINT:  Future log output will appear in directory "pg_log".
/ TODO

/ postgres is /usr/bin/postgres	,

bash-4.3# su - postgres -c psql
postgres=# \q
/ OK

/ we oef	,

/ postgres is trusted over network	,

/ we oef hier door te connect naar zichzelf, maar niet over 172.0.0.1	, maar 172.17.0.162	, wat hij zelf is	,

/ server & client container	,
[eric@localhost fedora]$ sudo docker run -it --rm test/postgres /bin/bash
bash-4.3# su - postgres -c postgres&
bash-4.3# yum install net-tools
bash-4.3# ifconfig
eth0: flags=67<UP,BROADCAST,RUNNING>  mtu 1500
        inet 172.17.0.162  netmask 255.255.0.0  broadcast 0.0.0.0
...
bash-4.3# su - postgres -c "psql -h 172.17.0.162" 
Password: 
/ Dit willen we niet	,

bash-4.3# vi /var/lib/pgsql/data/pg_hba.conf 
...
host all postgres 172.17.0.0/16 trust
host all all 172.17.0.0/16 md5

bash-4.3# su - postgres -c "psql -h 172.17.0.162" 
psql (9.3.6)
Type "help" for help.

postgres=# \q

/ Nu echt	,

$ vi Dockerfile

from fedora:21
maintainer Eric J. van der Velden <ericjvandervelden@gmail.com>
#run yum -y update
run yum -y install postgresql-server
run yum -y install net-tools
run yum clean all
run su - postgres -c "pg_ctl initdb"
run echo "listen_addresses='*'" >>/var/lib/pgsql/data/postgresql.conf
run echo "host all postgres 172.17.0.0/16 trust">>/var/lib/pgsql/data/pg_hba.conf
run echo "host all all 172.17.0.0/16 md5">>/var/lib/pgsql/data/pg_hba.conf

cmd ["su","-","postgres","-c","postgres"]
/ of	,
cmd su - postgres -c postgres

[eric@localhost fedora]$ sudo docker build -t test/postgres .
/ Dit kunnen we steeds doen	, 

/ start server	,
[eric@localhost fedora]$ sudo docker run --name mydb2 -d test/postgres
/ we kunnen -d of niet	, 

/ start client	,
[eric@localhost fedora]$ sudo docker run -it --rm --link mydb:myalias --link mydb2:myalias2 test/postgres /bin/bash

/ we overwrite de cmd met /bin/bash	, we use test/postgres omdat we de psql client willen	, 

bash-4.3# ifconfig
eth0: flags=67<UP,BROADCAST,RUNNING>  mtu 1500
        inet 172.17.0.179  netmask 255.255.0.0  broadcast 0.0.0.0
...
bash-4.3# cat /etc/hosts
172.17.0.179	ad2603bf1931					/ container self	,
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters
172.17.0.123	myalias							/ server container	,
172.17.0.178	myalias2						/ server container	,

bash-4.3# su - postgres -c "psql -h  myalias"
postgres=# \q
bash-4.3# su - postgres -c "psql -h  myalias2"
postgres=# \q
/ OK

/ doe NIET & in	, want dan exits de container meteen	,
cmd su - postgres -c postgres&
/ of	,
cmd ["su","-","postgres","-c","postgres&"]

/ doe ook NIET	,
cmd su - postgres -c "pg_ctl start"
/ want dan exits de container ook meteen	, 

/ & zo doet niets	, 
cmd ["su","-","postgres","-c","postgres","&"]
/ TODO


/ 7	.


/ 13	. 


[eric@localhost fedora_2]$ pwd
/home/eric/Devel/Docker/sshd/fedora_2
[eric@localhost fedora_2]$ cat Dockerfile 

from fedora 
run yum -y install openssh-server openssh-clients
run yum clean all

run awk -i inplace '{print gensub("(#)(PermitRootLogin yes)","\\2","g")}' /etc/ssh/sshd_config

run ssh-keygen -A
run ssh-keygen -N "" -t rsa -f ~/.ssh/id_rsa
run cat /root/.ssh/id_rsa.pub >>~/.ssh/authorized_keys

run if [ ! -e ~/.ssh ];then mkdir ~/.ssh;fi
copy id_dsa.pub /tmp/
run cat  /tmp/id_dsa.pub  >> ~/.ssh/authorized_keys

expose 22

cmd /usr/sbin/sshd -D

[eric@localhost fedora_2]$ sudo docker build --rm -t local/fd-sshd .

[eric@localhost Docker]$ sudo docker run --rm --name sshd -P local/fd-sshd

[eric@localhost Docker]$ sudo docker exec -it sshd /bin/bash
bash-4.3# yum -y install postgresql-server    
bash-4.3# su - postgres -c "pg_ctl initdb"




/ 7	.

/ Data volumes

[eric@localhost fedora]$ sudo docker run  --name mydb2  -it --rm  -v /db test/postgres /bin/bash
bash-4.3# ls
bin   db   etc	 lib	lost+found  mnt  proc  run   srv  tmp  var
boot  dev  home  lib64	media	    opt  root  sbin  sys  usr
/ Inderdaad	, db/	,

/ we kunnen ook in 
[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/postgresql/fedora
$ vi Dockerfile
volume /mydir
[eric@localhost fedora]$ sudo docker build -t test/postgres .
[eric@localhost fedora]$ sudo docker run  --name mydb  -it --rm  -v /db test/postgres /bin/bash
bash-4.3# ls
bin   db   etc	 lib	lost+found  mnt    opt	 root  sbin  sys  usr
boot  dev  home  lib64	media	    mydir  proc  run   srv   tmp  var
/ we zien dat volume in de Dockerfile hetzelfde effect heeft als -v op de command line	,

/ we create dir	,
/home/eric/Devel/Docker/postgresql/fedora/localdb

[eric@localhost localdb]$ ls /home/ -l
total 20
drwx------. 17 eric wheel  4096 Feb 14 20:39 eric

[eric@localhost fedora]$ sudo docker run  --name mydb2  -it --rm  -v /home/eric/Devel/Docker/postgresql/fedora/localdb:/db test/postgres /bin/bash
bash-4.3# ls db
ls: cannot open directory db: Permission denied

/ Lees	,
http://stackoverflow.com/questions/24288616/permission-denied-on-accessing-host-directory-in-docker

/ we zien in SELinux Alert Browser	,

SELinux is preventing docker from create access on the directory db.

/ 13	. 

/ Wat hier staat helpt NIET	,

Do allow this access for now by executing:
# grep docker /var/log/audit/audit.log | audit2allow -M mypol
# semodule -i mypol.pp

[eric@localhost fedora]$ sudo grep docker /var/log/audit/audit.log | audit2allow -M mypol
[eric@localhost fedora]$ ls -l
-rw-r--r--. 1 eric wheel 2228 Feb 16 22:01 mypol.pp
-rw-r--r--. 1 eric wheel 1055 Feb 16 22:01 mypol.te
...
allow docker_t etc_t:file { write create };
...

[eric@localhost fedora]$ sudo semodule -i mypol.pp
/ OK
/ Maar het helpt NIET	,
/ TODO

/ 13	.

/ Deze helpt WEL	,

[eric@localhost fedora]$ sudo chcon -Rt  svirt_sandbox_file_t /localdb

[eric@localhost fedora]$ sudo docker run  --name mydb2  -it --rm  -v /localdb:/db  test/postgres /bin/bash
bash-4.3# ls db
first.txt

/ Of als de dir in ~eric is	,
/ we hoeven geen abs path te geven aan chcon	, rel is OK	,

[eric@localhost fedora]$ sudo chcon -Rt  svirt_sandbox_file_t localdb/

[eric@localhost fedora]$ sudo docker run  --name mydb2  -it --rm  -v $(pwd)/localdb:/db test/postgres /bin/bash
bash-4.3# ls db
first.txt
bash-4.3# cat db/first.txt 
first
bash-4.3# vi second.txt
second
:w
bash-4.3# exit
[eric@localhost fedora]$ cat localdb/second.txt 
second

/ Inderdaad	, in de container hebben we write in het fs van de host	, de laptop	,

/ 7	.

/ We zijn in User Guide, Managing containers	,

/ Lees	Reference, Command line	,
https://docs.docker.com/reference/commandline/cli/#create

/ --volumes-from is net als -v	, alleen het komt van een andere container	,

/ docker create	,
This is similar to docker run -d except the container is never started.

[eric@localhost fedora]$  sudo docker create  -v /dbdata --name dbdataname training/postgres
c6e226fb784911e1c96cc24fe59b2ebc8eee3fe5e4e626ed16556bd8c7a89cdb
[eric@localhost fedora]$  sudo docker ps -a
CONTAINER ID        IMAGE                      COMMAND                CREATED             STATUS              PORTS               NAMES
c6e226fb7849        training/postgres:latest   "su postgres -c '/us   12 seconds ago                                              dbdataname          

[eric@localhost fedora]$  sudo docker run --volumes-from dbdataname --name db1 -d training/postgres
2420dd4fd578963e73a32851eb8983b3c41709baa3948e43e236ea4920ef2b3b
[eric@localhost fedora]$  sudo docker run --volumes-from dbdataname --name db2 -d training/postgres
ac7406447940d2b3ea0acf6fbff9e397ce86ea7c44785ea2b4f1b009a104620e

[eric@localhost fedora]$  sudo docker run --volumes-from dbdataname --name db3 -it --rm  training/postgres /bin/bash
root@48193d282cf6:/# ls
bin   dbdata  etc   lib    media  opt	root  sbin  sys  usr
boot  dev     home  lib64  mnt	  proc	run   srv   tmp  var

/ 7	.

/ Lees	Examples, Dockerizing PostgreSQL

$ sudo docker run --rm -P --name pg_test eg_postgresql
/ waarom -P
/ TODO

$ sudo docker run --rm -t -i --link pg_test:pg eg_postgresql bash
postgres@7ef98b1b7243:/$ psql -h $PG_PORT_5432_TCP_ADDR -p $PG_PORT_5432_TCP_PORT -d docker -U docker --password
/ $PG_PORT_5432_TCP_ADDR is ""	,
/ TODO
/ See User Guide, Linking containers together	,


/ we doen altijd	,

$ sudo docker run --rm --name mydb test/postgres
/ Inderdaad	, in fg	,
$  sudo docker run --rm -it --link mydb:myalias test/postgres bash
bash-4.3# su - postgres -c "psql -h myalias"
/ OK
/ Je kunt in /etc/hosts kijken	,

/ Deze helpt	,

/ 13	. 

/ Deze helpt OOK	,

[eric@localhost fedora]$ sudo docker run  --name mydb2  -it --rm  -v /localdb:/db --privileged=true test/postgres /bin/bash
bash-4.3# cat db/first.txt 
foo
/ OK

/ Lees einde Examples, Dockerizing PostgreSQL,	 met volumes	
/ TODO

/	7	.

/ Lees	over postgresql, volumes	,
http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker

/ 7	 . 

/ we geven als volume een bestaande dir, bijv /var/lib/pgsql met de config files	, een andere container kan deze dan lezen	,

[eric@localhost fedora]$ sudo docker run --rm -v /var/lib/pgsql --name foobar -it test/postgres bash
bash-4.3# 

[eric@localhost fedora]$ sudo docker run --rm -it --volumes-from  foobar fedora bash
bash-4.3# ls /var/lib/
alternatives  dbus  ebtables  games  initramfs	misc  os-prober  pgsql	rpm  rpm-state	systemd  yum
//////////////////////////////////////////
/ we zien pgsql/ erbij	,
/ zonder --volumes-from foobar 
bash-4.3# ls /var/lib/pgsql/
backups  data


/ 7	.

/ Lees	,
http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker
/ refs	,
http://www.offermann.us/2013/12/tiny-docker-pieces-loosely-joined.html
/ refs	,
http://crosbymichael.com/advanced-docker-volumes.html

/ 7	.

/ geef in google	,
red hat docker kubernetes

/ 7	. 

http://www.centurylinklabs.com/interviews/how-does-redhats-openshift-work-with-docker/

/ 7	.

/ video's docker kubernetes	,
https://www.youtube.com/watch?v=OEflMTJUV4c#t=255
https://www.youtube.com/watch?v=omTGY8LjVdA




/ Einde DOCKER

/ DOCKER SERVICE 

/ 7	.

/ Lees	,
https://registry.hub.docker.com/u/training/postgres/dockerfile/

[eric@localhost ubuntu]$  pwd
/home/eric/Devel/Docker/postgresql/ubuntu
$ vi Dockerfile

FROM ubuntu:14.04
MAINTAINER Docker Education Team <education@docker.com>

ENV PG_VERSION 9.3
RUN apt-get update
RUN apt-get -y install postgresql postgresql-client postgresql-contrib

RUN echo "host    all             all             0.0.0.0/0 trust" >> /etc/postgresql/$PG_VERSION/main/pg_hba.conf
RUN echo "listen_addresses='*'" >> /etc/postgresql/$PG_VERSION/main/postgresql.conf

RUN service postgresql start && \
 su postgres sh -c "createuser -d -r -s docker" && \
 su postgres sh -c "createdb -O docker docker" && \
 su postgres sh -c "psql -c \"GRANT ALL PRIVILEGES ON DATABASE docker to docker;\""

EXPOSE 5432
CMD ["su", "postgres", "-c", "/usr/lib/postgresql/$PG_VERSION/bin/postgres -D /var/lib/postgresql/$PG_VERSION/main/ -c config_file=/etc/postgresql/$PG_VERSION/main/postgresql.conf"]

[eric@localhost ubuntu]$ sudo docker build -t test/postgres_ubuntu .

Step 0 : FROM ubuntu:14.04
 ---> 5ba9dab47459														/ local image ubuntu:14.04=latest
Step 1 : MAINTAINER Docker Education Team <education@docker.com>
 ---> Running in 837832f560e7											/ intermediate container	,	
 ---> 4eeb1c9d566f														/ <none> container	,
Removing intermediate container 837832f560e7
Step 2 : ENV PG_VERSION 9.3
 ---> Running in 0cf086012ea2
 ---> cfdc782a539b
Removing intermediate container 0cf086012ea2
Step 3 : RUN apt-get -y install postgresql postgresql-client postgresql-contrib
 ---> Running in 94a942064b4a
...
debconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
debconf: unable to initialize frontend: Readline
debconf: (This frontend requires a controlling tty.)
debconf: falling back to frontend: Teletype
Adding user postgres to group ssl-cert

Creating config file /etc/logrotate.d/postgresql-common with new version
Building PostgreSQL dictionaries from installed myspell/hunspell packages...
Removing obsolete dictionary files:
invoke-rc.d: policy-rc.d denied execution of start.
Setting up postgresql-client (9.3+154) ...
Processing triggers for ureadahead (0.100.0-16) ...
Setting up postgresql-9.3 (9.3.4-1) ...
Creating new cluster 9.3/main ...
  config /etc/postgresql/9.3/main
  data   /var/lib/postgresql/9.3/main
  locale C
  port   5432
update-alternatives: using /usr/share/postgresql/9.3/man/man1/postmaster.1.gz to provide /usr/share/man/man1/postmaster.1.gz (postmaster.1.gz) in auto mode
invoke-rc.d: policy-rc.d denied execution of start.
Setting up postgresql (9.3+154) ...
Setting up postgresql-contrib-9.3 (9.3.4-1) ...
Setting up postgresql-contrib (9.3+154) ...
Processing triggers for libc-bin (2.19-0ubuntu6.5) ...
Processing triggers for sgml-base (1.26+nmu4ubuntu1) ...
 ---> 0357736dd93c
Removing intermediate container 94a942064b4a
Step 4 : RUN echo "host    all             all             0.0.0.0/0 trust" >> /etc/postgresql/$PG_VERSION/main/pg_hba.conf
 ---> Running in 4f71fc2ae141
 ---> a4ca0e2b2e38
Removing intermediate container 4f71fc2ae141
Step 5 : RUN echo "listen_addresses='*'" >> /etc/postgresql/$PG_VERSION/main/postgresql.conf
 ---> Running in e4f9baa0e9a4
 ---> b39570d24885
Removing intermediate container e4f9baa0e9a4
Step 6 : RUN service postgresql start &&  su postgres sh -c "createuser -d -r -s docker" &&  su postgres sh -c "createdb -O docker docker" &&  su postgres sh -c "psql -c \"GRANT ALL PRIVILEGES ON DATABASE docker to docker;\""
 ---> Running in 4c9c201356bd
 * Starting PostgreSQL 9.3 database server
   ...done.
GRANT
 ---> 6c28af3bf18d
Removing intermediate container 4c9c201356bd
Step 7 : EXPOSE 5432
 ---> Running in aab8527f07ca
 ---> 351531a6dd08
Removing intermediate container aab8527f07ca
Step 8 : CMD su postgres -c /usr/lib/postgresql/$PG_VERSION/bin/postgres -D /var/lib/postgresql/$PG_VERSION/main/ -c config_file=/etc/postgresql/$PG_VERSION/main/postgresql.conf
 ---> Running in e0cc04c9c245
 ---> 38dc5ba8cdc9
Removing intermediate container e0cc04c9c245
Successfully built 38dc5ba8cdc9


/ we zien 	,
run service postgresql start
/ Deze staat achter een run	,
/ we kunnen # service niet doen als command /usr/bin/bash is	,
/ TODO

[eric@localhost ubuntu]$ sudo docker run --rm --name pu -P test/postgres_ubuntu
2015-03-14 18:19:32 UTC LOG:  database system was interrupted; last known up at 2015-03-14 16:19:17 UTC
2015-03-14 18:19:32 UTC LOG:  database system was not properly shut down; automatic recovery in progress
2015-03-14 18:19:32 UTC LOG:  redo starts at 0/1782F38
2015-03-14 18:19:32 UTC LOG:  record with zero length at 0/17835C8
2015-03-14 18:19:32 UTC LOG:  redo done at 0/1783598
2015-03-14 18:19:32 UTC LOG:  last completed transaction was at log time 2015-03-14 16:19:17.956083+00
2015-03-14 18:19:32 UTC LOG:  autovacuum launcher started
2015-03-14 18:19:32 UTC LOG:  database system is ready to accept connections
/ OK
[eric@localhost own]$ sudo docker port pu
5432/tcp -> 0.0.0.0:49158

/////////////////////////////////
[eric@localhost hadoop]$ sudo su - postgres -c "psql -h localhost -p 49158"
psql (9.4.1, server 9.3.4)
SSL connection (protocol: TLSv1.2, cipher: AES128-GCM-SHA256, bits: 128, compression: off)
Type "help" for help.

postgres=# \l
                             List of databases
   Name    |  Owner   | Encoding  | Collate | Ctype |   Access privileges   
-----------+----------+-----------+---------+-------+-----------------------
 docker    | docker   | SQL_ASCII | C       | C     | =Tc/docker           +
           |          |           |         |       | docker=CTc/docker
 postgres  | postgres | SQL_ASCII | C       | C     | 
 template0 | postgres | SQL_ASCII | C       | C     | =c/postgres          +
           |          |           |         |       | postgres=CTc/postgres
 template1 | postgres | SQL_ASCII | C       | C     | =c/postgres          +
           |          |           |         |       | postgres=CTc/postgres
(4 rows)

//////////////////////////////////////////////////////////////////
[eric@localhost hadoop]$ sudo su - postgres -c "psql -U docker -h localhost -p 49158"
psql (9.4.1, server 9.3.4)
SSL connection (protocol: TLSv1.2, cipher: AES128-GCM-SHA256, bits: 128, compression: off)
Type "help" for help.

docker=# \dt
No relations found.

[eric@localhost hadoop]$ for i in $(awk '{print $1}' <(sudo docker ps -a));do sudo docker rm $i;done
/ OK

/ 7	. 

/ we overwrite cmd uit Dockerfile	,
[eric@localhost ubuntu]$ sudo docker run --rm --name pu -P -i -t test/postgres_ubuntu /bin/bash

root@5a09a86aaeca:/# service postgresql start
 * Starting PostgreSQL 9.3 database server                                                          [ OK ] 
/ OK

root@5a09a86aaeca:/# dpkg -S /usr/sbin/service
sysvinit-utils: /usr/sbin/service

/ 7	.

/ centos:7


/ Op centos:7 kunnen we service install	, maar is WH meer een front om systemctl heen	,

/ Lees	,
https://wiki.postgresql.org/wiki/YUM_Installation

/ Lees	,
https://registry.hub.docker.com/_/centos/

/ Dit is centos:7	,

/ Verschil met centos:6, want daar is service al, initscripts is al installed	,

[eric@localhost ubuntu]$ sudo docker run --rm --name centos -i -t centos /bin/bash
Unable to find image 'centos:latest' locally
centos:latest: The image you are pulling has been verified
5b12ef8fd570: Pull complete 
88f9454e60dd: Pull complete 
511136ea3c5a: Already exists 
Status: Downloaded newer image for centos:latest
[root@790f51a62c99 /]# yum install initscripts
Installing:
 initscripts                 x86_64              9.49.17-1.el7_0.1              updates              422 k
Installing for dependencies:
 hostname                    x86_64              3.13-3.el7                     base                  17 k
 kmod                        x86_64              14-9.el7                       base                  77 k
 sysvinit-tools              x86_64              2.88-14.dsf.el7                base                  63 k
/ OK

[root@790f51a62c99 /]# yum install postgresql-server
Installing:
 postgresql-server              x86_64              9.2.7-1.el7                 base                 3.8 M
Installing for dependencies:
 postgresql                     x86_64              9.2.7-1.el7                 base                 2.9 M
 postgresql-libs                x86_64              9.2.7-1.el7                 base                 229 k
 systemd-sysv                   x86_64              208-11.el7_0.6              updates               36 k
/ OK

[root@790f51a62c99 /]# service postgresql initdb
Hint: the preferred way to do this is now "postgresql-setup initdb"
/usr/bin/postgresql-setup: line 79: systemctl: command not found
failed to find PGDATA setting in postgresql.service

[root@790f51a62c99 /]# postgresql-setup initdb
/usr/bin/postgresql-setup: line 79: systemctl: command not found
failed to find PGDATA setting in postgresql.service

[root@790f51a62c99 /]# su - postgres -c "pg_ctl initdb"
The files belonging to this database system will be owned by user "postgres".
This user must also own the server process.

The database cluster will be initialized with locale "C".
The default database encoding has accordingly been set to "SQL_ASCII".
The default text search configuration will be set to "english".

fixing permissions on existing directory /var/lib/pgsql/data ... ok
creating subdirectories ... ok
selecting default max_connections ... 100
selecting default shared_buffers ... 32MB
creating configuration files ... ok
creating template1 database in /var/lib/pgsql/data/base/1 ... ok
initializing pg_authid ... ok
initializing dependencies ... ok
creating system views ... ok
loading system objects' descriptions ... ok
creating collations ... ok
creating conversions ... ok
creating dictionaries ... ok
setting privileges on built-in objects ... ok
creating information schema ... ok
loading PL/pgSQL server-side language ... ok
vacuuming database template1 ... ok
copying template1 to template0 ... ok
copying template1 to postgres ... ok

WARNING: enabling "trust" authentication for local connections
You can change this by editing pg_hba.conf or using the option -A, or
--auth-local and --auth-host, the next time you run initdb.

Success. You can now start the database server using:

    /usr/bin/postgres -D /var/lib/pgsql/data
or
    /usr/bin/pg_ctl -D /var/lib/pgsql/data -l logfile start

[root@790f51a62c99 /]# service postgresql start
Redirecting to /bin/systemctl start  postgresql.service
/usr/sbin/service: line 79: /bin/systemctl: No such file or directory

/ 7	.

/ centos:6

[eric@localhost ubuntu]$ sudo docker run --rm --name centos -i -t centos:6 /bin/bash
[root@4bc7116fd1fe /]# yum provides /sbin/service 
initscripts-9.03.46-1.el6.centos.1.x86_64 : The inittab file and the /etc/init.d scripts
/ is er al	,
[root@4bc7116fd1fe /]# yum install postgresql-server
===========================================================================================================
 Package                        Arch                Version                        Repository         Size
===========================================================================================================
Installing:
 postgresql-server              x86_64              8.4.20-1.el6_5                 base              3.4 M
Installing for dependencies:
 postgresql                     x86_64              8.4.20-1.el6_5                 base              2.6 M
 postgresql-libs                x86_64              8.4.20-1.el6_5                 base              201 k

[root@4bc7116fd1fe /]# service postgresql initdb
Initializing database:                                     [  OK  ]
/ Dit werkt nu gewoon	,
[root@4bc7116fd1fe /]# service postgresql start
Starting postgresql service: /etc/init.d/postgresql: line 114: echo: write error: Permission denied
                                                           [  OK  ]
/ Dit werkt nu gewoon	,
[root@4bc7116fd1fe /]# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?          168 Ss       0   0:00 /bin/bash
    1   157   146   146 ?           -1 S       26   0:00 /usr/bin/postmaster -p 5432 -D /var/lib/pgsql/data
  157   159   159   159 ?           -1 Ss      26   0:00 postgres: logger process                         
  157   161   161   161 ?           -1 Ss      26   0:00 postgres: writer process                         
  157   162   162   162 ?           -1 Ss      26   0:00 postgres: wal writer process                     
  157   163   163   163 ?           -1 Ss      26   0:00 postgres: autovacuum launcher process            
  157   164   164   164 ?           -1 Ss      26   0:00 postgres: stats collector process                
    1   168   168     1 ?          168 R+       0   0:00 ps ajx




 


/ Einde DOCKER SERVICE

/ DOCKER JAVA

/ 7	.

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/java/fedora
[eric@localhost fedora]$ cat Dockerfile 
from fedora
run cp /usr/share/zoneinfo/Europe/Amsterdam /etc/localtime
run yum -y install java-1.8.0-openjdk-headless
run yum -y install java-1.8.0-openjdk-devel
run yum clean all

[eric@localhost fedora]$ sudo docker build -t test/java .

/ we doen cp naar /etc/localtime	, want zonder	,
[eric@localhost fedora]$ sudo docker run --rm -it test/java bash
bash-4.3# date
Thu Feb 19 15:09:54 EST 2015

/ Lees	,
http://gbif.blogspot.nl/2011/01/setting-up-hadoop-cluster-part-1-manual.html

/ ntp
/ TODO

/ 7	. 

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/hadoop/fedora
$ vi Dockerfile
/ TODO

/ Einde DOCKER JAVA

/ DOCKER FEDORA SYSTEMD SSHD HTTPD JAVA

/ Lees	,
https://github.com/dockerimages/docker-systemd

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/systemd/fedora
 In fedora is systemd	, in centos7 not	,
[eric@localhost fedora]$ sudo docker build --rm -t local/fd-systemd .

/ in sshd/fedora_2 zonder systemd	,

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/sshd/fedora
[eric@localhost fedora]$ sudo docker build --rm -t local/fd-systemd-sshd .


[eric@localhost fedora]$ sudo docker run --rm --name sshd --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -P local/fd-systemd-sshd

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/java/fedora
[eric@localhost fedora]$ sudo docker build --rm -t local/fd-systemd-sshd-java .

[eric@localhost fedora]$ sudo docker run --rm --name java --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -P local/fd-systemd-sshd-java

/ 13	. 

/ we hebben op de laptop /opt/hadoop-2.6.0	,
/ we hoeven geen hadoop image te maken als we volumes use	,

[eric@localhost fedora]$ sudo docker run --rm --name java --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -v /opt/hadoop-2.6.0:/opt/hadoop-2.6.0 -P local/fd-systemd-sshd-java

[eric@localhost centos7_2]$ sudo su -
[root@localhost ~]# docker port java
22/tcp -> 0.0.0.0:49226
[root@localhost ~]# ssh localhost -p 49226
-bash-4.3# ls /opt
hadoop-2.6.0
/ OK






/ Einde DOCKER FEDORA SYSTEMD SSHD HTTPD JAVA


/ HADOOP INSTALL 

/ 7	.

[eric@localhost scripts]$ pwd
/home/eric/Devel/Docker/scripts
[eric@localhost scripts]$ cat stop-all-containers 
#!/usr/bin/bash
sudo docker stop $(awk 'NR>1{print $1}' <(sudo docker ps -a))
[eric@localhost scripts]$ cat rm-all-containers 
#!/usr/bin/bash
sudo docker rm $(awk 'NR>1{print $1}' <(sudo docker ps -a))
[eric@localhost scripts]$ cat rm-unused-images 
#!/usr/bin/bash
sudo docker rmi $(awk 'NR>1 && $1~/<none>/{print $3}' <(sudo docker images))

[eric@localhost bin]$ pwd
/home/eric/bin
[eric@localhost bin]$ ln -s /home/eric/Devel/Docker/scripts/stop-all-containers .
[eric@localhost bin]$ ln -s /home/eric/Devel/Docker/scripts/rm-all-containers .
[eric@localhost bin]$ ln -s /home/eric/Devel/Docker/scripts/rm-unused-images .

[eric@localhost scripts]$ pwd
/home/eric/Devel/Docker/scripts
[eric@localhost scripts]$ cat template-create-image
sudo docker build --rm -t local/c7-systemd-sshd-java-hadoop .
[eric@localhost scripts]$ cat template-run-image
sudo docker run --rm --name java --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -P local/c7-systemd-sshd-java


/ 7	. 

/ java	 container	,

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/java/fedora
$ vi Dockerfile

from local/fd-systemd-sshd 
run cp /usr/share/zoneinfo/Europe/Amsterdam /etc/localtime
run yum -y install java-1.8.0-openjdk-headless
run yum -y install java-1.8.0-openjdk-devel
run yum clean all
cmd ["/usr/lib/systemd/systemd"]

[eric@localhost fedora]$ sudo docker build --rm -t local/fd-systemd-sshd-java .
[eric@localhost fedora]$ sudo docker run --rm --name java --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -P local/fd-systemd-sshd-java

[eric@localhost fedora]$ sudo su -
[root@localhost ~]# docker port java
22/tcp -> 0.0.0.0:49214
[root@localhost ~]# ssh localhost -p 49214
...

/ 7	. 

/ hadoop container,	

/ 13	. 

/ VOORLOPIG

[eric@localhost centos7]$ pwd
/home/eric/Devel/Docker/hadoop/fedora

[eric@localhost centos7]$ cat Dockerfile 

from local/fd-systemd-sshd-java

run curl -O  http://apache.proserve.nl/hadoop/common/stable/hadoop-2.6.0.tar.gz
run	yum -y install tar;\
	tar -C /opt -xvzf /hadoop-2.6.0.tar.gz;\
	rm -f /hadoop-2.6.0.tar.gz
run export JAVA_HOME=/etc/alternatives/java_sdk

cmd ["/usr/lib/systemd/systemd"]

bash-4.3# curl -O  http://apache.proserve.nl/hadoop/common/stable/hadoop-2.6.0.tar.gz
/ OK
bash-4.3# yum install -y tar
bash-4.3#  tar -C /opt -xvzf hadoop-2.6.0.tar.gz 

bash-4.3#  find /opt/hadoop-2.6.0/ -name "*examples*"
/opt/hadoop-2.6.0/share/doc/hadoop/api/org/apache/hadoop/examples
/opt/hadoop-2.6.0/share/doc/hadoop/api/org/apache/hadoop/security/authentication/examples
/opt/hadoop-2.6.0/share/doc/hadoop/hadoop-mapreduce-examples
/opt/hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples
/opt/hadoop-2.6.0/share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.6.0-test-sources.jar
/opt/hadoop-2.6.0/share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.6.0-sources.jar
/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib-examples
/opt/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar



/ 13	. 

/ VOORLOPIG

/ of met volumes	,

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/hadoop/fedora
$ vi Dockerfile

from local/fd-systemd-sshd-java
#run yum -y install hadoop-common hadoop-hdfs hadoop-mapreduce hadoop-mapreduce-examples hadoop-yarn 

run export JAVA_HOME=/etc/alternatives/java_sdk
run yum -y install bashdb


cmd ["/usr/lib/systemd/systemd"]

/ Maar we use nu de java-container	,


[eric@localhost ]$ sudo docker run --rm --name java --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -v /opt/hadoop-2.6.0:/opt/hadoop-2.6.0 -P local/fd-systemd-sshd-java

[eric@localhost ]$ sudo su -
[root@localhost ~]# docker port java
22/tcp -> 0.0.0.0:49226
[root@localhost ~]# ssh localhost -p 49226

export JAVA_HOME=/etc/alternatives/java_sdk


/ Einde HADOOP INSTALL

/ HADOOP


/ 13	. 



# cd /opt/hadoop-2.6.0
bash-4.3# bin/hadoop jar  share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar
...
  teravalidate: Checking results of terasort
  wordcount: A map/reduce program that counts the words in the input files.
  wordmean: A map/reduce program that counts the average length of the words in the input files.
...
/OK
bash-4.3#  rm -rf /opt/output/
bash-4.3# bin/hadoop jar  share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount README.txt /opt/output
bash-4.3# vi /opt/output/part-r-00000 
(BIS),  1
(ECCN)  1
(TSU)   1
(see    1
5D002.C.1,      1
740.13) 1
<http://www.wassenaar.org/>     1
Administration  1
Apache  1
...

bash-4.3# rm -rf /opt/output;bin/hadoop jar  share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount README.txt LICENSE.txt /opt/output
/ OK

/ 7	.

/ compile	,

/ Lees hadoop docs, 
http://hadoop.apache.org/docs/stable/	, en 	,
/ en kies	MapReduce tutorial	,
http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html

/ Lees API docs	,
http://hadoop.apache.org/docs/stable/api/index.html


/ 13	 

/ zo	,

bash-4.3# (cd /opt/src; jar xvf /opt/hadoop-2.6.0/share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.6.0-sources.jar)

bash-4.3# javac -classpath ../hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:../hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:../hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:../hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar  org/apache/hadoop/examples/WordCount.java 
/ OK


/ 13	. 

/ of zo	, 

# export JAVA_HOME=/etc/alternatives/java_sdk
/ al	,
# export HADOOP_CLASSPATH=$JAVA_HOME/lib/tools.jar

/ Het is nog onduidelijk , WH pakt hij de WordCount uit de hadoop examples	, niet uit onze wcnt.jar hieronder, 
/ TODO

bash-4.3#  ../hadoop-2.6.0/bin/hadoop com.sun.tools.javac.Main org/apache/hadoop/examples/WordCount.java 
/ OK

bash-4.3# find -name "*.class"
./org/apache/hadoop/examples/WordCount$TokenizerMapper.class
./org/apache/hadoop/examples/WordCount$IntSumReducer.class
./org/apache/hadoop/examples/WordCount.class

bash-4.3#  jar cvf wcnt.jar $(find -name WordCount*.class)
bash-4.3# rm -rf ../output/; ../hadoop-2.6.0/bin/hadoop jar wcnt.jar  org.apache.hadoop.examples.WordCount ../hadoop-2.6.0/README.txt ../output
bash-4.3# vim ../output/part-r-00000 
...

/ 7	.

# pwd
/opt/src

#$ vi MyWordCount.java

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
public class MyWordCount {
  private static final Log LOG =LogFactory.getLog(DancingLinks.class.getName());
...
    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
System.out.println("Reduce: ");
LOG.info("Reduce");
/ Beide OK	, 

/ we zien	,
Reduce: 
15/02/21 15:04:24 INFO examples.MyWordCount: Reduce

 /7	.


/ Lees hadoop script	,
/ we zien	,
# export HADOOP_OPTS=-Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n
/ OK
bash-4.3# rm -rf ../output/; ../hadoop-2.6.0/bin/hadoop jar wcnt.jar  org.apache.hadoop.examples.MyWordCount INPUT.txt ../output
/ OK
/ We moeten wel een System.in.read() set in MyWordCount

bash-4.3# rm -rf ../output/; ../hadoop-2.6.0/bin/hadoop jar wcnt.jar  org.apache.hadoop.examples.WordCount INPUT.txt ../output
/ werkt OOK	, 
/ Let op WordCount	, zit niet in de wcnt.jar, maar in de examples	, WH in classpath	,
/ TODO

/ we doen	,
# vim wcnt.bash
#!/bin/bash

/ of	,
 ../hadoop-2.6.0/bin/hadoop com.sun.tools.javac.Main org/apache/hadoop/examples/MyWordCount.java 
/ of	,
rm -f $(find -name "*.class");javac -classpath ../hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:../hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:../hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:../hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:../hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar  org/apache/hadoop/examples/MyWordCount.java

rm -f wcnt.jar;jar cvf wcnt.jar $(find -name MyWordCount*.class)

rm -rf ../output/; ../hadoop-2.6.0/bin/hadoop jar wcnt.jar  org.apache.hadoop.examples.MyWordCount INPUT.txt ../output

$ vi INPUT.txt
Foo and Bar are walking
Foo and Bar are walking
Foo and Bar are walking

# ./wcnt.bash

Map: 
TOKEN: Foo
TOKEN: and
TOKEN: Bar
TOKEN: are
TOKEN: walking
Map: 
TOKEN: Foo
TOKEN: and
TOKEN: Bar
TOKEN: are
TOKEN: walking
Map: 
TOKEN: Foo
TOKEN: and
TOKEN: Bar
TOKEN: are
TOKEN: walking

Reduce: 
KEY: Bar, SUM: 1
KEY: Bar, SUM: 2
KEY: Bar, SUM: 3
Reduce: 
KEY: Foo, SUM: 1
KEY: Foo, SUM: 2
KEY: Foo, SUM: 3
Reduce: 
KEY: and, SUM: 1
KEY: and, SUM: 2
KEY: and, SUM: 3
Reduce: 
KEY: are, SUM: 1
KEY: are, SUM: 2
KEY: are, SUM: 3
Reduce: 
KEY: walking, SUM: 1
KEY: walking, SUM: 2
KEY: walking, SUM: 3

Reduce: 
KEY: Bar, SUM: 3
Reduce: 
KEY: Foo, SUM: 3
Reduce: 
KEY: and, SUM: 3
Reduce: 
KEY: are, SUM: 3
Reduce: 
KEY: walking, SUM: 3

/ Waarom die laatste keren in reduce	?
/ TODO

/ 13	. 

/ Lees	,
http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html

 /ze hebben ExampleDriver met een ProgramDriver (WH deprecated)	, daardoor kun je grep geven ipv.


/ 13.

bash-4.3# rm -rf ../output;../hadoop-2.6.0/bin/hadoop jar ../hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount INPUT.txt ../output 
/ OK
bash-4.3# rm -rf ../output;../hadoop-2.6.0/bin/hadoop jar ../hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar org.apache.hadoop.examples.WordCount INPUT.txt ../output 
/ ERR
Unknown program 'org.apache.hadoop.examples.WordCount' chosen.
Valid program names are:
  wordcount: A map/reduce program that counts the words in the input files.
/ TODO

/ Einde HADOOP

/ HADOOP HDFS

/ 7	.

/ Lees	http://hadoop.apache.org/docs/stable/, Single Node Setup	,
http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html

/ en boek(31)

bash-4.3# pwd
/opt/hadoop-2.6.0/etc/hadoop

$ vi core-site.xml

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

$ vi hdfs-site.xml

<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

bash-4.3# yum -y install openssh-server openssh-clients openssh

/ Op laptop	,
[eric@localhost fedora]$ sudo grep systemd /var/log/audit/audit.log | audit2allow -M mypol
[eric@localhost fedora]$ vi mypol.te 
module mypol 1.0;

require {
	type sysfs_t;
	type svirt_lxc_net_t;
	type proc_t;
	class dir mounton;
}

#============= svirt_lxc_net_t ==============
allow svirt_lxc_net_t proc_t:dir mounton;
allow svirt_lxc_net_t sysfs_t:dir mounton;

[eric@localhost fedora]$ sudo semodule -i mypol.pp

/ 7	.

/ Lees	,
https://docs.docker.com/installation/fedora/

/ 7	.

/ Lees Examples, Dockerizing an SSH Daemon	,
https://docs.docker.com/examples/running_ssh_service/

/ Lees	,
http://www.cyberciti.biz/faq/centos-ssh/

# vi /etc/ssh/sshd_config
PermitRootLogin yes

/ we zien	,
/usr/lib/systemd/system/sshd.service

bash-4.3#  sshd
sshd re-exec requires execution with an absolute path
/ we moeten het hele pad geven	,
bash-4.3#  /usr/sbin/sshd
Could not load host key: /etc/ssh/ssh_host_rsa_key
Could not load host key: /etc/ssh/ssh_host_ecdsa_key
Could not load host key: /etc/ssh/ssh_host_ed25519_key

bash-4.3# ssh-keygen -A
ssh-keygen: generating new host keys: RSA1 RSA DSA ECDSA ED25519 
bash-4.3# /usr/sbin/sshd
/ OK
bash-4.3# vi /etc/ssh/
moduli                    ssh_host_ecdsa_key        ssh_host_key              sshd_config
ssh_config                ssh_host_ecdsa_key.pub    ssh_host_key.pub          
ssh_host_dsa_key          ssh_host_ed25519_key      ssh_host_rsa_key          
ssh_host_dsa_key.pub      ssh_host_ed25519_key.pub  ssh_host_rsa_key.pub      

bash-4.3# ps ajx            
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?         5537 Ss       0   0:01 bash
    1  5530  5530  5530 ?           -1 Ss       0   0:00 /usr/sbin/sshd

/ 7	. 

/ we lezen weer	,
http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html

bash-4.3# ssh localhost
The authenticity of host 'localhost (::1)' can't be established.
ECDSA key fingerprint is 24:30:36:b1:60:15:d7:63:57:8c:59:ce:d1:53:56:bb.
Are you sure you want to continue connecting (yes/no)?  yes
Please type 'yes' or 'no': ye
Please type 'yes' or 'no': yes
Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
root@localhost's password: 
ctrl-c

bash-4.3#  ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
Generating public/private dsa key pair.
Your identification has been saved in /root/.ssh/id_dsa.
Your public key has been saved in /root/.ssh/id_dsa.pub.
The key fingerprint is:
d7:90:51:ca:fd:4c:cf:a3:7e:bd:d0:41:07:cb:9a:12 root@c89fb06b3d71
The key's randomart image is:
+--[ DSA 1024]----+
|          ... .  |
|         . = . o |
|          E . +..|
|           + *.o.|
|        S o + ooo|
|         . .  o o|
|             o ..|
|            . ...|
|             ....|
+-----------------+

/ Wat is verschil	,
bash-4.3# vi /etc/ssh/ssh_host_dsa_key
/ en	,
bash-4.3# vi ~/.ssh/id_dsa
/ en	,
bash-4.3# vi /etc/ssh/ssh_host_dsa_key.pub 
/ en 
bash-4.3# vi ~/.ssh/id_dsa.pub
/ TODO

bash-4.3#  cat /root/.ssh/known_hosts 
localhost ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEqu2fp6e8g6YLTIDaNFSpEDdzej0R3NW21wlXb0Rgms4IiU95DnTtB+R12GaWGakn+tM1A0tW9dgcIok6ol+lU=

bash-4.3#  cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
bash-4.3# ssh localhost
-bash-4.3# 
/ passphrase login	,
-bash-4.3# exit
logout
Connection to localhost closed.
bash-4.3# cat ~/.ssh/authorized_keys 
ssh-dss AAAAB3NzaC1kc3MAAACBAIgEDD80TfVy/ffi1QZWE/Nrhi+sh7w90S+eMYG4aBkAn4A6dUHzxVKBXXyPJpecLEZ8DF9oGNx79vB4XKaoKP/LtfZExTxx6iCp27VXOixEc86oW+KxYyf5WgliZQvylvp8H9FggZ19qjsMpyUaSuFEdldz4wk2AzQDdl4ZqUSvAAAAFQDBGcO5LHjFxGsj5kxv8u4YiVtRvQAAAIEAhRCoBnunvE+XRsctGFe7VbKJYqZj397RJ7ldJV6wrefjMTm8dv3gw8l2LOTjMsLpmTPbhkb8q6UwIJi11IyFcfGrZBgyRs8wl6eudB1UY2oVLKefyZV2c0o0/ZR4dMdx5wYMHjZJWROz33Ov6B6lH2rutrzLUwKF/rAZ2+9wZMgAAACAVqIPFO9yHVBiDUSIjdxp3VKLUmzZrDojYJASkMXiSIPD6KknXS1NCkdQUWArAQNcKXzFiWERV86nnvQhuJCD0pjWnHfv390OPsp93qMH7kiuZag2gKU/xTyZtppW/njUT8aUi+De4lsJTz9L05LLN3CVmI6VynOJP5UMOPJm5OU= root@c89fb06b3d71


/ Intermezzo

/ we doen ook op de laptop	,

[eric@localhost ]$ sudo systemctl start sshd
[eric@localhost ]$ sudo systemctl enable sshd
Created symlink from /etc/systemd/system/multi-user.target.wants/sshd.service to /usr/lib/systemd/system/sshd.service.

[eric@localhost sinatra]$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
Generating public/private dsa key pair.
Your identification has been saved in /home/eric/.ssh/id_dsa.
Your public key has been saved in /home/eric/.ssh/id_dsa.pub.
The key fingerprint is:
8b:c5:f9:56:02:69:03:3d:a1:1a:16:cc:55:01:8c:7f eric@localhost.localdomain
The key's randomart image is:
+--[ DSA 1024]----+
|   o.++++o       |
|    +..oo.       |
|    o.. =.       |
|   . o.oE+       |
|    .  .S . .    |
|       o o o     |
|      . . o      |
|         .       |
|                 |
+-----------------+
[eric@localhost sinatra]$ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
[eric@localhost sinatra]$ ssh localhost
Last login: Thu Mar  5 21:16:16 2015 from localhost.localdomain
[eric@localhost ~]$ exit
logout
Connection to localhost closed.



/ Einde Intermezzo

# cd ../../
bash-4.3# pwd
/opt/hadoop-2.6.0

bash-4.3# bin/hdfs namenode -format 
Listening for transport dt_socket at address: 4000
15/02/24 15:18:49 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = c89fb06b3d71/172.17.1.20
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/opt/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/opt/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/opt/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/etc/alternatives/java_sdk/lib/tools.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_31
************************************************************/
15/02/24 15:18:49 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
15/02/24 15:18:49 INFO namenode.NameNode: createNameNode [-format]
Formatting using clusterid: CID-d9ef4161-e351-4229-adad-7f7e5f800f8c
15/02/24 15:18:50 INFO namenode.FSNamesystem: No KeyProvider found.
15/02/24 15:18:50 INFO namenode.FSNamesystem: fsLock is fair:true
15/02/24 15:18:50 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
15/02/24 15:18:50 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
15/02/24 15:18:50 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
15/02/24 15:18:50 INFO blockmanagement.BlockManager: The block deletion will start around 2015 Feb 24 15:18:50
15/02/24 15:18:50 INFO util.GSet: Computing capacity for map BlocksMap
15/02/24 15:18:50 INFO util.GSet: VM type       = 64-bit
15/02/24 15:18:50 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB
15/02/24 15:18:50 INFO util.GSet: capacity      = 2^21 = 2097152 entries
15/02/24 15:18:50 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
15/02/24 15:18:50 INFO blockmanagement.BlockManager: defaultReplication         = 1
15/02/24 15:18:50 INFO blockmanagement.BlockManager: maxReplication             = 512
15/02/24 15:18:50 INFO blockmanagement.BlockManager: minReplication             = 1
15/02/24 15:18:50 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
15/02/24 15:18:50 INFO blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
15/02/24 15:18:50 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
15/02/24 15:18:50 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
15/02/24 15:18:50 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
15/02/24 15:18:50 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
15/02/24 15:18:50 INFO namenode.FSNamesystem: supergroup          = supergroup
15/02/24 15:18:50 INFO namenode.FSNamesystem: isPermissionEnabled = true
15/02/24 15:18:50 INFO namenode.FSNamesystem: HA Enabled: false
15/02/24 15:18:50 INFO namenode.FSNamesystem: Append Enabled: true
15/02/24 15:18:50 INFO util.GSet: Computing capacity for map INodeMap
15/02/24 15:18:50 INFO util.GSet: VM type       = 64-bit
15/02/24 15:18:50 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB
15/02/24 15:18:50 INFO util.GSet: capacity      = 2^20 = 1048576 entries
15/02/24 15:18:50 INFO namenode.NameNode: Caching file names occuring more than 10 times
15/02/24 15:18:50 INFO util.GSet: Computing capacity for map cachedBlocks
15/02/24 15:18:50 INFO util.GSet: VM type       = 64-bit
15/02/24 15:18:50 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB
15/02/24 15:18:50 INFO util.GSet: capacity      = 2^18 = 262144 entries
15/02/24 15:18:50 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
15/02/24 15:18:50 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
15/02/24 15:18:50 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
15/02/24 15:18:50 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
15/02/24 15:18:50 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
15/02/24 15:18:50 INFO util.GSet: Computing capacity for map NameNodeRetryCache
15/02/24 15:18:50 INFO util.GSet: VM type       = 64-bit
15/02/24 15:18:50 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
15/02/24 15:18:50 INFO util.GSet: capacity      = 2^15 = 32768 entries
15/02/24 15:18:50 INFO namenode.NNConf: ACLs enabled? false
15/02/24 15:18:50 INFO namenode.NNConf: XAttrs enabled? true
15/02/24 15:18:50 INFO namenode.NNConf: Maximum size of an xattr: 16384
15/02/24 15:18:50 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1456685200-172.17.1.20-1424809130423
15/02/24 15:18:50 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.
15/02/24 15:18:50 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
15/02/24 15:18:50 INFO util.ExitUtil: Exiting with status 0
15/02/24 15:18:50 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at c89fb06b3d71/172.17.1.20
************************************************************/
bash-4.3# sbin/start-dfs.sh 
...
Listening: ssh: Could not resolve hostname listening: Name or service not known
... 

/ Lees	,
http://stackoverflow.com/questions/21326274/hadoop-2-2-0-name-or-service-not-known-warning
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"

/ Lees	,
http://stackoverflow.com/questions/20011252/hadoop-2-2-0-64-bit-installing-but-cannot-start/20371832#20371832

The root cause is that the default native library in hadoop is built for 32-bit. The solution

1) Setup some environment variables in .bash_profile. Please refer to https://gist.github.com/ruo91/7154697

2) Rebuild your hadoop native library, please refer to 
http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/NativeLibraries.html


/ ad (1) Lees	,
https://gist.github.com/ruo91/7154697
export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_PREFIX/lib"
/ of	,
export HADOOP_HOME=/usr/local/aquarius/hadoop-2.2.0
export PATH=$HADOOP_HOME/bin:$PATH:...
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"

/ ad (2) Lees	,
http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/NativeLibraries.html

/ Inderdaad	,
bash-4.3#  find -name "*.so"
./lib/native/libhadoop.so
./lib/native/libhdfs.so

/////////////////////////////////////////////////////////////////////////////////////////
/ uiteindelijk we doen 	,

/ we zijn nog steeds op 	,
http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html

bash-4.3# export HADOOP_INSTALL=$(pwd)
bash-4.3# export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
bash-4.3#  export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"


bash-4.3# sbin/start-dfs.sh 
...
++ /opt/hadoop-2.6.0/bin/hdfs getconf -namenodes
+ /opt/hadoop-2.6.0/sbin/hadoop-daemons.sh --config /opt/hadoop-2.6.0/etc/hadoop --hostnames localhost --script /opt/hadoop-2.6.0/sbin/hdfs start namenode
+ /opt/hadoop-2.6.0/sbin/hadoop-daemons.sh --config /opt/hadoop-2.6.0/etc/hadoop --script /opt/hadoop-2.6.0/sbin/hdfs start datanode
++ /opt/hadoop-2.6.0/bin/hdfs getconf -secondarynamenodes
Starting secondary namenodes [0.0.0.0]
+ /opt/hadoop-2.6.0/sbin/hadoop-daemons.sh --config /opt/hadoop-2.6.0/etc/hadoop --hostnames 0.0.0.0 --script /opt/hadoop-2.6.0/sbin/hdfs start secondarynamenode
The authenticity of host '0.0.0.0 (0.0.0.0)' can't be established.
ECDSA key fingerprint is 24:30:36:b1:60:15:d7:63:57:8c:59:ce:d1:53:56:bb.
Are you sure you want to continue connecting (yes/no)? yes
0.0.0.0: Warning: Permanently added '0.0.0.0' (ECDSA) to the list of known hosts.
++ /opt/hadoop-2.6.0/bin/hdfs getconf -confKey dfs.namenode.shared.edits.dir
++ /opt/hadoop-2.6.0/bin/hdfs getconf -confKey dfs.ha.automatic-failover.enabled

/////////////////////////////////////////////////////////////////////////////////////

/ 7	. 

 /we zijn nog op 	,
http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html

NameNode - http://localhost:50070/
/ TODO

/ 7	.

/ Lees	,
http://tecadmin.net/setup-hadoop-2-4-single-node-cluster-on-linux/

/ we debug	,

# bashdb sbin/start-dfs.sh

/ op een gegeven moment	,
bashdb<50> bt
->0 in file `/opt/hadoop-2.6.0/etc/hadoop/hadoop-env.sh' at line 36
##1 source("/opt/hadoop-2.6.0/etc/hadoop/hadoop-env.sh") called from file `/opt/hadoop-2.6.0/sbin/../libexec/hadoop-config.sh' at line 133
 25:    export JAVA_HOME=${JAVA_HOME}
bashdb<47> pr $JAVA_HOME
/etc/alternatives/java_sdk

##2 source("/opt/hadoop-2.6.0/sbin/../libexec/hadoop-config.sh") called from file `/opt/hadoop-2.6.0/sbin/../libexec/hdfs-config.sh' at line 28
##3 source("/opt/hadoop-2.6.0/sbin/../libexec/hdfs-config.sh") called from file `sbin/start-dfs.sh' at line 30
##4 source("sbin/start-dfs.sh") called from file `/usr/bin/bashdb' at line 96
##5 main() called from file `/usr/bin/bashdb' at line 0
b

(/opt/hadoop-2.6.0/libexec/hadoop-config.sh:239):
239: => if [ "x$JAVA_LIBRARY_PATH" != "x" ]; then
/ TODO

55:	NAMENODES=$($HADOOP_PREFIX/bin/hdfs getconf -namenodes)
bashdb<148> 
15/02/25 16:18:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
/ TODO

(/opt/hadoop-2.6.0/sbin/start-dfs.sh:60):
60:	  --config "$HADOOP_CONF_DIR" \
bashdb<151> l
 55:    NAMENODES=$($HADOOP_PREFIX/bin/hdfs getconf -namenodes)
 56:    
 57:    echo "Starting namenodes on [$NAMENODES]"
 58:    
 59:    "$HADOOP_PREFIX/sbin/hadoop-daemons.sh" \
 60: =>   --config "$HADOOP_CONF_DIR" \
 61:      --hostnames "$NAMENODES" \
 62:      --script "$bin/hdfs" start namenode $nameStartOpt
 63:    
 64:    #---------------------------------------------------------
bashdb<152> n
localhost: Error: JAVA_HOME is not set and could not be found.
/ TODO

/ Einde HADOOP HDFS

/ HADOOP

/ als we test/openssh nemen, en daarop test/hadoop_tmp build	, dan zien we dat we /root/.ssh/authorized keys hebben, maar sshd draait NIET	,

[eric@localhost fedora_tmp]$ pwd
/home/eric/Devel/Docker/hadoop/fedora_tmp
[eric@localhost fedora_tmp]$ cat Dockerfile 
from test/openssh
[eric@localhost fedora_tmp]$ sudo docker build -t test/hadoop_tmp .
[eric@localhost fedora_tmp]$ sudo docker run --rm -it test/hadoop_tmp /usr/bin/bash
bash-4.3# cat root/.ssh/authorized_keys 
ssh-dss AAAAB3...
bash-4.3# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?            9 Ss       0   0:00 /usr/bin/bash
    1     9     9     1 ?            9 R+       0   0:00 ps ajx



/ Einde HADOOP

/ HADOOP SINGLE CLUSTER 

/ 7	. 

/ Als we standalone willen doen	, zorg dan dat core-site.xml leeg is	,
-bash-4.3# vi etc/hadoop/core-site.xml 
<configuration>
<!--
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
-->
</configuration>

/ anders ERR	, geen conn naar 9000 ... 
/ TODO

/ 7	. 

-bash-4.3# bash --debugger bin/hdfs namenode -format 

-bash-4.3# bash --debugger sbin/start-dfs.sh ""
/ deze start	,
/etc/alternatives/java_sdk/bin/java -D...  org.apache.hadoop.hdfs.server.namenode.NameNode
/etc/alternatives/java_sdk/bin/java -D...  org.apache.hadoop.hdfs.server.datanode.DataNode
/etc/alternatives/java_sdk/bin/java -D...  org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
/ sbin/start-dfs.sh calls 3 keer sbin/hadoop-daemons.sh	, met args namenode/datanode/secondarynamenode	,
/ sbin/hadoop-daemons.sh calls sbin/slaves.sh
/ sbin/slaves.sh calls ssh localhost	, localhost omdat localhost ook de slave is	, en gaat dan sbin/hadoop-daemon.sh exec	, 
/ sbin/hadoop-daemon.sh calls bin/hdfs	,met arg namenode/datanode/secondarynamenode	,
/ sbin/hdfs zoekt de class type	,
  CLASS='org.apache.hadoop.hdfs.server.namenode.NameNode'
  CLASS='org.apache.hadoop.hdfs.server.datanode.DataNode'
  CLASS='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode'
/ en execs deze	,

/ 13	. 

/ we moeten 3 files edit	,

/ we moeten  ALLEEN voor single cluster config , vanwege ssh localhost	,
-bash-4.3# vi etc/hadoop/hadoop-env.sh
export JAVA_HOME="/etc/alternatives/java_sdk"
# export JAVA_HOME= ${JAVA_HOME} 
/ bij distributed cluster config hoeft dit WH niet	,
/ TODO

$ vi etc/hadoop/core-site.xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

$ vi etc/hadoop/hdfs-site.xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

/ 7	.

-bash-4.3# sbin/start-dfs.sh
Starting namenodes on [localhost]
localhost: starting namenode, logging to /opt/hadoop-2.6.0/logs/hadoop-root-namenode-3791134b8c67.out
localhost: starting datanode, logging to /opt/hadoop-2.6.0/logs/hadoop-root-datanode-3791134b8c67.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /opt/hadoop-2.6.0/logs/hadoop-root-secondarynamenode-3791134b8c67.out

-bash-4.3# bin/hdfs dfs -mkdir /user
-bash-4.3# bin/hdfs dfs -mkdir /user/root
-bash-4.3# bin/hdfs  dfs -put etc/hadoop input
/ creates in hdfs input/ in /root/user/ en cp alle files in etc/hadoop/ in hdfs's /user/root/input/	,	 
-bash-4.3# bin/hadoop  jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar grep input output 'dfs[a-z.]+'

/ these 2	,
-bash-4.3# bin/hdfs dfs -get output output
-bash-4.3# cat output/*
/ or this 1	,
-bash-4.3# bin/hdfs dfs -cat output/*
6	dfs.audit.logger
4	dfs.class
3	dfs.server.namenode.
2	dfs.period
2	dfs.audit.log.maxfilesize
2	dfs.audit.log.maxbackupindex
1	dfsmetrics.log
1	dfsadmin
1	dfs.servers
1	dfs.replication
1	dfs.file

-bash-4.3# sbin/stop-dfs.sh
Stopping namenodes on [localhost]
localhost: stopping namenode
localhost: stopping datanode
Stopping secondary namenodes [0.0.0.0]
0.0.0.0: stopping secondarynamenode

/ 13	. 

-bash-4.3# sbin/start-dfs.sh
Starting namenodes on [localhost]
localhost: starting namenode, logging to /opt/hadoop-2.6.0/logs/hadoop-root-namenode-3791134b8c67.out
localhost: starting datanode, logging to /opt/hadoop-2.6.0/logs/hadoop-root-datanode-3791134b8c67.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /opt/hadoop-2.6.0/logs/hadoop-root-secondarynamenode-3791134b8c67.out
-bash-4.3# bin/hdfs dfs -cat output/*
6	dfs.audit.logger
4	dfs.class
3	dfs.server.namenode.
2	dfs.period
2	dfs.audit.log.maxfilesize
2	dfs.audit.log.maxbackupindex
1	dfsmetrics.log
1	dfsadmin
1	dfs.servers
1	dfs.replication
1	dfs.file
/ Is er nog	,

/ 13	. 

/ Om de mogelijkheden achter # bin/hdfs dfs te weten	, geef helemaal geen args	,
-bash-4.3# bin/hdfs dfs
Usage: hadoop fs [generic options]
	[-appendToFile <localsrc> ... <dst>]
	[-cat [-ignoreCrc] <src> ...]
...

/ we zien optie -help	, deze doet zoals hierboven, zonder optie, en geeft van allen een desc	,
-bash-4.3# bin/hdfs dfs -help
...

/ bijv	,
-ls [-d] [-h] [-R] [<path> ...] :
  List the contents that match the specified file pattern. If path is not
  specified, the contents of /user/<currentUser> will be listed. Directory entries
  are of the form:
  	permissions - userId groupId sizeOfDirectory(in bytes)
  modificationDate(yyyy-MM-dd HH:mm) directoryName
  
  and file entries are of the form:
  	permissions numberOfReplicas userId groupId sizeOfFile(in bytes)
  modificationDate(yyyy-MM-dd HH:mm) fileName
                                                                                 
  -d  Directories are listed as plain files.                                     
  -h  Formats the sizes of files in a human-readable fashion rather than a number
      of bytes.                                                                  
  -R  Recursively list the contents of directories.                              

-bash-4.3# bin/hdfs dfs -ls 
Found 2 items
drwxr-xr-x   - root supergroup          0 2015-03-28 18:29 input
drwxr-xr-x   - root supergroup          0 2015-03-28 18:31 output
/ Inderdaad	, in hdfs's /user/root	,
-bash-4.3# bin/hdfs dfs -ls input
Found 29 items
-rw-r--r--   1 root supergroup       4436 2015-03-28 18:29 input/capacity-scheduler.xml
-rw-r--r--   1 root supergroup       1335 2015-03-28 18:29 input/configuration.xsl
-rw-r--r--   1 root supergroup        318 2015-03-28 18:29 input/container-executor.cfg
-rw-r--r--   1 root supergroup        884 2015-03-28 18:29 input/core-site.xml
...
/ alle files uit gewone etc/hadoop	,
-bash-4.3# bin/hdfs dfs -ls output
Found 2 items
-rw-r--r--   1 root supergroup          0 2015-03-28 18:31 output/_SUCCESS
-rw-r--r--   1 root supergroup        197 2015-03-28 18:31 output/part-r-00000
-bash-4.3# bin/hdfs dfs -cat output/*
/ concats and prints	, _SUCCESS is toch leeg	,

/ Met arg	,
-bash-4.3# bin/hdfs dfs -ls /
Found 1 items
drwxr-xr-x   - root supergroup          0 2015-03-28 17:15 /user

-bash-4.3# bin/hdfs dfs -ls -d /
drwxr-xr-x   - root supergroup          0 2015-03-28 16:58 /

/ 13	. 

-bash-4.3# bin/hdfs dfs -rm -r /user/root
15/03/29 05:34:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /user/root
/ Geef bij delete dir altijd -r	,
-bash-4.3# bin/hdfs dfs -put etc/hadoop input
put: `input': No such file or directory
Did you mean -mkdir?  This command begins with a dash.
-bash-4.3# bin/hdfs dfs -mkdir /user/root

-bash-4.3# bin/hdfs dfs -put etc/hadoop input
/ of
-bash-4.3# bin/hdfs dfs -put etc/hadoop /user/root/input
/ OK

-bash-4.3# bin/hdfs dfs -ls  input
Found 30 items
-rw-r--r--   1 root supergroup       4436 2015-03-29 05:35 input/capacity-scheduler.xml
-rw-r--r--   1 root supergroup       1335 2015-03-29 05:35 input/configuration.xsl
...

/ we kunnen ook	niet in de default /user/root/ -put	, maar explicit geven /user/input bijv	,
-bash-4.3#  bin/hdfs dfs -mkdir  /user/input
-bash-4.3#  bin/hdfs dfs -put etc/hadoop/  /user/input
-bash-4.3#  bin/hdfs dfs -rm -r /user/input
15/03/29 05:57:54 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /user/input

/ 7	. 

/ Met yarn in single cluster config	,

-bash-4.3# sbin/stop-dfs.sh 
/ TODO (of dit moet?)
-bash-4.3# cp etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xml
-bash-4.3# vi etc/hadoop/mapred-site.xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

-bash-4.3# vi etc/hadoop/yarn-site.xml 
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>

-bash-4.3# sbin/start-dfs.sh

-bash-4.3# sbin/start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /opt/hadoop-2.6.0/logs/yarn-root-resourcemanager-.out
localhost: starting nodemanager, logging to /opt/hadoop-2.6.0/logs/yarn-root-nodemanager-3791134b8c67.out
-bash-4.3# sbin/start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /opt/hadoop-2.6.0/logs/yarn-root-resourcemanager-.out
localhost: starting nodemanager, logging to /opt/hadoop-2.6.0/logs/yarn-root-nodemanager-3791134b8c67.out

-bash-4.3# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
/etc/alternatives/java_sdk/bin/java -Dproc_namenode ... org.apache.hadoop.hdfs.server.namenode.NameNode
/etc/alternatives/java_sdk/bin/java -Dproc_datanode ... org.apache.hadoop.hdfs.server.datanode.DataNode
/etc/alternatives/java_sdk/bin/java -Dproc_secondarynamenode ... org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
/etc/alternatives/java_sdk/bin/java -Dproc_resourcemanager ... org.apache.hadoop.yarn.server.resourcemanager.ResourceManager
/etc/alternatives/java_sdk/bin/java -Dproc_nodemanager ... org.apache.hadoop.yarn.server.nodemanager.NodeManager

-bash-4.3# bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar grep input output 'dfs[a-z.]+'
15/03/29 06:52:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
15/03/29 06:52:50 WARN mapreduce.JobSubmitter: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
15/03/29 06:52:50 INFO input.FileInputFormat: Total input paths to process : 29
15/03/29 06:52:51 INFO mapreduce.JobSubmitter: number of splits:29
15/03/29 06:52:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1427626074661_0001
15/03/29 06:52:52 INFO mapred.YARNRunner: Job jar is not present. Not adding any jar to the list of resources.
15/03/29 06:52:52 INFO impl.YarnClientImpl: Submitted application application_1427626074661_0001
15/03/29 06:52:52 INFO mapreduce.Job: The url to track the job: http://3791134b8c67:8088/proxy/application_1427626074661_0001/
15/03/29 06:52:52 INFO mapreduce.Job: Running job: job_1427626074661_0001
15/03/29 06:53:00 INFO mapreduce.Job: Job job_1427626074661_0001 running in uber mode : false
15/03/29 06:53:00 INFO mapreduce.Job:  map 0% reduce 0%
15/03/29 06:53:12 INFO mapreduce.Job:  map 7% reduce 0%
15/03/29 06:53:13 INFO mapreduce.Job:  map 21% reduce 0%
15/03/29 06:53:24 INFO mapreduce.Job:  map 31% reduce 0%
15/03/29 06:53:25 INFO mapreduce.Job:  map 34% reduce 0%
15/03/29 06:53:26 INFO mapreduce.Job:  map 41% reduce 0%
15/03/29 06:53:34 INFO mapreduce.Job:  map 45% reduce 0%
15/03/29 06:53:35 INFO mapreduce.Job:  map 48% reduce 0%
15/03/29 06:53:36 INFO mapreduce.Job:  map 52% reduce 0%
15/03/29 06:53:37 INFO mapreduce.Job:  map 59% reduce 0%
15/03/29 06:53:38 INFO mapreduce.Job:  map 59% reduce 20%
15/03/29 06:53:44 INFO mapreduce.Job:  map 62% reduce 20%
15/03/29 06:53:45 INFO mapreduce.Job:  map 66% reduce 20%
15/03/29 06:53:46 INFO mapreduce.Job:  map 76% reduce 20%
15/03/29 06:53:47 INFO mapreduce.Job:  map 76% reduce 25%
15/03/29 06:53:53 INFO mapreduce.Job:  map 79% reduce 25%
15/03/29 06:53:54 INFO mapreduce.Job:  map 86% reduce 25%
15/03/29 06:53:55 INFO mapreduce.Job:  map 93% reduce 25%
15/03/29 06:53:56 INFO mapreduce.Job:  map 93% reduce 31%
15/03/29 06:53:58 INFO mapreduce.Job:  map 97% reduce 31%
15/03/29 06:53:59 INFO mapreduce.Job:  map 100% reduce 32%
15/03/29 06:54:00 INFO mapreduce.Job:  map 100% reduce 100%
15/03/29 06:54:01 INFO mapreduce.Job: Job job_1427626074661_0001 completed successfully
15/03/29 06:54:01 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=345
		FILE: Number of bytes written=3173502
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=80305
		HDFS: Number of bytes written=437
		HDFS: Number of read operations=90
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=29
		Launched reduce tasks=1
		Data-local map tasks=29
		Total time spent by all maps in occupied slots (ms)=258456
		Total time spent by all reduces in occupied slots (ms)=35578
		Total time spent by all map tasks (ms)=258456
		Total time spent by all reduce tasks (ms)=35578
		Total vcore-seconds taken by all map tasks=258456
		Total vcore-seconds taken by all reduce tasks=35578
		Total megabyte-seconds taken by all map tasks=264658944
		Total megabyte-seconds taken by all reduce tasks=36431872
	Map-Reduce Framework
		Map input records=2062
		Map output records=24
		Map output bytes=590
		Map output materialized bytes=513
		Input split bytes=3476
		Combine input records=24
		Combine output records=13
		Reduce input groups=11
		Reduce shuffle bytes=513
		Reduce input records=13
		Reduce output records=11
		Spilled Records=26
		Shuffled Maps =29
		Failed Shuffles=0
		Merged Map outputs=29
		GC time elapsed (ms)=4724
		CPU time spent (ms)=15060
		Physical memory (bytes) snapshot=7882571776
		Virtual memory (bytes) snapshot=64778194944
		Total committed heap usage (bytes)=5993136128
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=76829
	File Output Format Counters 
		Bytes Written=437
15/03/29 06:54:01 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
15/03/29 06:54:01 WARN mapreduce.JobSubmitter: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
15/03/29 06:54:01 INFO input.FileInputFormat: Total input paths to process : 1
15/03/29 06:54:01 INFO mapreduce.JobSubmitter: number of splits:1
15/03/29 06:54:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1427626074661_0002
15/03/29 06:54:01 INFO mapred.YARNRunner: Job jar is not present. Not adding any jar to the list of resources.
15/03/29 06:54:01 INFO impl.YarnClientImpl: Submitted application application_1427626074661_0002
15/03/29 06:54:01 INFO mapreduce.Job: The url to track the job: http://3791134b8c67:8088/proxy/application_1427626074661_0002/
15/03/29 06:54:01 INFO mapreduce.Job: Running job: job_1427626074661_0002
15/03/29 06:54:11 INFO mapreduce.Job: Job job_1427626074661_0002 running in uber mode : false
15/03/29 06:54:11 INFO mapreduce.Job:  map 0% reduce 0%
15/03/29 06:54:16 INFO mapreduce.Job:  map 100% reduce 0%
15/03/29 06:54:20 INFO mapreduce.Job:  map 100% reduce 100%
15/03/29 06:54:21 INFO mapreduce.Job: Job job_1427626074661_0002 completed successfully
15/03/29 06:54:21 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=291
		FILE: Number of bytes written=211013
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=567
		HDFS: Number of bytes written=197
		HDFS: Number of read operations=7
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2290
		Total time spent by all reduces in occupied slots (ms)=2450
		Total time spent by all map tasks (ms)=2290
		Total time spent by all reduce tasks (ms)=2450
		Total vcore-seconds taken by all map tasks=2290
		Total vcore-seconds taken by all reduce tasks=2450
		Total megabyte-seconds taken by all map tasks=2344960
		Total megabyte-seconds taken by all reduce tasks=2508800
	Map-Reduce Framework
		Map input records=11
		Map output records=11
		Map output bytes=263
		Map output materialized bytes=291
		Input split bytes=130
		Combine input records=0
		Combine output records=0
		Reduce input groups=5
		Reduce shuffle bytes=291
		Reduce input records=11
		Reduce output records=11
		Spilled Records=22
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=95
		CPU time spent (ms)=1110
		Physical memory (bytes) snapshot=431980544
		Virtual memory (bytes) snapshot=4327272448
		Total committed heap usage (bytes)=348651520
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=437
	File Output Format Counters 
		Bytes Written=197

-bash-4.3# bin/hdfs dfs -cat output/*
6	dfs.audit.logger
4	dfs.class
3	dfs.server.namenode.
2	dfs.period
2	dfs.audit.log.maxfilesize
2	dfs.audit.log.maxbackupindex
1	dfsmetrics.log
1	dfsadmin
1	dfs.servers
1	dfs.replication
1	dfs.file

/ 13	. 

/ de volgorde om start/stop is dus	,

-bash-4.3# sbin/start-dfs.sh
-bash-4.3# sbin/start-yarn.sh
...
-bash-4.3# sbin/stop-yarn.sh
-bash-4.3# sbin/stop-dfs.sh

/ in meer detail	,

-bash-4.3# sbin/start-dfs.sh
Starting namenodes on [localhost]
localhost: starting namenode, logging to /opt/hadoop-2.6.0/logs/hadoop-root-namenode-3791134b8c67.out
localhost: starting datanode, logging to /opt/hadoop-2.6.0/logs/hadoop-root-datanode-3791134b8c67.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /opt/hadoop-2.6.0/logs/hadoop-root-secondarynamenode-3791134b8c67.out
-bash-4.3# sbin/start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /opt/hadoop-2.6.0/logs/yarn-root-resourcemanager-.out
localhost: starting nodemanager, logging to /opt/hadoop-2.6.0/logs/yarn-root-nodemanager-3791134b8c67.out
-bash-4.3# sbin/stop-yarn.sh
stopping yarn daemons
stopping resourcemanager
localhost: stopping nodemanager
no proxyserver to stop
-bash-4.3# sbin/stop-dfs.sh
Stopping namenodes on [localhost]
localhost: stopping namenode
localhost: stopping datanode
Stopping secondary namenodes [0.0.0.0]
0.0.0.0: stopping secondarynamenode

/ 13	.

/als we willen wisselen bij,
-bash-4.3# bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar grep input output 'dfs[a-z.]+'
/ tussen wel/zonder yarn	,

/ stel we hebben zojuist met yarn deze gedaan	,
...
-bash-4.3# sbin/stop-yarn.sh
-bash-4.3# vi etc/hadoop/mapred-site.xml
<configuration>
</configuration>
-bash-4.3#  bin/hdfs dfs -rm -r output
-bash-4.3# bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar grep input output 'dfs[a-z.]+'
-bash-4.3#  bin/hdfs dfs -rm -r output
-bash-4.3# vi etc/hadoop/mapred-site.xml
<configuration>
   <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
-bash-4.3# sbin/start-yarn.sh
-bash-4.3# bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar grep input output 'dfs[a-z.]+'
...

/ 13	. 

/ Kijk in /tmp/hadoop-root	,
...

/ Einde HADOOP TAR SINGLE CLUSTER 


/ HADOOP YUM SINGLE CLUSTER 

/ 7	. 

/ als we wisselen tussen hadoop-2.6.0 en yum's hadoop-2.4.1, rm -rf /tmp/hadoop-root	, 


/ 7	.

-bash-4.3# yum install hadoop-common hadoop-hdfs hadoop-mapreduce hadoop-mapreduce-examples hadoop-yarn hadoop-common-native	

-bash-4.3# yum list available "*hadoop*"
hadoop-client.noarch                                    2.4.1-6.fc21                      fedora 
hadoop-common-native.x86_64                             2.4.1-6.fc21                      fedora 
hadoop-devel.i686                                       2.4.1-6.fc21                      fedora 
hadoop-devel.x86_64                                     2.4.1-6.fc21                      fedora 
hadoop-hdfs-fuse.x86_64                                 2.4.1-6.fc21                      fedora 
hadoop-httpfs.noarch                                    2.4.1-6.fc21                      fedora 
hadoop-maven-plugin.noarch                              2.4.1-6.fc21                      fedora 
hadoop-tests.noarch                                     2.4.1-6.fc21                      fedora 
hadoop-yarn-security.x86_64                             2.4.1-6.fc21                      fedora 

/ we doen	,
-bash-4.3# yum -y install hadoop-common-native

/ we kunnen ook 	,
-bash-4.3# repoquery -R  hadoop-hdfs | grep hadoop
/=
-bash-4.3# repoquery --requires  hadoop-hdfs | grep hadoop
...

-bash-4.3# repoquery --requires --resolve  hadoop-hdfs | grep hadoop
/ kan ook nog	,

/ 7	. 

/ als we standalone willen, zorg dan dat core-site.xml leeg is	,
-bash-4.3# vi /etc/hadoop/core-site.xml 
<configuration>
</configuration>

-bash-4.3# pwd
/root
-bash-4.3# cp /etc/hadoop/* input/
-bash-4.3#  ls
anaconda-ks.cfg  input
-bash-4.3# hadoop jar /usr/share/java/hadoop/hadoop-mapreduce-examples.jar grep input output 'dfs[a-z.]+' 
-bash-4.3# cat output/*
4	dfs.class
4	dfs.audit.logger
3	dfs.server.namenode.
2	dfs.audit.log.maxbackupindex
2	dfs.period
2	dfs.audit.log.maxfilesize
2	dfs.replication
1	dfsmetrics.log
1	dfsadmin
1	dfs.servers
1	dfs.safemode.min.datanodes
1	dfs.safemode.extension
1	dfs.namenode.name.dir
1	dfs.namenode.checkpoint.dir
1	dfs.file
1	dfs.datanode.ipc.address
1	dfs.datanode.http.address
1	dfs.datanode.data.dir
1	dfs.datanode.address
1	dfs.http.address

/ 7	. 

/ Wat geeft yum aan config files	?

/ hebben ze al	,
-bash-4.3# vi /etc/hadoop/hadoop-env.sh 
# The java implementation to use.
export JAVA_HOME=/usr/lib/jvm/jre

-bash-4.3# ls /usr/lib/jvm/jre
bin  lib
-bash-4.3# ls /usr/lib/jvm/jre -l
lrwxrwxrwx. 1 root root 21 Mar 23 13:04 /usr/lib/jvm/jre -> /etc/alternatives/jre

-bash-4.3# ls /etc/alternatives/java_sdk
bin  include  jre  lib  tapset
-bash-4.3# ls /etc/alternatives/jre
bin  lib

/ jre ipv java_sdk kan ook	,

/ 13	. 

-bash-4.3# cp /etc/hadoop/core-site.xml /etc/hadoop/core-site.xml.bak
-bash-4.3# cat /etc/hadoop/core-site.xml 
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>fs.default.name</name>
    <value>hdfs://localhost:8020</value>
  </property>

  <!-- HTTPFS proxy user setting -->
  <property>
    <name>hadoop.proxyuser.tomcat.hosts</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.tomcat.groups</name>
    <value>*</value>
  </property>

</configuration>

-bash-4.3# cp /etc/hadoop/hdfs-site.xml /etc/hadoop/hdfs-site.xml.bak
-bash-4.3# cat /etc/hadoop/hdfs-site.xml
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <!-- Immediately exit safemode as soon as one DataNode checks in. 
       On a multi-node cluster, these configurations must be removed.  -->
  <property>
    <name>dfs.safemode.extension</name>
    <value>0</value>
  </property>
  <property>
     <name>dfs.safemode.min.datanodes</name>
     <value>1</value>
  </property>
  <property>
     <name>hadoop.tmp.dir</name>
     <value>/var/lib/hadoop-hdfs/${user.name}</value>
  </property>
  <property>
     <name>dfs.namenode.name.dir</name>
     <value>file:///var/lib/hadoop-hdfs/${user.name}/dfs/namenode</value>
  </property>
  <property>
     <name>dfs.namenode.checkpoint.dir</name>
     <value>file:///var/lib/hadoop-hdfs/${user.name}/dfs/secondarynamenode</value>
  </property>
  <property>
     <name>dfs.datanode.data.dir</name>
     <value>file:///var/lib/hadoop-hdfs/${user.name}/dfs/datanode</value>
  </property>
  <property>
      <name>dfs.http.address</name>
      <value>0.0.0.0:50070</value>
  </property>
  <property>
      <name>dfs.datanode.address</name>
      <value>0.0.0.0:50010</value>
  </property>
  <property>
      <name>dfs.datanode.http.address</name>
      <value>0.0.0.0:50075</value>
  </property>
  <property>
      <name>dfs.datanode.ipc.address</name>
      <value>0.0.0.0:50020</value>
  </property>
</configuration>

-bash-4.3# cp /etc/hadoop/mapred-site.xml /etc/hadoop/mapred-site.xml.bak
-bash-4.3# cat /etc/hadoop/mapred-site.xml
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>mapred.job.tracker</name>
    <value>localhost:8021</value>
  </property>

  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>

  <property>
    <description>To set the value of tmp directory for map and reduce tasks.</description>
    <name>mapreduce.task.tmp.dir</name>
    <value>/var/cache/hadoop-mapreduce/${user.name}/tasks</value>
  </property>

</configuration>

-bash-4.3# cp /etc/hadoop/yarn-site.xml /etc/hadoop/yarn-site.xml.bak
-bash-4.3# cat /etc/hadoop/yarn-site.xml
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>

<!--
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>
-->

  <property>
    <name>yarn.dispatcher.exit-on-error</name>
    <value>true</value>
  </property>

  <property>
    <description>List of directories to store localized files in.</description>
    <name>yarn.nodemanager.local-dirs</name>
    <value>/var/cache/hadoop-yarn/${user.name}/nm-local-dir</value>
  </property>

  <property>
    <description>Where to store container logs.</description>
    <name>yarn.nodemanager.log-dirs</name>
    <value>/var/log/hadoop-yarn/containers</value>
  </property>

<!--
  <property>
    <description>Where to aggregate logs to.</description>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>/var/log/hadoop-yarn/apps</value>
  </property>
-->

  <property>
    <description>Classpath for typical applications.</description>
     <name>yarn.application.classpath</name>
     <value>
        $HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/$HADOOP_COMMON_DIR/*,
        $HADOOP_COMMON_HOME/$HADOOP_COMMON_LIB_JARS_DIR/*,
        $HADOOP_HDFS_HOME/$HDFS_DIR/*,$HADOOP_HDFS_HOME/$HDFS_LIB_JARS_DIR/*,
        $HADOOP_MAPRED_HOME/$MAPRED_DIR/*,
        $HADOOP_MAPRED_HOME/$MAPRED_LIB_JARS_DIR/*,
        $HADOOP_YARN_HOME/$YARN_DIR/*,$HADOOP_YARN_HOME/$YARN_LIB_JARS_DIR/*
     </value>
  </property>
</configuration>

/ 7	. 

/ single cluster	,

/ 13	. 

/ als we willen wisselen tussen hadoop-2.6.0 en yum's 2.4.1, rm -rf /tmp/hadoop-root

# hadoop version
Hadoop 2.4.1


/ Lees	,
http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/SingleCluster.html
/ of	,
http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html

/ we edit	,
-bash-4.3# vi /etc/hadoop/core-site.xml
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

-bash-4.3# vi /etc/hadoop/hdfs-site.xml
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

-bash-4.3# vi /etc/hadoop/mapred-site.xml
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
</configuration>

/ dan	,

# rm /tmp/hadoop-root 
/ al, als we van 2.6.0  komen	,
-bash-4.3# hdfs namenode -format
Re-format filesystem in Storage Directory /tmp/hadoop-root/dfs/name ? (Y or N) 
...
/ OK

/ we zagen ook een keer	,
Re-format filesystem in Storage Directory /var/lib/hadoop-hdfs/root/dfs/namenode ? (Y or N) Y
/ TODO

-bash-4.3# which hdfs
/usr/bin/hdfs
-bash-4.3# which start-dfs.sh
/usr/sbin/start-dfs.sh

-bash-4.3# start-dfs.sh 
/ OK	, 
-bash-4.3# ps ajx --width 2000
/usr/lib/jvm/jre/bin/java ... org.apache.hadoop.hdfs.server.namenode.NameNode
/usr/lib/jvm/jre/bin/java ... org.apache.hadoop.hdfs.server.datanode.DataNode
/usr/lib/jvm/jre/bin/java ... org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode

-bash-4.3# hdfs dfs -mkdir -p /user/root
-bash-4.3# hdfs dfs -put /etc/hadoop/ input
-bash-4.3# hdfs dfs -ls input
Found 22 items
-rw-r--r--   1 root supergroup       3589 2015-03-29 15:35 input/capacity-scheduler.xml
-rw-r--r--   1 root supergroup       1335 2015-03-29 15:35 input/configuration.xsl
...

/ er is /usr/share/hadoop , en /usr/share/java/hadoop	,
-bash-4.3# pwd
/root
-bash-4.3# hadoop jar /usr/share/java/hadoop/hadoop-mapreduce-examples.jar grep input output 'dfs[a-z.]+'
/ OK
-bash-4.3# hdfs dfs -cat output/*
4	dfs.class
4	dfs.audit.logger
3	dfs.server.namenode.
2	dfs.audit.log.maxbackupindex
2	dfs.period
2	dfs.audit.log.maxfilesize
2	dfs.replication
1	dfsmetrics.log
1	dfsadmin
1	dfs.servers
1	dfs.safemode.min.datanodes
1	dfs.safemode.extension
1	dfs.namenode.name.dir
1	dfs.namenode.checkpoint.dir
1	dfs.file
1	dfs.datanode.ipc.address
1	dfs.datanode.http.address
1	dfs.datanode.data.dir
1	dfs.datanode.address
1	dfs.http.address

-bash-4.3# hdfs dfs -rm -r output

/ we kunnen start-dfs.sh gewoon aanlaten	, 
/ het is alleen zo dat we yarn willen doen, we mapred-site.xml en yarn-site.xml in orde moeten hebben	, en  start-yarn.sh moeten doen	, en daarna hadoop jar ...	,
/ willen we van yarn af, moeten we stop-yarn.sh en mapred-site.xml leeg maken, en dan kunnen we weer hadoop jar ...

/ 7	. 

/ met yarn	,

-bash-4.3# vi /etc/hadoop/mapred-site.xml
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

-bash-4.3# vi /etc/hadoop/yarn-site.xml
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>

-bash-4.3# start-yarn.sh
-bash-4.3# hdfs dfs -rm -r output
/ OK
/ Dit kan pas NA start-yarn.sh	, omdat mapred-site.xml zo staat	,

-bash-4.3# hadoop jar /usr/share/java/hadoop/hadoop-mapreduce-examples.jar grep input output 'dfs[a-z.]+'
-bash-4.3# hdfs dfs -cat output/*
4	dfs.class
4	dfs.audit.logger
3	dfs.server.namenode.
2	dfs.audit.log.maxbackupindex
2	dfs.period
2	dfs.audit.log.maxfilesize
2	dfs.replication
1	dfsmetrics.log
1	dfsadmin
1	dfs.servers
1	dfs.safemode.min.datanodes
1	dfs.safemode.extension
1	dfs.namenode.name.dir
1	dfs.namenode.checkpoint.dir
1	dfs.file
1	dfs.datanode.ipc.address
1	dfs.datanode.http.address
1	dfs.datanode.data.dir
1	dfs.datanode.address
1	dfs.http.address

-bash-4.3# stop-yarn.sh 
stopping yarn daemons
stopping resourcemanager
localhost: stopping nodemanager
localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9
no proxyserver to stop

-bash-4.3# stop-dfs.sh
Stopping namenodes on [localhost]
localhost: stopping namenode
localhost: stopping datanode
Stopping secondary namenodes [0.0.0.0]
0.0.0.0: stopping secondarynamenode



/ Einde HADOOP YUM SINGLE CLUSTER 

/ DOCKER HADOOP YUM SINGLE CLUSTER

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/hadoop/fedora
[eric@localhost fedora]$ cp /opt/hadoop-2.6.0/etc/hadoop/core-site.xml .
[eric@localhost fedora]$ cp /opt/hadoop-2.6.0/etc/hadoop/hdfs-site.xml .
[eric@localhost fedora]$ cp /opt/hadoop-2.6.0/etc/hadoop/mapred-site.xml .
[eric@localhost fedora]$ cp /opt/hadoop-2.6.0/etc/hadoop/yarn-site.xml .

[eric@localhost fedora]$ cat core-site.xml 
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

[eric@localhost fedora]$ cat hdfs-site.xml 
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

[eric@localhost fedora]$ cat mapred-site.xml 
<configuration>
<!--
   <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
-->
</configuration>

[eric@localhost fedora]$ cat yarn-site.xml 
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>

/ Hij staat nu dus NIET yarn-configured	,


[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/hadoop/fedora
$ vi Dockerfile

from local/fd-systemd-sshd-java
run yum -y install hadoop-common hadoop-common-native hadoop-hdfs hadoop-mapreduce hadoop-mapreduce-examples hadoop-yarn

#run export JAVA_HOME=/etc/alternatives/java_sdk;\
run yum -y update glibc-common
# run yum -y install bashdb

run yum clean all

add core-site.xml /etc/hadoop/
add hdfs-site.xml /etc/hadoop/
add mapred-site.xml /etc/hadoop/
add yarn-site.xml /etc/hadoop/

expose 50070 8088

cmd ["/usr/lib/systemd/systemd"]

[eric@localhost fedora]$ sudo docker build --rm -t local/fd-systemd-sshd-java-hadoop-single-cluster .
/ OK
/ Maar leidt tot 363 packages import	, en r2.4.1	,
/ TODO

eric@localhost fedora]$ sudo docker run --rm --name hadoop --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -P local/fd-systemd-sshd-java-hadoop-single-cluster

/ in ander window	,
[eric@localhost Docker]$ sudo docker exec -it hadoop /usr/bin/bash
/ of	,
[root@localhost ~]# docker port hadoop
22/tcp -> 0.0.0.0:49160
50070/tcp -> 0.0.0.0:49161
8088/tcp -> 0.0.0.0:49162
[root@localhost ~]# ssh localhost -p 49160


/ we hebben op dit moment dus GEEN yarn	,

-bash-4.3# hdfs namenode -format
/ OK
-bash-4.3# start-dfs.sh

Starting namenodes on [localhost]
The authenticity of host 'localhost (::1)' can't be established.
ECDSA key fingerprint is e5:8f:51:31:35:22:88:e6:cc:e3:ad:ef:de:1c:49:06.
Are you sure you want to continue connecting (yes/no)? yes
localhost: Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.

Starting secondary namenodes [0.0.0.0]
The authenticity of host '0.0.0.0 (0.0.0.0)' can't be established.
ECDSA key fingerprint is e5:8f:51:31:35:22:88:e6:cc:e3:ad:ef:de:1c:49:06.
Are you sure you want to continue connecting (yes/no)? yes
0.0.0.0: Warning: Permanently added '0.0.0.0' (ECDSA) to the list of known hosts.

/ OK

http://localhost:49161/dfshealth.html#tab-overview
/ OK

-bash-4.3#  hdfs dfs -mkdir -p /user/root

-bash-4.3# hdfs dfs -put /etc/hadoop/ input

-bash-4.3# hdfs dfs -ls /user/root
drwxr-xr-x   - root supergroup          0 2015-03-30 16:28 /user/root/input

-bash-4.3# hadoop jar /usr/share/java/hadoop/hadoop-mapreduce-examples.jar grep input output 'dfs[a-z.]+'
/ OK

/ 7	. 

/ Nu met yarn	,

/ we hebben namenode, datanode	, secondnamenode run	,

$ vi /etc/hadoop/mapred-site.xml

<configuration>
   <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

/ Deze WAS	,

<configuration>
</configuration>

$ vi /etc/hadoop/mapred-site.xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
/ onveranderd	, het is de mapred-site.xml die bepaalt of yarn of niet	,

-bash-4.3# start-yarn.sh 
/ we zien nu ook resourcemanager, nodemanager	,

-bash-4.3# hdfs dfs -rm -r output
-bash-4.3# hadoop jar /usr/share/java/hadoop/hadoop-mapreduce-examples.jar grep input output 'dfs[a-z.]+'
-bash-4.3# hdfs dfs -cat output/*
4	dfs.class
4	dfs.audit.logger
3	dfs.server.namenode.
2	dfs.period
2	dfs.audit.log.maxfilesize
2	dfs.audit.log.maxbackupindex
1	dfsmetrics.log
1	dfsadmin
1	dfs.servers
1	dfs.replication
1	dfs.file

-bash-4.3# hdfs dfs -rm -r output
-bash-4.3# stop-yarn.sh 
stopping yarn daemons
stopping resourcemanager
localhost: stopping nodemanager
localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9
no proxyserver to stop

-bash-4.3# stop-dfs.sh 
Stopping namenodes on [localhost]
localhost: stopping namenode
localhost: stopping datanode
Stopping secondary namenodes [0.0.0.0]
0.0.0.0: stopping secondarynamenode



/ Einde DOCKER HADOOP YUM SINGLE CLUSTER

/ DOCKER YUM HADOOP CLUSTER

/ 7	. 

/ Eerst met de hand	,

/ master	,
[eric@localhost fedora]$ sudo docker run --rm --name hadoop --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -P local/fd-systemd-sshd-java-hadoop-single-cluster
[eric@localhost fedora]$ sudo docker exec -it hadoop /usr/bin/bash


/ slave 1
[eric@localhost fedora]$ sudo docker run --rm --name hadoop1 --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -P local/fd-systemd-sshd-java-hadoop-single-cluster
[eric@localhost fedora]$ sudo docker exec -it hadoop1 /usr/bin/bash


/ slave 2
[eric@localhost fedora]$ sudo docker run --rm --name hadoop2 --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -P local/fd-systemd-sshd-java-hadoop-single-cluster
[eric@localhost fedora]$ sudo docker exec -it hadoop2 /usr/bin/bash

/ we doen op master, slave1, slave2,

bash-4.3# cat /etc/hadoop/core-site.xml 
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://d619c3294a5e:9000</value>
    </property>
</configuration>

bash-4.3# cat /etc/hadoop/hdfs-site.xml 
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
   <property>
     <name>dfs.permissions</name>
     <value>false</value>
   </property>
</configuration>

bash-4.3# cat /etc/hadoop/mapred-site.xml
<configuration>
   <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

bash-4.3# cat /etc/hadoop/yarn-site.xml 
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
  <property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>d619c3294a5e:8025</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>d619c3294a5e:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>d619c3294a5e:8040</value>
  </property>
</configuration>

 /master
bash-4.3# cat /etc/hosts
172.17.0.133	d619c3294a5e
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters

172.17.0.134    a24b25b8d915
172.17.0.136    313a40d67b89

/ slave1
bash-4.3# cat /etc/hosts
172.17.0.134	a24b25b8d915
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters

172.17.0.133    d619c3294a5e
172.17.0.136    313a40d67b89

/ slave2
bash-4.3# cat /etc/hosts
172.17.0.136	313a40d67b89
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters

172.17.0.133    d619c3294a5e
172.17.0.134    a24b25b8d915

/ Dan iedere machine moeten we ssh naar de andere, voor known_hosts	, yes/no	, 
/ TODO (option )

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/hadoop/fedora
$ vi Dockerfile

from local/fd-systemd-sshd-java

run yum -y install hadoop-common hadoop-common-native hadoop-hdfs hadoop-mapreduce hadoop-mapreduce-examples hadoop-yarn 

run yum -y update glibc-common

run yum clean all

add core-site.xml /etc/hadoop/
add hdfs-site.xml /etc/hadoop/
add mapred-site.xml /etc/hadoop/
add yarn-site.xml /etc/hadoop/

expose 50070 8088

cmd ["/usr/lib/systemd/systemd"]

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/sshd/fedora
[eric@localhost fedora]$ vi Dockerfile 

from local/fd-systemd

run yum -y install openssh-server openssh-clients
run yum clean all
run systemctl enable sshd.service

run awk -i inplace '{print gensub("(#)(PermitRootLogin yes)","\\2","g")}' /etc/ssh/sshd_config

run ssh-keygen -A

run ssh-keygen -N "" -t rsa -f ~/.ssh/id_rsa
run cat /root/.ssh/id_rsa.pub >>~/.ssh/authorized_keys

expose 22

cmd ["/usr/lib/systemd/systemd"]

/ Door	,
run ssh-keygen -N "" -t rsa -f ~/.ssh/id_rsa
run cat /root/.ssh/id_rsa.pub >>~/.ssh/authorized_keys
/ kun je vanuit  master, of een slave password less in log naar een andere	,
/ Alleen known_hosts probleem nog	,
/ TODO

/ Lees	,
http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/ClusterSetup.html

/ op master	,
bash-4.3# hadoop-daemon.sh  start namenode

/ op slave1, slave2
bash-4.3#  hadoop-daemon.sh start datanode

/ op master	,
bash-4.3# yarn-daemon.sh start resourcemanager

/ op slave1, slave2
bash-4.3# yarn-daemon.sh start nodemanager

/ master
bash-4.3# ps ajx --width 4096
    0 20368 20347     0 ?        20940 Sl       0   0:28 /usr/lib/jvm/jre/bin/java -Dproc_namenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr -Dhadoop.id.str= -Dhadoop.root.logger=INFO,console -Djava.library.path=/usr/lib64/hadoop -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop--namenode-d619c3294a5e.log -Dhadoop.home.dir=/usr -Dhadoop.id.str= -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/usr/lib64/hadoop -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.NameNode
    0 20466 20444     0 ?        20940 Sl       0   1:19 /usr/lib/jvm/jre/bin/java -Dproc_resourcemanager -Xmx1000m -Dhadoop.log.dir=/var/log/hadoop-yarn -Dyarn.log.dir=/var/log/hadoop-yarn -Dhadoop.log.file=yarn.log -Dyarn.log.file=yarn.log -Dyarn.home.dir= -Dyarn.id.str=yarn -Dhadoop.root.logger=INFO,console -Dyarn.root.logger=INFO,console -Djava.library.path=/usr/lib64/hadoop -Dyarn.policy.file=hadoop-policy.xml -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Dhadoop.log.dir=/var/log/hadoop-yarn -Dyarn.log.dir=/var/log/hadoop-yarn -Dhadoop.log.file=yarn-yarn-resourcemanager-d619c3294a5e.log -Dyarn.log.file=yarn-yarn-resourcemanager-d619c3294a5e.log -Dyarn.home.dir= -Dyarn.id.str=yarn -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/usr/lib64/hadoop -Dyarn.policy.file=hadoop-policy.xml -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Dhadoop.log.dir=/var/log/hadoop-yarn -Dyarn.log.dir=/var/log/hadoop-yarn -Dhadoop.log.file=yarn-yarn-resourcemanager-d619c3294a5e.log -Dyarn.log.file=yarn-yarn-resourcemanager-d619c3294a5e.log -Dyarn.home.dir=/usr -Dhadoop.home.dir=/usr -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/usr/lib64/hadoop -classpath /etc/hadoop:/etc/hadoop:/etc/hadoop:/usr/share/hadoop/common/lib/*:/usr/share/hadoop/common/*:/usr/share/hadoop/hdfs:/usr/share/hadoop/hdfs/*:/usr/share/hadoop/yarn/lib/*:/usr/share/hadoop/yarn/*:/usr/share/hadoop/mapreduce/lib/*:/usr/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/usr/share/hadoop/yarn/*:/usr/share/hadoop/yarn/lib/*:/etc/hadoop/rm-config/log4j.properties org.apache.hadoop.yarn.server.resourcemanager.ResourceManager

/ op slave1, slave2
bash-4.3# ps ajx --width 4096
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0  1722  1705     0 ?         3194 Sl       0   0:25 /usr/lib/jvm/jre/bin/java -Dproc_datanode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr -Dhadoop.id.str= -Dhadoop.root.logger=INFO,console -Djava.library.path=/usr/lib64/hadoop -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop--datanode-313a40d67b89.log -Dhadoop.home.dir=/usr -Dhadoop.id.str= -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/usr/lib64/hadoop -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -server -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode
    0  1823  1801     0 ?         3194 Sl       0   0:51 /usr/lib/jvm/jre/bin/java -Dproc_nodemanager -Xmx1000m -Dhadoop.log.dir=/var/log/hadoop-yarn -Dyarn.log.dir=/var/log/hadoop-yarn -Dhadoop.log.file=yarn.log -Dyarn.log.file=yarn.log -Dyarn.home.dir= -Dyarn.id.str=yarn -Dhadoop.root.logger=INFO,console -Dyarn.root.logger=INFO,console -Djava.library.path=/usr/lib64/hadoop -Dyarn.policy.file=hadoop-policy.xml -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Dhadoop.log.dir=/var/log/hadoop-yarn -Dyarn.log.dir=/var/log/hadoop-yarn -Dhadoop.log.file=yarn-yarn-nodemanager-313a40d67b89.log -Dyarn.log.file=yarn-yarn-nodemanager-313a40d67b89.log -Dyarn.home.dir= -Dyarn.id.str=yarn -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/usr/lib64/hadoop -Dyarn.policy.file=hadoop-policy.xml -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -server -Dhadoop.log.dir=/var/log/hadoop-yarn -Dyarn.log.dir=/var/log/hadoop-yarn -Dhadoop.log.file=yarn-yarn-nodemanager-313a40d67b89.log -Dyarn.log.file=yarn-yarn-nodemanager-313a40d67b89.log -Dyarn.home.dir=/usr -Dhadoop.home.dir=/usr -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/usr/lib64/hadoop -classpath /etc/hadoop:/etc/hadoop:/etc/hadoop:/usr/share/hadoop/common/lib/*:/usr/share/hadoop/common/*:/usr/share/hadoop/hdfs:/usr/share/hadoop/hdfs/*:/usr/share/hadoop/yarn/lib/*:/usr/share/hadoop/yarn/*:/usr/share/hadoop/mapreduce/lib/*:/usr/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/usr/share/hadoop/yarn/*:/usr/share/hadoop/yarn/lib/*:/etc/hadoop/nm-config/log4j.properties org.apache.hadoop.yarn.server.nodemanager.NodeManager
    0  2025  2009  2009 ?           -1 Z        0   0:06 [java] <defunct>
    0  2032  2010  2010 ?           -1 Z        0   0:06 [java] <defunct>
    0  2039  2015  2015 ?           -1 Z        0   0:06 [java] <defunct>
    0  2040  2018  2018 ?           -1 Z        0   0:06 [java] <defunct>
    0  2043  2011  2011 ?           -1 Z        0   0:06 [java] <defunct>
    0  2046  2013  2013 ?           -1 Z        0   0:06 [java] <defunct>
    0  2048  2026  2026 ?           -1 Z        0   0:06 [java] <defunct>
    0  2074  2024  2024 ?           -1 Z        0   0:06 [java] <defunct>
    0  2605  2600  2600 ?           -1 Z        0   0:06 [java] <defunct>
    0  2619  2614  2614 ?           -1 Z        0   0:06 [java] <defunct>
    0  2631  2621  2621 ?           -1 Z        0   0:05 [java] <defunct>
    0  2643  2620  2620 ?           -1 Z        0   0:06 [java] <defunct>
    0  2649  2622  2622 ?           -1 Z        0   0:06 [java] <defunct>
    0  2667  2640  2640 ?           -1 Z        0   0:06 [java] <defunct>
    0  2899  2891  2891 ?           -1 Z        0   0:06 [java] <defunct>
    0  2905  2892  2892 ?           -1 Z        0   0:06 [java] <defunct>
    0  2913  2900  2900 ?           -1 Z        0   0:06 [java] <defunct>
    0  2964  2960  2960 ?           -1 Z        0   0:05 [java] <defunct>
    0  2983  2979  2979 ?           -1 Z        0   0:06 [java] <defunct>

/ op master doen we alle commands	,

bash-4.3# hdfs dfs -mkdir -p /user/root
bash-4.3# hdfs dfs -put /etc/hadoop/ input
bash-4.3#  hadoop jar /usr/share/java/hadoop/hadoop-mapreduce-examples.jar grep input output 'dfs[a-z.]+'
bash-4.3# hdfs dfs -cat output/*



/ 7	. 

/ evt	. 

/ In beide containers,
-bash-4.3# yum -y install net-tools
-bash-4.3# yum -y install nmap
-bash-4.3# yum -y install hostname 

-bash-4.3# hostnamectl
   Static hostname: 1909545f9976
         Icon name: computer-container
           Chassis: container
        Machine ID: b29d8dedf20e4407870c8198ce58e377
           Boot ID: 149de8c2463a448ba86abfd2ae249e07
    Virtualization: docker
  Operating System: Fedora 21 (Twenty One)
       CPE OS Name: cpe:/o:fedoraproject:fedora:21
            Kernel: Linux 3.18.3-201.fc21.x86_64
      Architecture: x86-64

/ Einde DOCKER YUM HADOOP CLUSTER


/ DEBUG HADOOP TAR SINGLE CLUSTER

/ Lees	,
http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html
 
/ 7	.

[eric@localhost centos7_2]$ sudo su -
[root@localhost ~]# docker port java
22/tcp -> 0.0.0.0:49226
[root@localhost ~]# ssh localhost -p 49226
-bash-4.3# ls /opt
hadoop-2.6.0
-bash-4.3# export JAVA_HOME=/etc/alternatives/java_sdk

-bash-4.3# yum search locale
Failed to set locale, defaulting to C
/ RM deze msg	,
glibc-common.x86_64 : Common binaries and locale data for glibc
-bash-4.3#  yum -y update glibc-common
Failed to set locale, defaulting to C
---> Package glibc-common.x86_64 0:2.20-5.fc21 will be updated
---> Package glibc-common.x86_64 0:2.20-8.fc21 will be an update
---> Package glibc.x86_64 0:2.20-5.fc21 will be updated
---> Package glibc.x86_64 0:2.20-8.fc21 will be an update
-bash-4.3# yum -y install bashdb
/ de msg over locale is weg	,

/ Intermezzo

-bash-4.3# locale
LANG=en_US.UTF-8
LC_CTYPE="en_US.UTF-8"
LC_NUMERIC="en_US.UTF-8"
LC_TIME="en_US.UTF-8"
LC_COLLATE="en_US.UTF-8"
LC_MONETARY="en_US.UTF-8"
LC_MESSAGES="en_US.UTF-8"
LC_PAPER="en_US.UTF-8"
LC_NAME="en_US.UTF-8"
LC_ADDRESS="en_US.UTF-8"
LC_TELEPHONE="en_US.UTF-8"
LC_MEASUREMENT="en_US.UTF-8"
LC_IDENTIFICATION="en_US.UTF-8"
LC_ALL=

/we kunnen ook	,

-bash-4.3# yum grouplist
-bash-4.3# yum -y groupinstall "C Development Tools and Libraries"
...

/ Einde Intermezzo


/ 7	.

/ Lees	,
http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html

bash-4.3# bash -debugger bin/hdfs namenode -format 
bashdb<0> pr $0
bin/hdfs
bin=`which $0`
/opt/hadoop-2.6.0/bin/hdfs
bin=`dirname ${bin}`
/opt/hadoop-2.6.0/bin
bin=`cd "$bin" > /dev/null; pwd`
DEFAULT_LIBEXEC_DIR="$bin"/../libexec
HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
bashdb<9> pr $HADOOP_LIBEXEC_DIR
/opt/hadoop-2.6.0/bin/../libexec
. $HADOOP_LIBEXEC_DIR/hdfs-config.sh

bashdb<10> s
(/opt/hadoop-2.6.0/libexec/hdfs-config.sh:21):
21:	bin=`which "$0"`
bashdb<11> pr $0
bin/hdfs
if [ -e "${HADOOP_LIBEXEC_DIR}/hadoop-config.sh" ]; then
  . ${HADOOP_LIBEXEC_DIR}/hadoop-config.sh

bashdb<19> s
(/opt/hadoop-2.6.0/libexec/hadoop-config.sh:50):
50:	this="${BASH_SOURCE-$0}"
bashdb<21> pr $this
/opt/hadoop-2.6.0/bin/../libexec/hadoop-config.sh
bashdb<22> pr $0
bin/hdfs
common_bin=$(cd -P -- "$(dirname -- "$this")" && pwd -P)
/opt/hadoop-2.6.0/libexec
/ -P is van physical	,

/ Intermezzo

/ als we yum -y install bashdb, dan is /usr/share/bashdb een dir	, maar als we bashdb zelf install	, is /usr/share/share/bashdb  de dir, en maken we een soft link /usr/share/bashdb naar deze	, 
/ als we daar zijn	,
bash-4.3# pwd
/usr/share/bashdb
bash-4.3# pwd -P
/usr/share/share/bashdb

/ Einde Intermezzo

script="$(basename -- "$this")"
this="$common_bin/$script"
/opt/hadoop-2.6.0/libexec/hadoop-config.sh

[ -f "$common_bin/hadoop-layout.sh" ] && . "$common_bin/hadoop-layout.sh"
/ NEE
HADOOP_COMMON_DIR=${HADOOP_COMMON_DIR:-"share/hadoop/common"}
share/hadoop/common
HADOOP_COMMON_LIB_JARS_DIR=${HADOOP_COMMON_LIB_JARS_DIR:-"share/hadoop/common/lib"}
share/hadoop/common/lib
HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_COMMON_LIB_NATIVE_DIR:-"lib/native"}
lib/native
HDFS_DIR=${HDFS_DIR:-"share/hadoop/hdfs"}
share/hadoop/hdfs
HDFS_LIB_JARS_DIR=${HDFS_LIB_JARS_DIR:-"share/hadoop/hdfs/lib"}
share/hadoop/hdfs/lib
YARN_DIR=${YARN_DIR:-"share/hadoop/yarn"}
share/hadoop/yarn
YARN_LIB_JARS_DIR=${YARN_LIB_JARS_DIR:-"share/hadoop/yarn/lib"}
share/hadoop/yarn/lib
MAPRED_DIR=${MAPRED_DIR:-"share/hadoop/mapreduce"}
share/hadoop/mapreduce
MAPRED_LIB_JARS_DIR=${MAPRED_LIB_JARS_DIR:-"share/hadoop/mapreduce/lib"}
share/hadoop/mapreduce/lib

HADOOP_DEFAULT_PREFIX=$(cd -P -- "$common_bin"/.. && pwd -P)
/opt/hadoop-2.6.0
HADOOP_PREFIX=${HADOOP_PREFIX:-$HADOOP_DEFAULT_PREFIX}
/opt/hadoop-2.6.0
export HADOOP_PREFIX

# Allow alternate conf dir location.
if [ -e "${HADOOP_PREFIX}/conf/hadoop-env.sh" ]; then
/ NEE
else
  DEFAULT_CONF_DIR="etc/hadoop"
export HADOOP_CONF_DIR="${HADOOP_CONF_DIR:-$HADOOP_PREFIX/$DEFAULT_CONF_DIR}"
/opt/hadoop-2.6.0/etc/hadoop

if [ -f "${HADOOP_CONF_DIR}/hadoop-env.sh" ]; then
  . "${HADOOP_CONF_DIR}/hadoop-env.sh"

bashdb<70> s
(/opt/hadoop-2.6.0/etc/hadoop/hadoop-env.sh:25):
bashdb<71> pr $0
bin/hdfs
/ We zijn  dus nog steeds in dit script, en de exports gelden dus in dit script en daaronder	,
25:	export JAVA_HOME=${JAVA_HOME}
33:	export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/etc/hadoop"}
/opt/hadoop-2.6.0/etc/hadoop

for f in $HADOOP_HOME/contrib/capacity-scheduler/*.jar; do
/contrib/capacity-scheduler/*.jar
  if [ "$HADOOP_CLASSPATH" ]; then
/NEE
  else
    export HADOOP_CLASSPATH=$f

# Extra Java runtime options.  Empty by default.
49:	export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"
-Djava.net.preferIPv4Stack=true

# Command specific options appended to HADOOP_OPTS when specified
export HADOOP_NAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_NAMENODE_OPTS"
-Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender
export HADOOP_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS $HADOOP_DATANODE_OPTS"
-Dhadoop.security.logger=ERROR,RFAS

export HADOOP_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_SECONDARYNAMENODE_OPTS"
-Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender

export HADOOP_NFS3_OPTS="$HADOOP_NFS3_OPTS"
""
export HADOOP_PORTMAP_OPTS="-Xmx512m $HADOOP_PORTMAP_OPTS"
-Xmx512m

# The following applies to multiple commands (fs, dfs, fsck, distcp etc)
export HADOOP_CLIENT_OPTS="-Xmx512m $HADOOP_CLIENT_OPTS"
-Xmx512m

# This **MUST** be uncommented to enable secure HDFS if using privileged ports
# to provide authentication of data transfer protocol.  This **MUST NOT** be
# defined if SASL is configured for authentication of data transfer protocol
# using non-privileged ports.
export HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER}
""

# Where log files are stored in the secure data environment.
export HADOOP_SECURE_DN_LOG_DIR=${HADOOP_LOG_DIR}/${HADOOP_HDFS_USER}
/

export HADOOP_PID_DIR=${HADOOP_PID_DIR}
""
export HADOOP_SECURE_DN_PID_DIR=${HADOOP_PID_DIR}
""

# A string representing this instance of hadoop. $USER by default.
export HADOOP_IDENT_STRING=$USER
root

/t
(/opt/hadoop-2.6.0/libexec/hadoop-config.sh:137):
  . "${HADOOP_CONF_DIR}/hadoop-env.sh"
/d
137:	bindv6only=$(/sbin/sysctl -n net.ipv6.bindv6only 2> /dev/null)
0
148:	export MALLOC_ARENA_MAX=${MALLOC_ARENA_MAX:-4}
4

JAVA=$JAVA_HOME/bin/java
/etc/alternatives/java_sdk/bin/java
# some Java parameters
JAVA_HEAP_MAX=-Xmx1000m

/ Intermezzo

[eric@localhost hadoop-2.6.0]$ ls /etc/alternatives/java_sdk -l
lrwxrwxrwx. 1 root root 58 Mar  4 20:07 /etc/alternatives/java_sdk -> /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-5.b13.fc21.x86_64
[eric@localhost hadoop-2.6.0]$ ls /etc/alternatives/java -l
lrwxrwxrwx. 1 root root 71 Mar  4 20:07 /etc/alternatives/java -> /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-5.b13.fc21.x86_64/jre/bin/java
[eric@localhost hadoop-2.6.0]$ ls /etc/alternatives/javac -l
lrwxrwxrwx. 1 root root 68 Mar  4 20:07 /etc/alternatives/javac -> /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-5.b13.fc21.x86_64/bin/javac

/ Einde Intermezzo

180:	CLASSPATH="${HADOOP_CONF_DIR}"
/opt/hadoop-2.6.0/etc/hadoop

if [ "$HADOOP_COMMON_HOME" = "" ]; then
  if [ -d "${HADOOP_PREFIX}/$HADOOP_COMMON_DIR" ]; then
    export HADOOP_COMMON_HOME=$HADOOP_PREFIX
/opt/hadoop-2.6.0

if [ -d "$HADOOP_COMMON_HOME/$HADOOP_COMMON_LIB_JARS_DIR" ]; then
  CLASSPATH=${CLASSPATH}:$HADOOP_COMMON_HOME/$HADOOP_COMMON_LIB_JARS_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*

CLASSPATH=${CLASSPATH}:$HADOOP_COMMON_HOME/$HADOOP_COMMON_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*

# default log directory & file
if [ "$HADOOP_LOG_DIR" = "" ]; then
  HADOOP_LOG_DIR="$HADOOP_PREFIX/logs"
/opt/hadoop-2.6.0/logs

if [ "$HADOOP_LOGFILE" = "" ]; then
  HADOOP_LOGFILE='hadoop.log'
hadoop.log

# default policy file for service-level authorization
if [ "$HADOOP_POLICYFILE" = "" ]; then
  HADOOP_POLICYFILE="hadoop-policy.xml"

if [ ... -o -d "${HADOOP_PREFIX}/$HADOOP_COMMON_LIB_NATIVE_DIR" ]; then
/ JA
/opt/hadoop-2.6.0/lib/native

  if [ -d "${HADOOP_PREFIX}/$HADOOP_COMMON_LIB_NATIVE_DIR" ]; then
    if [ "x$JAVA_LIBRARY_PATH" != "x" ]; then
/ NEE
    else
      JAVA_LIBRARY_PATH=${HADOOP_PREFIX}/$HADOOP_COMMON_LIB_NATIVE_DIR
/opt/hadoop-2.6.0/lib/native

# setup a default TOOL_PATH
TOOL_PATH="${TOOL_PATH:-$HADOOP_PREFIX/share/hadoop/tools/lib/*}"
/opt/hadoop-2.6.0/share/hadoop/tools/lib/activation-1.1.jar /opt/hadoop-2.6.0/share/hadoop/tools/lib/apacheds-i18n-2.0.0-M15.jar /op...


/ Intermezzo

/ Let op verschil	,
bashdb<158> pr $HADOOP_PREFIX/share/hadoop/tools/lib'/*'
/opt/hadoop-2.6.0/share/hadoop/tools/lib/*
bashdb<157> pr $HADOOP_PREFIX/share/hadoop/tools/lib/*
/opt/hadoop-2.6.0/share/hadoop/tools/lib/activation-1.1.jar /opt/hadoop-2.6.0/share/hadoop/tools/lib/apacheds-i18n-2.0.0-M15.jar /o...

[eric@localhost hadoop-2.6.0]$ diff share/hadoop/common/lib/ share/hadoop/tools/lib/
Only in share/hadoop/tools/lib/: aws-java-sdk-1.7.4.jar
Only in share/hadoop/common/lib/: hadoop-annotations-2.6.0.jar
Only in share/hadoop/tools/lib/: hadoop-ant-2.6.0.jar
Only in share/hadoop/tools/lib/: hadoop-archives-2.6.0.jar
Only in share/hadoop/tools/lib/: hadoop-aws-2.6.0.jar
Only in share/hadoop/tools/lib/: hadoop-datajoin-2.6.0.jar
Only in share/hadoop/tools/lib/: hadoop-distcp-2.6.0.jar
Only in share/hadoop/tools/lib/: hadoop-extras-2.6.0.jar
Only in share/hadoop/tools/lib/: hadoop-gridmix-2.6.0.jar
Only in share/hadoop/tools/lib/: hadoop-openstack-2.6.0.jar
Only in share/hadoop/tools/lib/: hadoop-rumen-2.6.0.jar
Only in share/hadoop/tools/lib/: hadoop-sls-2.6.0.jar
Only in share/hadoop/tools/lib/: hadoop-streaming-2.6.0.jar
Only in share/hadoop/tools/lib/: jackson-annotations-2.2.3.jar
Only in share/hadoop/tools/lib/: jackson-core-2.2.3.jar
Only in share/hadoop/tools/lib/: jackson-databind-2.2.3.jar
Only in share/hadoop/tools/lib/: joda-time-2.5.jar
Only in share/hadoop/tools/lib/: metrics-core-3.0.1.jar
Only in share/hadoop/common/lib/: slf4j-api-1.7.5.jar
Only in share/hadoop/common/lib/: slf4j-log4j12-1.7.5.jar

/ Einde Intermezzo

HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.log.dir=$HADOOP_LOG_DIR"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs

HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.log.file=$HADOOP_LOGFILE"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log

HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.home.dir=$HADOOP_PREFIX"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0

HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.id.str=$HADOOP_IDENT_STRING"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root

HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.root.logger=${HADOOP_ROOT_LOGGER:-INFO,console}"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console

if [ "x$JAVA_LIBRARY_PATH" != "x" ]; then
/opt/hadoop-2.6.0/lib/native
  HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native
  export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$JAVA_LIBRARY_PATH
:/opt/hadoop-2.6.0/lib/native

HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.policy.file=$HADOOP_POLICYFILE"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml

# put hdfs in classpath if present
if [ "$HADOOP_HDFS_HOME" = "" ]; then
  if [ -d "${HADOOP_PREFIX}/$HDFS_DIR" ]; then
/opt/hadoop-2.6.0/share/hadoop/hdfs
    export HADOOP_HDFS_HOME=$HADOOP_PREFIX
/opt/hadoop-2.6.0

if [ -d "$HADOOP_HDFS_HOME/$HDFS_DIR/webapps" ]; then
  CLASSPATH=${CLASSPATH}:$HADOOP_HDFS_HOME/$HDFS_DIR
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs

if [ -d "$HADOOP_HDFS_HOME/$HDFS_LIB_JARS_DIR" ]; then
  CLASSPATH=${CLASSPATH}:$HADOOP_HDFS_HOME/$HDFS_LIB_JARS_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*

CLASSPATH=${CLASSPATH}:$HADOOP_HDFS_HOME/$HDFS_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*

# put yarn in classpath if present
if [ "$HADOOP_YARN_HOME" = "" ]; then
  if [ -d "${HADOOP_PREFIX}/$YARN_DIR" ]; then
    export HADOOP_YARN_HOME=$HADOOP_PREFIX
/opt/hadoop-2.6.0

if [ -d "$HADOOP_YARN_HOME/$YARN_LIB_JARS_DIR" ]; then
  CLASSPATH=${CLASSPATH}:$HADOOP_YARN_HOME/$YARN_LIB_JARS_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/*

280:	CLASSPATH=${CLASSPATH}:$HADOOP_YARN_HOME/$YARN_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/*:/opt/hadoop-2.6.0/share/hadoop/yarn/*

# put mapred in classpath if present AND different from YARN
if [ "$HADOOP_MAPRED_HOME" = "" ]; then
  if [ -d "${HADOOP_PREFIX}/$MAPRED_DIR" ]; then
/opt/hadoop-2.6.0/share/hadoop/mapreduce
    export HADOOP_MAPRED_HOME=$HADOOP_PREFIX
/opt/hadoop-2.6.0

if [ "$HADOOP_MAPRED_HOME/$MAPRED_DIR" != "$HADOOP_YARN_HOME/$YARN_DIR" ] ; then
  if [ -d "$HADOOP_MAPRED_HOME/$MAPRED_DIR/webapps" ]; then
/ NEE

  if [ -d "$HADOOP_MAPRED_HOME/$MAPRED_LIB_JARS_DIR" ]; then
    CLASSPATH=${CLASSPATH}:$HADOOP_MAPRED_HOME/$MAPRED_LIB_JARS_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/*:/opt/hadoop-2.6.0/share/hadoop/yarn/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/*

  CLASSPATH=${CLASSPATH}:$HADOOP_MAPRED_HOME/$MAPRED_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/*:/opt/hadoop-2.6.0/share/hadoop/yarn/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/*

if [[ ( "$HADOOP_CLASSPATH" != "" ) && ( "$HADOOP_USE_CLIENT_CLASSLOADER" = "" ) ]]; then
/contrib/capacity-scheduler/*.jar
  # Prefix it if its to be preceded
  if [ "$HADOOP_USER_CLASSPATH_FIRST" != "" ]; then
/ NEE
  else
    CLASSPATH=${CLASSPATH}:${HADOOP_CLASSPATH}
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/*:/opt/hadoop-2.6.0/share/hadoop/yarn/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar

/t
(/opt/hadoop-2.6.0/bin/hdfs:73):
. $HADOOP_LIBEXEC_DIR/hdfs-config.sh
/d
78:	COMMAND=$1
namenode
79:	shift
bashdb<228> pr $1
-format

if [ "$COMMAND" = "namenode" ] ; then
  CLASS='org.apache.hadoop.hdfs.server.namenode.NameNode'

bashdb<235> pr $HADOOP_OPTS
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true
bashdb<237> pr $HADOOP_NAMENODE_OPTS
-Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender 

  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_NAMENODE_OPTS"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender 

205:	export CLASSPATH=$CLASSPATH
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/*:/opt/hadoop-2.6.0/share/hadoop/yarn/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar

HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,NullAppender}"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender  -Dhadoop.security.logger=INFO,NullAppender

else
  # run it
  exec "$JAVA" -Dproc_$COMMAND $JAVA_HEAP_MAX $HADOOP_OPTS $CLASS "$@"

/////////////////////////
/ we moeten voor deze line	,

$ vi libexec/hadoop-config.sh

# so that filenames w/ spaces are handled correctly in loops below
#IFS=
...
# restore ordinary behaviour
#unset IFS

/ Anders ERR met bash --debugger	, zonder --debugger is OK	, 
/ Blijkbaar vindt hij dit een class name	, 
 -Djava.net.preferIPv4Stack=true
/ omdat er spaces in voor mogen komen	?
/ TODO

/ 7	.

/ we gaan verder op	,
http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html

/ we moeten eerst	,
bash-4.3# vi etc/hadoop/core-site.xml 
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

bash-4.3# vi etc/hadoop/hdfs-site.xml 

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

/ Ook omdat we een arg moeten geven aan sbin/start-dfs.sh voor debug	,  comment we out	,
# get arguments
if [ 0 -eq 1 ];then
if [ $# -ge 1 ]; then
        nameStartOpt="$1"
        shift
        case "$nameStartOpt" in
          (-upgrade)
                ;;
          (-rollback)
                dataStartOpt="$nameStartOpt"
                ;;
          (*)
                  echo $usage
                  exit 1
            ;;
        esac
fi
fi

sh-4.3# bash --debugger sbin/start-dfs.sh ""

25:	bin=`dirname "${BASH_SOURCE-$0}"`
/opt/hadoop-2.6.0/sbin

26:	bin=`cd "$bin"; pwd`
28:	DEFAULT_LIBEXEC_DIR="$bin"/../libexec
29:	HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
30:	. $HADOOP_LIBEXEC_DIR/hdfs-config.sh
/ sets CLASSPATH, ...

NAMENODES=$($HADOOP_PREFIX/bin/hdfs getconf -namenodes)
/ bash --debugger gaat hier NIET in	, wel bin/hdfs getconf -namenodes apart	, en er komt localhost uit	,
/ we kunnen ook in de debugger	,
bashdb<9> pr $(bin/hdfs getconf -namenodes)
localhost

/ we insert .	, voor debug	,
. "$HADOOP_PREFIX/sbin/hadoop-daemons.sh" \
  --config "$HADOOP_CONF_DIR" \
  --hostnames "$NAMENODES" \
  --script "$bin/hdfs" start namenode $nameStartOpt

/ Intermezzo

/ Maakt het uit, een . ervoor of niet?
/ TODO

/ we gaan zo debug, maar als we (zonder . zoals in orig)	, n geven , zien we de namenode started	,

bashdb<12> n
The authenticity of host 'localhost (::1)' can't be established.
ECDSA key fingerprint is e5:8f:51:31:35:22:88:e6:cc:e3:ad:ef:de:1c:49:06.
Are you sure you want to continue connecting (yes/no)? yes
localhost: Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
localhost: starting namenode, logging to /opt/hadoop-2.6.0/logs/hadoop-root-namenode-3791134b8c67.out
(/opt/hadoop-2.6.0/sbin/start-dfs.sh:72):
bashdb<13> she 
    1 13283 13270 13270 ?           -1 Sl       0   0:05 /etc/alternatives/java_sdk/bin/java -Dproc_namenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop-root-namenode-3791134b8c67.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.NameNode

/ Hierna wordt de datanode started, see beneden	, 

/ Einde Intermezzo 

/ Intermezzo 

/ als we dit apart willen debug	,

bash-4.3# bash --debugger sbin/hadoop-daemons.sh -- --config /opt/hadoop-2.6.0/etc/hadoop --hostname localhost --script /opt/hadoop-2.6.0/sbin/hdfs start namenode 

/ --script /opt/hadoop-2.6.0/sbin/hdfs wordt hadoopScript beneden	, maar wordt used? TODO	,
/ staat los van start namenode	, die worden wel used!

/ Let op de extra -- 	, dit moet als we --debugger doen	, anders krijgen we	,
bad option: `--config'
Use -h or --help to see options.
/ Dit zijn WH bashdb messages	,
/ TODO

/ Einde Intermezzo

bashdb<10> s
(/opt/hadoop-2.6.0/sbin/hadoop-daemons.sh:21):
bashdb<11> pr "$0"
sbin/start-dfs.sh
bashdb<12> pr "$@"
--config /opt/hadoop-2.6.0/etc/hadoop --hostnames localhost --script /opt/hadoop-2.6.0/sbin/hdfs start namenode

bin=`dirname "${BASH_SOURCE-$0}"`
bin=`cd "$bin"; pwd`
/opt/hadoop-2.6.0/sbin

34:	. $HADOOP_LIBEXEC_DIR/hadoop-config.sh
bashdb<10> s
74:if [ $# -gt 1 ]
then
    if [ "--config" = "$1" ]
      then
          shift
          confdir=$1
/opt/hadoop-2.6.0/etc/hadoop
          shift
          HADOOP_CONF_DIR=$confdir
bashdb<46> pr "$@"
--hostnames localhost --script /opt/hadoop-2.6.0/sbin/hdfs start namenode
/ dus eraf gehaald: --config /opt/hadoop-2.6.0/etc/hadoop

if [ $# -gt 1 ]
then
    if [ "--hosts" = "$1" ]
/NEE
    elif [ "--hostnames" = "$1" ]
    then
        shift
        export HADOOP_SLAVE_NAMES=$1
localhost
        shift
bashdb<60> pr "$@"
--script /opt/hadoop-2.6.0/sbin/hdfs start namenode

/ --config /opt/hadoop-2.6.0/etc/hadoop is onnodig, want dat is de default in hadoop-config.sh	,

/t

. "$bin/slaves.sh" --config $HADOOP_CONF_DIR cd "$HADOOP_PREFIX" \; "$bin/hadoop-daemon.sh" --config $HADOOP_CONF_DIR "$@"
# [ERICJ] exec -> .
/s
bashdb<21> pr "$@"
--config /opt/hadoop-2.6.0/etc/hadoop cd /opt/hadoop-2.6.0 \; /opt/hadoop-2.6.0/sbin/hadoop-daemon.sh --config /opt/hadoop-2.6.0/etc/hadoop --script /opt/hadoop-2.6.0/sbin/hdfs start namenode

bin=`dirname "${BASH_SOURCE-$0}"`
bin=`cd "$bin"; pwd`
/opt/hadoop-2.6.0/sbin

. $HADOOP_LIBEXEC_DIR/hadoop-config.sh

bashdb<30> pr "$@"
cd /opt/hadoop-2.6.0 \; /opt/hadoop-2.6.0/sbin/hadoop-daemon.sh --config /opt/hadoop-2.6.0/etc/hadoop --script /opt/hadoop-2.6.0/sbin/hdfs start namenode

if [ "$HADOOP_SLAVE_NAMES" != '' ] ; then
localhost
/set in hadoop-config.sh
  SLAVE_NAMES=$HADOOP_SLAVE_NAMES

 ssh $HADOOP_SSH_OPTS $slave $"${@// /\\ }" \
   2>&1 | sed "s/^/$slave: /" &

bashdb<56> pr $"${@// /\\ }"
cd /opt/hadoop-2.6.0 \; /opt/hadoop-2.6.0/sbin/hadoop-daemon.sh --config /opt/hadoop-2.6.0/etc/hadoop --script /opt/hadoop-2.6.0/sbin/hdfs start namenode

/ --script /opt/hadoop-2.6.0/sbin/hdfs wordt niet used	, wordt set als hadoopScript	, maar hdfsScript=bin/hdfs wordt exec	,

/ we veranderen	,
-bash-4.3# vi etc/hadoop/hadoop-env.sh
export JAVA_HOME="/etc/alternatives/java_sdk"
# [ERICJ] ${JAVA_HOME} -> "/etc/alternatives/java_sdk"

/ Maar voor het debuggen hoeft dat eig niet eens, we slaan ssh localhost over	,

-bash-4.3# bash --debugger sbin/hadoop-daemon.sh -- --config /opt/hadoop-2.6.0/etc/hadoop --script /opt/hadoop-2.6.0/sbin/hdfs start namenode

/ we moeten bash opnieuw start	,

/ let op -- extra	, omdat we --debugger	,
/ daarom bovenin in script	,

if [ "--" == "$1" ];then shift;fi

bin=`dirname "${BASH_SOURCE-$0}"`
bin=`cd "$bin"; pwd`
/opt/hadoop-2.6.0/sbin

. $HADOOP_LIBEXEC_DIR/hadoop-config.sh

bashdb<9> pr "$@"
--script /opt/hadoop-2.6.0/sbin/hdfs start namenode

if [ "--script" = "$1" ]
  then
    shift
    hadoopScript=$1
/opt/hadoop-2.6.0/sbin/hdfs
    shift

startStop=$1
start
shift
command=$1
namenode
shift

  . "${HADOOP_CONF_DIR}/hadoop-env.sh"
bashdb<25> s
export JAVA_HOME="/etc/alternatives/java_sdk"

export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/etc/hadoop"}
/opt/hadoop-2.6.0/etc/hadoop
...
99:	export HADOOP_IDENT_STRING=$USER
root

/t
if [ "$HADOOP_PID_DIR" = "" ]; then
  HADOOP_PID_DIR=/tmp
if [ "$HADOOP_LOG_DIR" = "" ]; then
/NEE
/opt/hadoop-2.6.0/logs

/ Intermezzo

[eric@localhost hadoop-2.6.0]$ ls -ltr
...
drwxr-xr-x. 2 20000 20000  4096 Mar 26 20:25 bin
drwxr-xr-x. 2 root  root   4096 Mar 27 12:01 logs

/ wanneer created?
/ TODO

/ Einde Intermezzo

# some variables
export HADOOP_LOGFILE=hadoop-$HADOOP_IDENT_STRING-$command-$HOSTNAME.log
hadoop-root-namenode-.log
/ TODO ($HOSTNAME)
export HADOOP_ROOT_LOGGER=${HADOOP_ROOT_LOGGER:-"INFO,RFA"}
INFO,RFA
export HADOOP_SECURITY_LOGGER=${HADOOP_SECURITY_LOGGER:-"INFO,RFAS"}
INFO,RFAS
export HDFS_AUDIT_LOGGER=${HDFS_AUDIT_LOGGER:-"INFO,NullAppender"}
INFO,NullAppender
log=$HADOOP_LOG_DIR/hadoop-$HADOOP_IDENT_STRING-$command-$HOSTNAME.out
/opt/hadoop-2.6.0/logs/hadoop-root-namenode-.out
pid=$HADOOP_PID_DIR/hadoop-$HADOOP_IDENT_STRING-$command.pid
/tmp/hadoop-root-namenode.pid
HADOOP_STOP_TIMEOUT=${HADOOP_STOP_TIMEOUT:-5}
5

# Set default scheduling priority
if [ "$HADOOP_NICENESS" = "" ]; then
    export HADOOP_NICENESS=0
0

case $startStop in
  (start)
    [ -w "$HADOOP_PID_DIR" ] ||  mkdir -p "$HADOOP_PID_DIR"
/tmp
   if [ -f $pid ]; then
/ NEE

   if [ "$HADOOP_MASTER" != "" ]; then
/ NEE

    hadoop_rotate_log $log
/opt/hadoop-2.6.0/logs/hadoop-root-namenode-.out
/ TODO

    cd "$HADOOP_PREFIX"
    case $command in
namenode
      namenode|secondarynamenode|datanode|journalnode|dfs|dfsadmin|fsck|balancer|zkfc)
        if [ -z "$HADOOP_HDFS_HOME" ]; then
          hdfsScript="$HADOOP_PREFIX"/bin/hdfs
/opt/hadoop-2.6.0/bin/hdfs

        nohup nice -n $HADOOP_NICENESS $hdfsScript --config $HADOOP_CONF_DIR $command "$@" > "$log" 2>&1 < /dev/null &

/ Voor debugging, 
        . $hdfsScript --config $HADOOP_CONF_DIR $command "$@" > "$log" 2>&1 < /dev/null &
/s
bashdb<49> pr [[ "$@" == "" ]] ;echo $?
[[ ==  ]]
0
/ Voor hadoop-daemon.sh was het --config /opt/hadoop-2.6.0/etc/hadoop --script /opt/hadoop-2.6.0/sbin/hdfs start namenode	, maar is allemaal al process	,

bashdb<50> s
(/opt/hadoop-2.6.0/bin/hdfs:28):
bashdb<48> pr "$@"
--config /opt/hadoop-2.6.0/etc/hadoop namenode

/ de 1ste keer dat we in bin/hdfs waren	, was het bin/hdfs namenode -format	,

bin=`which $0`
/ klopt nu niet 	, omdat we . $hdfsScript hebben gedaan	, 
bin=`dirname ${bin}`
bashdb<61> eval bin=/opt/hadoop-2.6.0/bin
bin=`cd "$bin" > /dev/null; pwd`

. $HADOOP_LIBEXEC_DIR/hdfs-config.sh

bashdb<67> pr "$@"
namenode

if [ "$COMMAND" = "namenode" ] ; then
  CLASS='org.apache.hadoop.hdfs.server.namenode.NameNode'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_NAMENODE_OPTS"
-Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop-root-namenode-.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop-2.6.0/lib/native:/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender
/ Nu zonder space aan het begin	,
/ TODO

276:	  exec "$JAVA" -Dproc_$COMMAND $JAVA_HEAP_MAX $HADOOP_OPTS $CLASS "$@"
bashdb<83> pr [[ "$@" == "" ]]; echo $?
[[ ==  ]]
0

/t
(/opt/hadoop-2.6.0/sbin/hadoop-daemon.sh:172):
        nohup nice -n $HADOOP_NICENESS $hdfsScript --config $HADOOP_CONF_DIR $command "$@" > "$log" 2>&1 < /dev/null &
/d
172:	    echo $! > $pid
11943 

/ Klopt	,
bashdb<49> sh
bashdb $ ps ajx --width 2000

 9025 11484 11484  9025 pts/0    12128 S        0   0:00 bash --debugger sbin/hadoop-daemon.sh -- --config /opt/hadoop-2.6.0/etc/hadoop --script /opt/hadoop-2.6.0/sbin/hdfs start namenode

11484 11943 11484  9025 pts/0    12128 Sl       0   0:07 /etc/alternatives/java_sdk/bin/java -Dproc_namenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop-root-namenode-.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.NameNode

184:	      echo "ulimit -a for user $USER" >> $log
185:	      ulimit -a >> $log 2>&1
bashdb<59> she
bashdb $ cat /opt/hadoop-2.6.0/logs/hadoop-root-namenode-.out
ulimit -a for user root
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 63628
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1048576
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 1048576
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited

/ Einde van 	,
-bash-4.3# bash --debugger sbin/hadoop-daemon.sh -- --config /opt/hadoop-2.6.0/etc/hadoop --script /opt/hadoop-2.6.0/sbin/hdfs start namenode
/ OK

/ als we niet zelf hadden started	, dan 	,
/t
(/opt/hadoop-2.6.0/sbin/start-dfs.sh:72):
"$HADOOP_PREFIX/sbin/hadoop-daemons.sh" \
  --config "$HADOOP_CONF_DIR" \
  --hostnames "$NAMENODES" \
  --script "$bin/hdfs" start namenode $nameStartOpt
/d
localhost: starting namenode, logging to /opt/hadoop-2.6.0/logs/hadoop-root-namenode-3791134b8c67.out

/ we add weer een . voor	,
# datanodes (using default slaves file)

if [ -n "$HADOOP_SECURE_DN_USER" ]; then
/ NEE
else
.  "$HADOOP_PREFIX/sbin/hadoop-daemons.sh" \
    --config "$HADOOP_CONF_DIR" \
    --script "$bin/hdfs" start datanode $dataStartOpt
fi
# [ERICJ] extra . ervoor om step in bij debug   ,
bashdb<13> s
(/opt/hadoop-2.6.0/sbin/hadoop-daemons.sh:21):
bashdb<13> pr "$@"
--config /opt/hadoop-2.6.0/etc/hadoop --script /opt/hadoop-2.6.0/sbin/hdfs start datanode
/ --script /opt/hadoop-2.6.0/sbin/hdfs wordt NIET used	, want will be hadoopScript	, maar hdfsScript will be used	, en is WEL OK: /opt/hadoop-2.6.0/bin/hdfs

34:	. $HADOOP_LIBEXEC_DIR/hadoop-config.sh
bashdb<21> pr "$@"
--script /opt/hadoop-2.6.0/sbin/hdfs start datanode

36:	. "$bin/slaves.sh" --config $HADOOP_CONF_DIR cd "$HADOOP_PREFIX" \; "$bin/hadoop-daemon.sh" --config $HADOOP_CONF_DIR "$@"
bashdb<22> s
(/opt/hadoop-2.6.0/sbin/slaves.sh:30):
--config /opt/hadoop-2.6.0/etc/hadoop cd /opt/hadoop-2.6.0 \; /opt/hadoop-2.6.0/sbin/hadoop-daemon.sh --config /opt/hadoop-2.6.0/etc/hadoop --script /opt/hadoop-2.6.0/sbin/hdfs start datanode

43:	. $HADOOP_LIBEXEC_DIR/hadoop-config.sh
bashdb<31> pr "$@"
cd /opt/hadoop-2.6.0 \; /opt/hadoop-2.6.0/sbin/hadoop-daemon.sh --config /opt/hadoop-2.6.0/etc/hadoop --script /opt/hadoop-2.6.0/sbin/hdfs start datanode

46:	  . "${HADOOP_CONF_DIR}/hadoop-env.sh"
bashdb<53> s
(/opt/hadoop-2.6.0/sbin/slaves.sh:54):
if [ "$HADOOP_SLAVE_NAMES" != '' ] ; then
/ NEE
else
  SLAVE_FILE=${HADOOP_SLAVES:-${HADOOP_CONF_DIR}/slaves}
bashdb<55> pr [[ ${HADOOP_SLAVES} == "" ]];echo $?
0
/opt/hadoop-2.6.0/etc/hadoop/slaves
  SLAVE_NAMES=$(cat "$SLAVE_FILE" | sed  's/#.*$//;/^$/d')
localhost
for slave in $SLAVE_NAMES ; do
 ssh $HADOOP_SSH_OPTS $slave $"${@// /\\ }" \
   2>&1 | sed "s/^/$slave: /" &
bashdb<61> pr "$@"
/=
bashdb<62> pr $"${@// /\\ }"
cd /opt/hadoop-2.6.0 \; /opt/hadoop-2.6.0/sbin/hadoop-daemon.sh --config /opt/hadoop-2.6.0/etc/hadoop --script /opt/hadoop-2.6.0/sbin/hdfs start datanode

/ Dit doen we zelf	, de ssh localhost hoeven we niet te doen	,
-bash-4.3# bash --debugger sbin/hadoop-daemon.sh -- --config /opt/hadoop-2.6.0/etc/hadoop -pt /opt/hadoop-2.6.0/sbin/hdfs start datanode

bashdb<8> pr "$@"
--config /opt/hadoop-2.6.0/etc/hadoop --script /opt/hadoop-2.6.0/sbin/hdfs start datanode
47:	. $HADOOP_LIBEXEC_DIR/hadoop-config.sh
bashdb<10> pr "$@"
--script /opt/hadoop-2.6.0/sbin/hdfs start datanode

#default value
hadoopScript="$HADOOP_PREFIX"/bin/hadoop
if [ "--script" = "$1" ]
  then
    shift
    hadoopScript=$1
/opt/hadoop-2.6.0/sbin/hdfs
    shift
fi
startStop=$1
start
shift
command=$1
datanode
shift
/ nu is "$@" == ""	,

  . "${HADOOP_CONF_DIR}/hadoop-env.sh"
bashdb<24> s

/ sets	,
25:	export JAVA_HOME="/etc/alternatives/java_sdk"
34:	export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/etc/hadoop"}
/opt/hadoop-2.6.0/etc/hadoop
37:	for f in $HADOOP_HOME/contrib/capacity-scheduler/*.jar; do
39:	    export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$f
/contrib/capacity-scheduler/*.jar
50:	export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"
-Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true
53:	export HADOOP_NAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_NAMENODE_OPTS"
-Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender
54:	export HADOOP_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS $HADOOP_DATANODE_OPTS"
-Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS
56:	export HADOOP_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_SECONDARYNAMENODE_OPTS"
-Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender
58:	export HADOOP_NFS3_OPTS="$HADOOP_NFS3_OPTS"
""
59:	export HADOOP_PORTMAP_OPTS="-Xmx512m $HADOOP_PORTMAP_OPTS"
-Xmx512m -Xmx512m
62:	export HADOOP_CLIENT_OPTS="-Xmx512m $HADOOP_CLIENT_OPTS"
-Xmx512m -Xmx512m
70:	export HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER}
""
76:	export HADOOP_SECURE_DN_LOG_DIR=${HADOOP_LOG_DIR}/${HADOOP_HDFS_USER}
/opt/hadoop-2.6.0/logs/
95:	export HADOOP_PID_DIR=${HADOOP_PID_DIR}
""
96:	export HADOOP_SECURE_DN_PID_DIR=${HADOOP_PID_DIR}
""
99:	export HADOOP_IDENT_STRING=$USER
root

/t
->0 in file `sbin/hadoop-daemon.sh' at line 88

# Determine if we're starting a secure datanode, and if so, redefine appropriate variables
if [ "$command" == "datanode" ] && [ "$EUID" -eq 0 ] && [ -n "$HADOOP_SECURE_DN_USER" ]; then
/ NEE	,
"$HADOOP_SECURE_DN_USER"==""	,

if [ "$HADOOP_PID_DIR" = "" ]; then
  HADOOP_PID_DIR=/tmp

# some variables
export HADOOP_LOGFILE=hadoop-$HADOOP_IDENT_STRING-$command-$HOSTNAME.log
hadoop-root-datanode-.log
export HADOOP_ROOT_LOGGER=${HADOOP_ROOT_LOGGER:-"INFO,RFA"}
INFO,RFA
export HADOOP_SECURITY_LOGGER=${HADOOP_SECURITY_LOGGER:-"INFO,RFAS"}
INFO,RFAS
export HDFS_AUDIT_LOGGER=${HDFS_AUDIT_LOGGER:-"INFO,NullAppender"}
INFO,NullAppender
log=$HADOOP_LOG_DIR/hadoop-$HADOOP_IDENT_STRING-$command-$HOSTNAME.out
/opt/hadoop-2.6.0/logs/hadoop-root-datanode-.out
pid=$HADOOP_PID_DIR/hadoop-$HADOOP_IDENT_STRING-$command.pid
/tmp/hadoop-root-datanode.pid
HADOOP_STOP_TIMEOUT=${HADOOP_STOP_TIMEOUT:-5}
5

case $startStop in
  (start)
    [ -w "$HADOOP_PID_DIR" ] ||  mkdir -p "$HADOOP_PID_DIR"
/tmp
    if [ -f $pid ]; then
/NEE
/tmp/hadoop-root-datanode.pid

    if [ "$HADOOP_MASTER" != "" ]; then
/NEE

    hadoop_rotate_log $log
    echo starting $command, logging to $log
starting datanode, logging to /opt/hadoop-2.6.0/logs/hadoop-root-datanode-.out
    cd "$HADOOP_PREFIX"
    case $command in
      namenode|secondarynamenode|datanode|journalnode|dfs|dfsadmin|fsck|balancer|zkfc)
        if [ -z "$HADOOP_HDFS_HOME" ]; then
/ NEE
/opt/hadoop-2.6.0
        else
          hdfsScript="$HADOOP_HDFS_HOME"/bin/hdfs

       . $hdfsScript --config $HADOOP_CONF_DIR $command "$@"
# [ERICJ]
#        nohup nice -n $HADOOP_NICENESS $hdfsScript --config $HADOOP_CONF_DIR $command "$@" > "$log" 2>&1 < /dev/null &
bashdb<48> s
(/opt/hadoop-2.6.0/bin/hdfs:28):

bin=`which $0`
/ Deze moeten we nu correct	, omdat we . doen	,
bashdb<51> eval bin=/opt/hadoop-2.6.0/bin/hdfs
bin=`dirname ${bin}`
bin=`cd "$bin" > /dev/null; pwd`

bashdb<57> pr "$@"
--config /opt/hadoop-2.6.0/etc/hadoop datanode

34:	. $HADOOP_LIBEXEC_DIR/hdfs-config.sh
bashdb<59> pr "$@"
datanode

COMMAND=$1
shift
elif [ "$COMMAND" = "datanode" ] ; then
  CLASS='org.apache.hadoop.hdfs.server.datanode.DataNode'
  if [ "$starting_secure_dn" = "true" ]; then
/ NEE
  else
    HADOOP_OPTS="$HADOOP_OPTS -server $HADOOP_DATANODE_OPTS"
-Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop-root-datanode-.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop-2.6.0/lib/native:/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -server -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS

export CLASSPATH=$CLASSPATH
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/*:/opt/hadoop-2.6.0/share/hadoop/yarn/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar

HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,NullAppender}"
else
  # run it
  exec "$JAVA" -Dproc_$COMMAND $JAVA_HEAP_MAX $HADOOP_OPTS $CLASS "$@"
exec /etc/alternatives/java_sdk/bin/java -Dproc_datanode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop-root-datanode-.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop-2.6.0/lib/native:/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -server -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode

/t
->0 in file `sbin/start-dfs.sh' at line 86
else
  "$HADOOP_PREFIX/sbin/hadoop-daemons.sh" \
    --config "$HADOOP_CONF_DIR" \
    --script "$bin/hdfs" start datanode $dataStartOpt
/d
localhost: starting datanode, logging to /opt/hadoop-2.6.0/logs/hadoop-root-datanode-3791134b8c67.out
bashdb<17> she
bashdb $ ps axj --width 2000
    1 16720 16700 16700 ?           -1 Sl       0   0:06 /etc/alternatives/java_sdk/bin/java -Dproc_namenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop-root-namenode-3791134b8c67.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.NameNode
    1 16849 16837 16837 ?           -1 Sl       0   0:05 /etc/alternatives/java_sdk/bin/java -Dproc_datanode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop-root-datanode-3791134b8c67.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -server -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode
16510 17168 17168  9025 pts/0    17171 S        0   0:00 /usr/bin/bash --init-file /tmp/bashdb_profile_16510

SECONDARY_NAMENODES=$($HADOOP_PREFIX/bin/hdfs getconf -secondarynamenodes 2>/dev/null)
0.0.0.0

if [ -n "$SECONDARY_NAMENODES" ]; then
  echo "Starting secondary namenodes [$SECONDARY_NAMENODES]"

  "$HADOOP_PREFIX/sbin/hadoop-daemons.sh" \
      --config "$HADOOP_CONF_DIR" \
      --hostnames "$SECONDARY_NAMENODES" \
      --script "$bin/hdfs" start secondarynamenode
fi

The authenticity of host '0.0.0.0 (0.0.0.0)' can't be established.
ECDSA key fingerprint is e5:8f:51:31:35:22:88:e6:cc:e3:ad:ef:de:1c:49:06.
Are you sure you want to continue connecting (yes/no)? yes
0.0.0.0: Warning: Permanently added '0.0.0.0' (ECDSA) to the list of known hosts.
0.0.0.0: starting secondarynamenode, logging to /opt/hadoop-2.6.0/logs/hadoop-root-secondarynamenode-3791134b8c67.out

bashdb<23> she 
bashdb $ ps ajx --width 2000
    1 16720 16700 16700 ?           -1 Sl       0   0:06 /etc/alternatives/java_sdk/bin/java -Dproc_namenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop-root-namenode-3791134b8c67.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.NameNode
    1 16849 16837 16837 ?           -1 Sl       0   0:05 /etc/alternatives/java_sdk/bin/java -Dproc_datanode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop-root-datanode-3791134b8c67.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -server -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode
    1 17281 17269 17269 ?           -1 Sl       0   0:03 /etc/alternatives/java_sdk/bin/java -Dproc_secondarynamenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop-root-secondarynamenode-3791134b8c67.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode

bashdb $  ls /opt/hadoop-2.6.0/logs -ltr
total 608
-rw-r--r--. 1 root root    721 Mar 27 18:30 hadoop-root-namenode-.out.3
-rw-r--r--. 1 root root    721 Mar 27 20:48 hadoop-root-namenode-.out.2
-rw-r--r--. 1 root root      0 Mar 27 22:09 SecurityAuth-root.audit
-rw-r--r--. 1 root root    721 Mar 27 22:30 hadoop-root-namenode-.out.1
-rw-r--r--. 1 root root    721 Mar 27 23:37 hadoop-root-namenode-.out
-rw-r--r--. 1 root root 131047 Mar 27 23:47 hadoop-root-namenode-.log
-rw-r--r--. 1 root root    721 Mar 27 23:49 hadoop-root-namenode-3791134b8c67.out.3
-rw-r--r--. 1 root root    721 Mar 28 11:42 hadoop-root-namenode-3791134b8c67.out.2
-rw-r--r--. 1 root root    721 Mar 28 11:46 hadoop-root-namenode-3791134b8c67.out.1
-rw-r--r--. 1 root root  22781 Mar 28 16:15 hadoop-root-datanode-.log
-rw-r--r--. 1 root root    721 Mar 28 16:59 hadoop-root-namenode-3791134b8c67.out
-rw-r--r--. 1 root root    721 Mar 28 16:59 hadoop-root-datanode-3791134b8c67.out
-rw-r--r--. 1 root root  21503 Mar 28 16:59 hadoop-root-datanode-3791134b8c67.log
-rw-r--r--. 1 root root    721 Mar 28 17:05 hadoop-root-secondarynamenode-3791134b8c67.out
-rw-r--r--. 1 root root  22260 Mar 28 17:06 hadoop-root-secondarynamenode-3791134b8c67.log
-rw-r--r--. 1 root root 368055 Mar 28 17:11 hadoop-root-namenode-3791134b8c67.log

SHARED_EDITS_DIR=$($HADOOP_PREFIX/bin/hdfs getconf -confKey dfs.namenode.shared.edits.dir 2>&-)
""
114:	AUTOHA_ENABLED=$($HADOOP_PREFIX/bin/hdfs getconf -confKey dfs.ha.automatic-failover.enabled)
false

/ Einde start-dfs.sh

/ 7	.

./share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar
 47615 Thu Nov 13 21:12:36 CET 2014 org/apache/hadoop/hdfs/server/namenode/NameNode.class

/ 7	. 

[eric@localhost fedora]$ sudo docker build --rm -t local/c7-systemd-sshd-java-hadoop .

/ 7	.

/ op laptop		,

/ Met command achter ssh localhost doet hij cmd op (remote) en komt weer terug	, je logt niet in	,

[eric@localhost own]$ ssh localhost cd /tmp';'ls
/ of	,
[eric@localhost own]$ ssh localhost 'cd /tmp;ls'
/ of	,
[eric@localhost own]$ ssh localhost "cd /tmp;ls"
/ of	,
[eric@localhost own]$ ssh localhost "cd /tmp
> ls"
hsperfdata_eric
hsperfdata_root
systemd-private-64cedaa28e08471b87fcb449e68b5550-colord.service-H3fk1a
systemd-private-64cedaa28e08471b87fcb449e68b5550-cups.service-b9JpvZ
systemd-private-64cedaa28e08471b87fcb449e68b5550-rtkit-daemon.service-tQvQsF
/ OK

[eric@localhost own]$ ssh localhost cd /tmp;ls
/ we zien ls van current dir	,
/ TODO

export eric=saskia
ssh localhost 'echo "$eric"'


/ 7	. 


 /7	. 

/ met ret (return) ga je na step er weer uit	,

/ 7	.

[eric@localhost own]$ sudo yum install java-1.8.0-openjdk-devel
/ op laptop	,

==========================================================================================================
 Package                             Arch           Version                         Repository       Size
==========================================================================================================
Installing:
 java-1.8.0-openjdk-devel            x86_64         1:1.8.0.31-5.b13.fc21           updates         9.7 M
Installing for dependencies:
 java-1.8.0-openjdk                  x86_64         1:1.8.0.31-5.b13.fc21           updates         213 k
 ttmkfdir                            x86_64         3.0.9-44.fc21                   fedora           52 k
 xorg-x11-fonts-Type1                noarch         7.5-14.fc21                     fedora          522 k
Updating for dependencies:
 java-1.8.0-openjdk-headless         x86_64         1:1.8.0.31-5.b13.fc21           updates          31 M

Transaction Summary
==========================================================================================================
Install  1 Package  (+3 Dependent packages)
Upgrade             ( 1 Dependent package)

Total download size: 41 M
Is this ok [y/d/N]: y
Downloading packages:
updates/21/x86_64/prestodelta                                                      | 1.1 MB  00:00:01     
Delta RPMs reduced 31 M of updates to 11 M (63% saved)
(1/5): ttmkfdir-3.0.9-44.fc21.x86_64.rpm                                           |  52 kB  00:00:00     
(2/5): java-1.8.0-openjdk-1.8.0.31-5.b13.fc21.x86_64.rpm                           | 213 kB  00:00:02     
(3/5): xorg-x11-fonts-Type1-7.5-14.fc21.noarch.rpm                                 | 522 kB  00:00:06     
(4/5): java-1.8.0-openjdk-headless-1.8.0.31-3.b13.fc21_1.8.0.31-5.b13.fc21.x86_64. |  11 MB  00:00:26     
(5/5): java-1.8.0-openjdk-devel-1.8.0.31-5.b13.fc21.x86_64.rpm                     | 9.7 MB  00:00:33     
Finishing delta rebuilds of 1 package(s) (31 M)
----------------------------------------------------------------------------------------------------------
Total                                                                     463 kB/s |  22 MB  00:00:47     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction (shutdown inhibited)
  Updating   : 1:java-1.8.0-openjdk-headless-1.8.0.31-5.b13.fc21.x86_64                               1/6 
warning: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-5.b13.fc21.x86_64/jre/lib/security/US_export_policy.jar created as /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-5.b13.fc21.x86_64/jre/lib/security/US_export_policy.jar.rpmnew
warning: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-5.b13.fc21.x86_64/jre/lib/security/local_policy.jar created as /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-5.b13.fc21.x86_64/jre/lib/security/local_policy.jar.rpmnew
  Installing : ttmkfdir-3.0.9-44.fc21.x86_64                                                          2/6 
  Installing : xorg-x11-fonts-Type1-7.5-14.fc21.noarch                                                3/6 
  Installing : 1:java-1.8.0-openjdk-1.8.0.31-5.b13.fc21.x86_64                                        4/6 
  Installing : 1:java-1.8.0-openjdk-devel-1.8.0.31-5.b13.fc21.x86_64                                  5/6 
  Cleanup    : 1:java-1.8.0-openjdk-headless-1.8.0.31-3.b13.fc21.x86_64                               6/6 
warning: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-3.b13.fc21.x86_64/jre/lib/security/local_policy.jar saved as /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-3.b13.fc21.x86_64/jre/lib/security/local_policy.jar.rpmsave
warning: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-3.b13.fc21.x86_64/jre/lib/security/US_export_policy.jar saved as /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-3.b13.fc21.x86_64/jre/lib/security/US_export_policy.jar.rpmsave
  Verifying  : ttmkfdir-3.0.9-44.fc21.x86_64                                                          1/6 
  Verifying  : xorg-x11-fonts-Type1-7.5-14.fc21.noarch                                                2/6 
  Verifying  : 1:java-1.8.0-openjdk-headless-1.8.0.31-5.b13.fc21.x86_64                               3/6 
  Verifying  : 1:java-1.8.0-openjdk-devel-1.8.0.31-5.b13.fc21.x86_64                                  4/6 
  Verifying  : 1:java-1.8.0-openjdk-1.8.0.31-5.b13.fc21.x86_64                                        5/6 
  Verifying  : 1:java-1.8.0-openjdk-headless-1.8.0.31-3.b13.fc21.x86_64                               6/6 

Installed:
  java-1.8.0-openjdk-devel.x86_64 1:1.8.0.31-5.b13.fc21                                                   

Dependency Installed:
  java-1.8.0-openjdk.x86_64 1:1.8.0.31-5.b13.fc21             ttmkfdir.x86_64 0:3.0.9-44.fc21            
  xorg-x11-fonts-Type1.noarch 0:7.5-14.fc21                  

Dependency Updated:
  java-1.8.0-openjdk-headless.x86_64 1:1.8.0.31-5.b13.fc21                                                

Complete!

/ 7	.

export 50070

/ 7	.

bash-4.3# bash --debugger bin/hdfs dfs -mkdir /user
bashdb<6> pr "$@"
dfs -mkdir /user
34:	. $HADOOP_LIBEXEC_DIR/hdfs-config.sh
/ sets JAVA_HOME, ...
COMMAND=$1
dfs
shift


elif [ "$COMMAND" = "dfs" ] ; then
  CLASS=org.apache.hadoop.fs.FsShell
bashdb<26> pr $HADOOP_OPTS
-Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true
bashdb<25> pr $HADOOP_CLIENT_OPTS
-Xmx512m
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"

276:	  exec "$JAVA" -Dproc_$COMMAND $JAVA_HEAP_MAX $HADOOP_OPTS $CLASS "$@"
bashdb<32> pr "$@"
-mkdir /user






/ Einde DEBUG HADOOP TAR SINGLE CLUSTER

/ HADOOP SRC

/ 7	. 

/ op laptop	,
[eric@localhost Hadoop]$ pwd
/home/eric/Devel/Java/Hadoop
[eric@localhost Hadoop]$ tar xvzf ~/Downloads/hadoop-2.6.0-src.tar.gz 


/ Einde HADOOP SRC

/ DEBUG HADOOP SCRIPTS TODO (RM PARTS)

/ source foo.bash includes code in foo.bash in current script, so $0 is current script, not foo.bash	,
/ when foo.bash is called, then $0=foo.bash
/ which in bash geeft full path	,

/ Doe WH	,
bash-4.3# pwd
/opt/hadoop-2.6.0
bash-4.3# export HADOOP_HOME=$(pwd)
/ TODO

/ we hebben 	,
bash-4.3# export HADOOP_INSTALL=$(pwd)
// bash-4.3# export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
// bash-4.3#  export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"



bash-4.3# bashdb sbin/start-dfs.sh
25:	bin=`dirname "${BASH_SOURCE-$0}"`
/ BASH_SOURCE is OK, maar als je pr doet, zie je een verkeerde	,
/ TODO

30:	. $HADOOP_LIBEXEC_DIR/hdfs-config.sh
bashdb<6> s
(/opt/hadoop-2.6.0/libexec/hdfs-config.sh:21):
21:	bin=`which "$0"`
/ Overwrite	,
bashdb<10> ev bin=/opt/hadoop-2.6.0/sbin/start-dfs.sh

28:	  . ${HADOOP_LIBEXEC_DIR}/hadoop-config.sh
bashdb<17> s
(/opt/hadoop-2.6.0/libexec/hadoop-config.sh:50):
50:	this="${BASH_SOURCE-$0}"
bashdb<19> pr $this
/opt/hadoop-2.6.0/sbin/../libexec/hadoop-config.sh
51:	common_bin=$(cd -P -- "$(dirname -- "$this")" && pwd -P)
/opt/hadoop-2.6.0/libexec
57:	HADOOP_COMMON_DIR=${HADOOP_COMMON_DIR:-"share/hadoop/common"}
share/hadoop/common
58:	HADOOP_COMMON_LIB_JARS_DIR=${HADOOP_COMMON_LIB_JARS_DIR:-"share/hadoop/common/lib"}
share/hadoop/common/lib
59:	HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_COMMON_LIB_NATIVE_DIR:-"lib/native"}
lib/native
60:	HDFS_DIR=${HDFS_DIR:-"share/hadoop/hdfs"}
share/hadoop/hdfs
61:	HDFS_LIB_JARS_DIR=${HDFS_LIB_JARS_DIR:-"share/hadoop/hdfs/lib"}
share/hadoop/hdfs/lib
62:	YARN_DIR=${YARN_DIR:-"share/hadoop/yarn"}
share/hadoop/yarn
63:	YARN_LIB_JARS_DIR=${YARN_LIB_JARS_DIR:-"share/hadoop/yarn/lib"}
share/hadoop/yarn/lib
64:	MAPRED_DIR=${MAPRED_DIR:-"share/hadoop/mapreduce"}
share/hadoop/mapreduce
65:	MAPRED_LIB_JARS_DIR=${MAPRED_LIB_JARS_DIR:-"share/hadoop/mapreduce/lib"}
share/hadoop/mapreduce/lib
69:	HADOOP_DEFAULT_PREFIX=$(cd -P -- "$common_bin"/.. && pwd -P)
/opt/hadoop-2.6.0
70:	HADOOP_PREFIX=${HADOOP_PREFIX:-$HADOOP_DEFAULT_PREFIX}
/opt/hadoop-2.6.0
71:	export HADOOP_PREFIX
93:	  DEFAULT_CONF_DIR="etc/hadoop"
96:	export HADOOP_CONF_DIR="${HADOOP_CONF_DIR:-$HADOOP_PREFIX/$DEFAULT_CONF_DIR}"
/opt/hadoop-2.6.0/etc/hadoop
if [ -f "${HADOOP_CONF_DIR}/hadoop-env.sh" ]; then
  . "${HADOOP_CONF_DIR}/hadoop-env.sh"
bashdb<57> s
(/opt/hadoop-2.6.0/etc/hadoop/hadoop-env.sh:25):
25:	export JAVA_HOME=${JAVA_HOME}
/etc/alternatives/java_sdk
33:	export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/etc/hadoop"}
/opt/hadoop-2.6.0/etc/hadoop
36:	for f in $HADOOP_HOME/contrib/capacity-scheduler/*.jar; do
bashdb<46> pr $f
/opt/hadoop-2.6.0/contrib/capacity-scheduler/*.jar
/ TODO (zijn er NIET)	,
  if [ "$HADOOP_CLASSPATH" ]; then
/ JA
$HADOOP_CLASSPATH=/etc/alternatives/java_sdk/lib/tools.jar
    export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$f
bashdb<48> pr $HADOOP_CLASSPATH
/etc/alternatives/java_sdk/lib/tools.jar:/opt/hadoop-2.6.0/contrib/capacity-scheduler/*.jar

bashdb<49> pr $HADOOP_OPTS
""
export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"

52:	export HADOOP_NAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_NAMENODE_OPTS"
-Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender

53:	export HADOOP_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS $HADOOP_DATANODE_OPTS"
-Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender
/ TODO ($HADOOP_DATANODE_OPTS was al -Dhdfs.audit.logger=INFO,NullAppender)

55:	export HADOOP_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_SECONDARYNAMENODE_OPTS"
-Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender

export HADOOP_NFS3_OPTS="$HADOOP_NFS3_OPTS"
""
export HADOOP_PORTMAP_OPTS="-Xmx512m $HADOOP_PORTMAP_OPTS"
-Xmx512m
export HADOOP_CLIENT_OPTS="-Xmx512m $HADOOP_CLIENT_OPTS"
-Xmx512m

export HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER}
""
export HADOOP_SECURE_DN_LOG_DIR=${HADOOP_LOG_DIR}/${HADOOP_HDFS_USER}
""
export HADOOP_PID_DIR=${HADOOP_PID_DIR}
""
export HADOOP_SECURE_DN_PID_DIR=${HADOOP_PID_DIR}
""
export HADOOP_IDENT_STRING=$USER
""

/t
(/opt/hadoop-2.6.0/libexec/hadoop-config.sh:137):
  . "${HADOOP_CONF_DIR}/hadoop-env.sh"
/d
bindv6only=$(/sbin/sysctl -n net.ipv6.bindv6only 2> /dev/null)
0

export MALLOC_ARENA_MAX=${MALLOC_ARENA_MAX:-4}
4

JAVA=$JAVA_HOME/bin/java
/etc/alternatives/java_sdk/bin/java
JAVA_HEAP_MAX=-Xmx1000m

CLASSPATH="${HADOOP_CONF_DIR}"
/opt/hadoop-2.6.0/etc/hadoop

if [ "$HADOOP_COMMON_HOME" = "" ]; then
  if [ -d "${HADOOP_PREFIX}/$HADOOP_COMMON_DIR" ]; then
/ JA
$HADOOP_COMMON_DIR=share/hadoop/common
    export HADOOP_COMMON_HOME=$HADOOP_PREFIX
 /opt/hadoop-2.6.0

if [ -d "$HADOOP_COMMON_HOME/$HADOOP_COMMON_LIB_JARS_DIR" ]; then
/ JA
bashdb<100> pr $HADOOP_COMMON_LIB_JARS_DIR
share/hadoop/common/lib
  CLASSPATH=${CLASSPATH}:$HADOOP_COMMON_HOME/$HADOOP_COMMON_LIB_JARS_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*

CLASSPATH=${CLASSPATH}:$HADOOP_COMMON_HOME/$HADOOP_COMMON_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*

if [ "$HADOOP_LOG_DIR" = "" ]; then
  HADOOP_LOG_DIR="$HADOOP_PREFIX/logs"
if [ "$HADOOP_LOGFILE" = "" ]; then
  HADOOP_LOGFILE='hadoop.log'
if [ "$HADOOP_POLICYFILE" = "" ]; then
  HADOOP_POLICYFILE="hadoop-policy.xml"

if [ -d "${HADOOP_PREFIX}/build/native" -o -d "${HADOOP_PREFIX}/$HADOOP_COMMON_LIB_NATIVE_DIR" ]; then
/ JA
bashdb<93> pr ${HADOOP_PREFIX}/$HADOOP_COMMON_LIB_NATIVE_DIR
/opt/hadoop-2.6.0/lib/native

  if [ -d "${HADOOP_PREFIX}/$HADOOP_COMMON_LIB_NATIVE_DIR" ]; then
    if [ "x$JAVA_LIBRARY_PATH" != "x" ]; then
/ NEE
    else
      JAVA_LIBRARY_PATH=${HADOOP_PREFIX}/$HADOOP_COMMON_LIB_NATIVE_DIR
/opt/hadoop-2.6.0/lib/native

TOOL_PATH="${TOOL_PATH:-$HADOOP_PREFIX/share/hadoop/tools/lib/*}"
/opt/hadoop-2.6.0/share/hadoop/tools/lib/activation-1.1.jar /opt/hadoop-2.6.0/share/hadoop/tools/lib/apacheds-i18n-2.0.0-M15.jar /opt/hadoop-2.6.0/share/hadoop/tools/lib/apac...

/ Eerder	,
bashdb<100> pr $HADOOP_OPTS
 -Djava.net.preferIPv4Stack=true
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.log.dir=$HADOOP_LOG_DIR"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.log.file=$HADOOP_LOGFILE"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.home.dir=$HADOOP_PREFIX"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.id.str=$HADOOP_IDENT_STRING"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str=
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.root.logger=${HADOOP_ROOT_LOGGER:-INFO,console}"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str= -Dhadoop.root.logger=INFO,console

if [ "x$JAVA_LIBRARY_PATH" != "x" ]; then
  HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str= -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native
  export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$JAVA_LIBRARY_PATH
/opt/hadoop-2.6.0/lib/native

243:	HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.policy.file=$HADOOP_POLICYFILE"
 -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str= -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml

246:	HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"
/ Al	,

# put hdfs in classpath if present
if [ "$HADOOP_HDFS_HOME" = "" ]; then
  if [ -d "${HADOOP_PREFIX}/$HDFS_DIR" ]; then
bashdb<123> pr $HDFS_DIR
share/hadoop/hdfs
    export HADOOP_HDFS_HOME=$HADOOP_PREFIX
/opt/hadoop-2.6.0

if [ -d "$HADOOP_HDFS_HOME/$HDFS_DIR/webapps" ]; then
  CLASSPATH=${CLASSPATH}:$HADOOP_HDFS_HOME/$HDFS_DIR
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs

if [ -d "$HADOOP_HDFS_HOME/$HDFS_LIB_JARS_DIR" ]; then
  CLASSPATH=${CLASSPATH}:$HADOOP_HDFS_HOME/$HDFS_LIB_JARS_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*

CLASSPATH=${CLASSPATH}:$HADOOP_HDFS_HOME/$HDFS_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*

# put yarn in classpath if present
if [ "$HADOOP_YARN_HOME" = "" ]; then
  if [ -d "${HADOOP_PREFIX}/$YARN_DIR" ]; then
bashdb<137> pr $YARN_DIR
share/hadoop/yarn
    export HADOOP_YARN_HOME=$HADOOP_PREFIX
/opt/hadoop-2.6.0

if [ -d "$HADOOP_YARN_HOME/$YARN_DIR/webapps" ]; then
/ NEE

if [ -d "$HADOOP_YARN_HOME/$YARN_LIB_JARS_DIR" ]; then
  CLASSPATH=${CLASSPATH}:$HADOOP_YARN_HOME/$YARN_LIB_JARS_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/*

CLASSPATH=${CLASSPATH}:$HADOOP_YARN_HOME/$YARN_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/*:/opt/hadoop-2.6.0/share/hadoop/yarn/*

# put mapred in classpath if present AND different from YARN
if [ "$HADOOP_MAPRED_HOME" = "" ]; then
  if [ -d "${HADOOP_PREFIX}/$MAPRED_DIR" ]; then
bashdb<146> pr $MAPRED_DIR
share/hadoop/mapreduce
    export HADOOP_MAPRED_HOME=$HADOOP_PREFIX
/opt/hadoop-2.6.0

if [ "$HADOOP_MAPRED_HOME/$MAPRED_DIR" != "$HADOOP_YARN_HOME/$YARN_DIR" ] ; then
  if [ -d "$HADOOP_MAPRED_HOME/$MAPRED_DIR/webapps" ]; then
/ NEE

  if [ -d "$HADOOP_MAPRED_HOME/$MAPRED_LIB_JARS_DIR" ]; then
    CLASSPATH=${CLASSPATH}:$HADOOP_MAPRED_HOME/$MAPRED_LIB_JARS_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/*:/opt/hadoop-2.6.0/share/hadoop/yarn/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/*

  CLASSPATH=${CLASSPATH}:$HADOOP_MAPRED_HOME/$MAPRED_DIR'/*'
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/*:/opt/hadoop-2.6.0/share/hadoop/yarn/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/*

if [[ ( "$HADOOP_CLASSPATH" != "" ) && ( "$HADOOP_USE_CLIENT_CLASSLOADER" = "" ) ]]; then
$HADOOP_CLASSPATH=/etc/alternatives/java_sdk/lib/tools.jar:/opt/hadoop-2.6.0/contrib/capacity-scheduler/*.jar
  # Prefix it if its to be preceded
  if [ "$HADOOP_USER_CLASSPATH_FIRST" != "" ]; then
/ NEE
  else
    CLASSPATH=${CLASSPATH}:${HADOOP_CLASSPATH}
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/*:/opt/hadoop-2.6.0/share/hadoop/yarn/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/*:/etc/alternatives/java_sdk/lib/tools.jar:/opt/hadoop-2.6.0/contrib/capacity-scheduler/*.jar

/t
/opt/hadoop-2.6.0/sbin/start-dfs.sh:33
. $HADOOP_LIBEXEC_DIR/hdfs-config.sh
/d

/ we zien ook hier	,
bashdb<161> pr $CLASSPATH
/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/*:/opt/hadoop-2.6.0/share/hadoop/yarn/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/*:/etc/alternatives/java_sdk/lib/tools.jar:/opt/hadoop-2.6.0/contrib/capacity-scheduler/*.jar
bashdb<162> pr $JAVA
/etc/alternatives/java_sdk/bin/java
bashdb<163> pr $JAVA_HOME
/etc/alternatives/java_sdk

nameStartOpt="$nameStartOpt $@"
""
NAMENODES=$($HADOOP_PREFIX/bin/hdfs getconf -namenodes)
/s



/ 7	.

sbin/start-dfs.sh
	. $HADOOP_LIBEXEC_DIR/hdfs-config.sh
	  . ${HADOOP_LIBEXEC_DIR}/hadoop-config.sh
  			. "${HADOOP_CONF_DIR}/hadoop-env.sh"


bash-4.3# sbin/start-dfs.sh

start-dfs.sh(1), $BASH_SOURCE=sbin/start-dfs.sh
start-dfs.sh(1), $bin=/opt/hadoop-2.6.0/sbin
hdfs-config.sh,$0=sbin/start-dfs.sh
hdfs-config.sh, $bin=/opt/hadoop-2.6.0/sbin/start-dfs.sh
hdfs-config.sh(1), $bin=/opt/hadoop-2.6.0/sbin
hadoop-config.sh, $BASH_SOURCE=/opt/hadoop-2.6.0/sbin/../libexec/hadoop-config.sh
hadoop-config.sh, $0=sbin/start-dfs.sh
hadoop-config.sh(1), $this=/opt/hadoop-2.6.0/libexec/hadoop-config.sh
hdfs-config.sh(2), $bin=/opt/hadoop-2.6.0/sbin
start-dfs.sh(2), $bin=/opt/hadoop-2.6.0/sbin
start-dfs.sh(3), $bin=/opt/hadoop-2.6.0/sbin
Starting namenodes on [
bin/hdfs(1):	/opt/hadoop-2.6.0/bin
hdfs-config.sh,$0=/opt/hadoop-2.6.0/bin/hdfs
hdfs-config.sh, $bin=/opt/hadoop-2.6.0/bin/hdfs
hdfs-config.sh(1), $bin=/opt/hadoop-2.6.0/bin
hadoop-config.sh, $BASH_SOURCE=/opt/hadoop-2.6.0/bin/../libexec/hadoop-config.sh
hadoop-config.sh, $0=/opt/hadoop-2.6.0/bin/hdfs
				hadoop-config.sh(1), $this=/opt/hadoop-2.6.0/libexec/hadoop-config.sh
		hdfs-config.sh(2), $bin=/opt/hadoop-2.6.0/bin
localhost]
hadoop-config.sh, $BASH_SOURCE=/opt/hadoop-2.6.0/sbin/../libexec/hadoop-config.sh
hadoop-config.sh, $0=/opt/hadoop-2.6.0/sbin/hadoop-daemons.sh
				hadoop-config.sh(1), $this=/opt/hadoop-2.6.0/libexec/hadoop-config.sh
hadoop-config.sh, $BASH_SOURCE=/opt/hadoop-2.6.0/sbin/../libexec/hadoop-config.sh
hadoop-config.sh, $0=/opt/hadoop-2.6.0/sbin/slaves.sh
				hadoop-config.sh(1), $this=/opt/hadoop-2.6.0/libexec/hadoop-config.sh
sed: -e expression #1, char 9: unknown option to `s'

/ 7	. 

/ we doen met de hand	, omdat bashdb er niet invalt	,

bash-4.3# bashdb /opt/hadoop-2.6.0/bin/hdfs getconf -namenodes
bashdb<2> ev bin=/opt/hadoop-2.6.0/bin/hdfs

38:	. $HADOOP_LIBEXEC_DIR/hdfs-config.sh
bashdb<10> s
(/opt/hadoop-2.6.0/libexec/hdfs-config.sh:21):
bashdb<16> ev bin=/opt/hadoop-2.6.0/bin/hdfs

bashdb<159> pr $COMMAND
getconf
 CLASS=org.apache.hadoop.hdfs.tools.GetConf
280:	  exec "$JAVA" -Dproc_$COMMAND $JAVA_HEAP_MAX $HADOOP_OPTS $CLASS "$@"

/ we doen zelf	,
$ vi /tmp/my.bash
export CLASSPATH='/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/*:/opt/hadoop-2.6.0/share/hadoop/yarn/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/*:/etc/alternatives/java_sdk/lib/tools.jar:/opt/hadoop-2.6.0/contrib/capacity-scheduler/*.jar'

java -Dproc_getconf -Xmx1000m  -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop-2.6.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop-2.6.0 -Dhadoop.id.str= -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop-2.6.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.hdfs.tools.GetConf -namenodes

bash-4.3# (cd bin;/tmp/my.bash)
localhost

/ 7	.

/dan in start-dfs.sh

"$HADOOP_PREFIX/sbin/hadoop-daemons.sh" \
  --config "$HADOOP_CONF_DIR" \
  --hostnames "$NAMENODES" \
  --script "$bin/hdfs" start namenode $nameStartOpt

/ dat is	,

+ /opt/hadoop-2.6.0/sbin/hadoop-daemons.sh --config /opt/hadoop-2.6.0/etc/hadoop --hostnames 'localhost' --script /opt/hadoop-2.6.0/sbin/hdfs start namenode

35:	exec "$bin/slaves.sh" --config $HADOOP_CONF_DIR cd "$HADOOP_PREFIX" \; "$bin/hadoop-daemon.sh" --config $HADOOP_CONF_DIR "$@"


/ dat is	,

++ CLASSPATH='/opt/hadoop-2.6.0/etc/hadoop:/opt/hadoop-2.6.0/share/hadoop/common/lib/*:/opt/hadoop-2.6.0/share/hadoop/common/*:/opt/hadoop-2.6.0/share/hadoop/hdfs:/opt/hadoop-2.6.0/share/hadoop/hdfs/lib/*:/opt/hadoop-2.6.0/share/hadoop/hdfs/*:/opt/hadoop-2.6.0/share/hadoop/yarn/lib/*:/opt/hadoop-2.6.0/share/hadoop/yarn/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/lib/*:/opt/hadoop-2.6.0/share/hadoop/mapreduce/*:/etc/alternatives/java_sdk/lib/tools.jar:/opt/hadoop-2.6.0/contrib/capacity-scheduler/*.jar'

+ exec /opt/hadoop-2.6.0/sbin/slaves.sh --config /opt/hadoop-2.6.0/etc/hadoop cd /opt/hadoop-2.6.0 ';' /opt/hadoop-2.6.0/sbin/hadoop-daemon.sh --config /opt/hadoop-2.6.0/etc/hadoop --script /opt/hadoop-2.6.0/sbin/hdfs start namenode

+ ssh localhost cd /opt/hadoop-2.6.0 ';' /opt/hadoop-2.6.0/sbin/hadoop-daemon.sh --config /opt/hadoop-2.6.0/etc/hadoop --script /opt/hadoop-2.6.0/sbin/hdfs start namenode
localhost: Error: JAVA_HOME is not set and could not be found.
bash-4.3# ssh localhost


/7	.

bash-4.3# bashdb sbin/hadoop-daemons.sh --config /opt/hadoop-2.6.0/etc/hadoop --hostnames 'localhost' --script /opt/hadoop-2.6.0/sbin/hdfs start namenode
/ OK, geen $0-problems	,
33:	. $HADOOP_LIBEXEC_DIR/hadoop-config.sh
/ OK, geen $0-problems	,


/ Einde DEBUG HADOOP SCRIPTS TODO (RM PARTS)

/ DOCKER SUPERVISORD 

/ we kunnen in een Dockerfile NIET	,
run /usr/sbin/sshd
cmd /usr/bin/bash
/ een daemon sshd gaat niet	,

/ we moeten dit doen met supervisord	,

[eric@localhost ssh]$ pwd
/home/eric/Devel/Docker/supervisord
[eric@localhost supervisord]$ cat Dockerfile 

from fedora
run yum -y install openssh-server openssh-clients openssh
run yum -y install supervisor
run yum clean all

run echo "PermitRootLogin yes">>/etc/ssh/sshd_config

run ssh-keygen -A
run ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
run cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys

expose 22

run awk -i inplace '$0 ~ "daemon"{print gensub("false","true","g")}$0 !~ "daemon"' /etc/supervisord.conf
run echo >>/etc/supervisord.conf
run echo "[program:sshd]" >>/etc/supervisord.conf
run echo "command=/usr/sbin/sshd -D" >> /etc/supervisord.conf

cmd /usr/bin/supervisord -c /etc/supervisord.conf

[eric@localhost supervisord]$ sudo docker build -t test/supervisor .

[eric@localhost supervisord]$ sudo docker run -it --rm --name sv -p 22 test/supervisor
2015-03-07 15:43:48,365 CRIT Supervisor running as root (no user in config file)
2015-03-07 15:43:48,398 INFO RPC interface 'supervisor' initialized
2015-03-07 15:43:48,398 CRIT Server 'unix_http_server' running without any HTTP authentication checking
2015-03-07 15:43:48,399 INFO supervisord started with pid 1
2015-03-07 15:43:49,402 INFO spawned: 'sshd' with pid 9
2015-03-07 15:43:50,404 INFO success: sshd entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)

/we doen in een ander window	,
[eric@localhost fedora]$ sudo docker port sv 22
0.0.0.0:49195
/ TODO


 



/ Intermezzo

/ Lees	,
http://stackoverflow.com/questions/28336304/how-to-configure-docker-container-to-run-usr-sbin-sshd-upon-startup

I have a docker container, which I need to run as a deamon with -d flag. Is there a way to specify, that I want to run /usr/sbin/sshd as a startup process for this container? I have tried this, but my container did not stay around: sudo docker run -p 9000:9000 -d me/my-container /usr/sbin/sshd

So within a docker container, you'll want sshd to actually run in the foreground (not as a daemon), because docker itself will treat the container as a daemon. You'll also need to make sure sshd uses the right port. So try:
sudo docker run -p 9000:9000 -d me/mycontainer /usr/sbin/sshd -p 9000 -D

/ Einde Intermezzo


/ Einde DOCKER SUPERVISORD 

/ DOCKER OPENSSH

/ 7	. 

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/openssh/fedora

from fedora
run yum -y install openssh-server openssh-clients openssh
run yum clean all

#run echo 'root:walnoot' | chpasswd

run echo "PermitRootLogin yes">>/etc/ssh/sshd_config

run ssh-keygen -A

expose 22

cmd /usr/sbin/sshd -D

[eric@localhost fedora]$ sudo docker build -t test/openssh .
[eric@localhost fedora]$ sudo docker run -P --name openssh --rm  test/openssh
/ OK

/ In ander window	,
[eric@localhost fedora]$ sudo docker port openssh
22/tcp -> 0.0.0.0:49204
[eric@localhost fedora]$ ssh root@localhost -p 49204

The authenticity of host '[localhost]:49204 ([127.0.0.1]:49204)' can't be established.
ECDSA key fingerprint is da:74:92:bd:61:19:18:19:12:26:b2:fc:40:47:3f:3b.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[localhost]:49204' (ECDSA) to the list of known hosts.
/ 1 keer per container	,

root@localhost's password: walnoot
-bash-4.3# exit

[eric@localhost fedora]$ ssh root@localhost -p 49204
root@localhost's password: 
Last login: Sun Mar  8 15:12:26 2015 from 172.17.42.1
-bash-4.3# yum install net-tools
-bash-4.3# ifconfig
eth0: flags=67<UP,BROADCAST,RUNNING>  mtu 1500
        inet 172.17.1.192  netmask 255.255.0.0  broadcast 0.0.0.0
lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0

/ 7	. 

[eric@localhost ubuntu]$ pwd
/home/eric/Devel/Docker/postgresql/ubuntu

/ Lees	,
https://docs.docker.com/examples/running_ssh_service 
$ vi Dockerfile
...
[eric@localhost fedora]$ sudo docker build -t test/openssh-ubuntu .
[eric@localhost fedora]$ sudo docker run -P --name openssh --rm  test/openssh-ubuntu
/ OK

/ 7	.

[eric@localhost fedora]$ ssh root@172.17.42.1
/ is naar de laptop	,

/ 7	.

/ 13	. 

[eric@localhost fedora]$ sudo su -
[root@localhost ~]# ls ~/.ssh
ls: cannot access /root/.ssh: No such file or directory
[root@localhost ~]# ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
[root@localhost ~]# ls ~/.ssh
id_dsa  id_dsa.pub

/ 13	. 

run ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
run cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
/ Deze in Dockerfile alleen als je in de container # ssh localhost wilt doen	, password less	,

/ 7	. 

/ Lees	,
http://hortonworks.com/kb/generating-ssh-keys-for-passwordless-login/

/ Dit is niet wat we willen , maar het lukt NIET	,
[eric@localhost fedora]$ sudo su -
# ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
[root@localhost ~]# scp -P 49208 ~/.ssh/id_dsa.pub root@localhost:~/.ssh/id_dsa.pub
The authenticity of host '[localhost]:49208 ([127.0.0.1]:49208)' can't be established.
ECDSA key fingerprint is da:74:92:bd:61:19:18:19:12:26:b2:fc:40:47:3f:3b.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[localhost]:49208' (ECDSA) to the list of known hosts.
root@localhost's password: 
scp: /root/.ssh/id_dsa.pub: No such file or directory
/ TODO

/ 7	. 

/////////////////////////////////////////////////////
/ password less login in container,

/ Lees	,
http://hortonworks.com/kb/generating-ssh-keys-for-passwordless-login/
http://crosbymichael.com/setup-password-less-login-over-ssh.html

/ Eerst op de laptop	, als root	,
[root@localhost ~]# ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa

/we edit .ssh/authorized_keys op de target met de hand	,
/ moet anders	, 
/ TODO

[eric@localhost fedora]$ sudo docker build -t test/openssh .

[eric@localhost fedora]$ sudo docker run -P --name openssh --rm  test/openssh
...
/ In andere bash	,
[eric@localhost Docker]$ sudo docker port openssh
22/tcp -> 0.0.0.0:49210
[eric@localhost fedora]$ sudo su -
[root@localhost ~]# ssh root@localhost -p 49209
The authenticity of host '[localhost]:49209 ([127.0.0.1]:49209)' can't be established.
ECDSA key fingerprint is da:74:92:bd:61:19:18:19:12:26:b2:fc:40:47:3f:3b.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[localhost]:49209' (ECDSA) to the list of known hosts.
Last login: Sun Mar  8 18:08:19 2015 from 172.17.42.1
-bash-4.3#  mkdir .ssh
-bash-4.3# vi .ssh/authorized_keys
/ copy .ssh/id_dsa.pub hierin , met de hand	,

-bash-4.3# chmod 644 .ssh/authorized_keys 
/ TODO (Nodig?)
/ WH NIET	,

-bash-4.3# exit

/ op laptop, als root	,  want we hebben de public dsa van root copy over	, sudo is niet OK	,
[root@localhost ~]# ssh root@localhost -p 49209
Last login: Sun Mar  8 18:10:10 2015 from 172.17.42.1
-bash-4.3# exit

/	7	.

/ PASSWORDLESS SSH DOCKER

/ op laptop	, als root	,
[root@localhost ~]# ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/openssh/fedora
[eric@localhost fedora]$ sudo cp /root/.ssh/id_dsa.pub .
/ want copy in Dockerfile is rel to .	,

[eric@localhost fedora]$ vi ../../openssh/fedora/Dockerfile 

from fedora
run yum -y install openssh-server openssh-clients openssh
run yum clean all

#run echo 'root:walnoot' | chpasswd

run echo "PermitRootLogin yes">>/etc/ssh/sshd_config

run ssh-keygen -A

expose 22

run mkdir ~/.ssh
copy id_dsa.pub /tmp/
run cat  /tmp/id_dsa.pub  >> ~/.ssh/authorized_keys

cmd /usr/sbin/sshd -D

/ is nodig	,
run mkdir ~/.ssh
/ TODO

/ werkt rsa ipv dsa ook	?
/ TODO

[eric@localhost fedora]$ sudo docker build -t test/openssh .
[eric@localhost fedora]$ sudo docker run -P --name openssh --rm  test/openssh

/ ander bash	,
[root@localhost ~]# docker port openssh
22/tcp -> 0.0.0.0:49211
[root@localhost ~]# ssh root@localhost -p 49211
The authenticity of host '[localhost]:49211 ([127.0.0.1]:49211)' can't be established.
ECDSA key fingerprint is da:74:92:bd:61:19:18:19:12:26:b2:fc:40:47:3f:3b.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[localhost]:49211' (ECDSA) to the list of known hosts.
-bash-4.3# exit
logout
Connection to localhost closed.
[root@localhost ~]# ssh root@localhost -p 49211
Last login: Thu Mar 12 16:34:33 2015 from 172.17.42.1
-bash-4.3# 


/ Einde PASSWORDLESS SSH DOCKER








/ 7	.

/ we geven een ander cmd	, die overwrites die uit Dockerfile	,
[eric@localhost openssh]$ sudo docker run -P --name openssh --rm test/openssh /usr/bin/bash
bash-4.3# /usr/sbin/sshd 
bash-4.3# vi ~/.ssh/id_dsa.pub 
...  root@c4e263145f81
/ TODO

[eric@localhost fedora]$  sudo docker inspect --format='{{.NetworkSettings.IPAddress}}' openssh
172.17.1.172
/ TODO

 7	. 

/ we set Dockerfile hierboven in openssh/fedora	, en maken openssh/ubuntu	,
/ lees	,
https://docs.docker.com/examples/running_ssh_service/
/ Neem deze Dockerfile	, 
[eric@localhost ubuntu]$ sudo docker run -P --name openssh-ubuntu --rm  test/openssh-ubuntu 
 we set Dockerfile hierboven in openssh/fedora	, en maken openssh/ubuntu	,
/ lees	,
https://docs.docker.com/examples/running_ssh_service/
/ Neem deze Dockerfile	, 
$ vi Dockerfile
...
RUN echo 'root:screencast' | chpasswd

[eric@localhost ubuntu]$ sudo docker run -P --name openssh-ubuntu --rm  test/openssh-ubuntu 
/ OK

/ In ander window	,
[eric@localhost fedora]$ sudo docker port openssh-ubuntu
22/tcp -> 0.0.0.0:49202
[eric@localhost fedora]$ ssh root@localhost -p 49202
root@localhost's password: screencast
root@76ecd9c47286:~#  
/ OK

/ 76ecd9c47286 is de container id	, 
/ kunnen we niet use voor ssh 	, in ssh root@76ecd9c47286	,

[eric@localhost fedora]$ sudo docker ps -a
CONTAINER ID        IMAGE                        COMMAND               CREATED             STATUS              PORTS                   NAMES
76ecd9c47286        test/openssh-ubuntu:latest   "/usr/sbin/sshd -D"   9 minutes ago       Up 9 minutes        0.0.0.0:49202->22/tcp   openssh-ubuntu      



/ Einde DOCKER OPENSSH

/ DOCKER OPENSSH SUPERVISORD

/ we kunnen run test/openssh of test/supervisor	,
/ In beide gevallen draait sshd	,
/ In test/openssh kan root passwordless login	, we kunnen bovenop test/supervisor een images build, openssh_alt	, die dat ook doet	,

[eric@localhost supervisord]$ sudo docker run --rm -P --name sv test/supervisor
2015-03-13 15:53:50,180 CRIT Supervisor running as root (no user in config file)
2015-03-13 15:53:50,207 INFO RPC interface 'supervisor' initialized
2015-03-13 15:53:50,207 CRIT Server 'unix_http_server' running without any HTTP authentication checking
2015-03-13 15:53:50,208 INFO supervisord started with pid 1
2015-03-13 15:53:51,211 INFO spawned: 'sshd' with pid 8
2015-03-13 15:53:52,213 INFO success: sshd entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)

/ In ander window	,
[eric@localhost fedora]$ sudo docker port sv
22/tcp -> 0.0.0.0:49212
[eric@localhost fedora]$ sudo su -
[root@localhost ~]# ssh localhost -p 49212
...
/ root kan niet login	, want  er is NIET in Dockerfile van supervisor	, of van een child	,
run awk -i inplace '{print gensub("(#)(PermitRootLogin yes)","\\2","g")}' /etc/ssh/sshd_config
run ssh-keygen -A
/ en voor password less login, moeten we ook in Dockerfile	 van supervisor	, of van een child	,,
run mkdir ~/.ssh
copy id_dsa.pub /tmp/
run cat  /tmp/id_dsa.pub  >> ~/.ssh/authorized_keys
/ waarbij id_dsa.pub uit /root/.ssh/ komt	, hier op de laptop gen, en copied naar .	,


[eric@localhost supervisord]$ sudo docker run --rm -P --name os test/openssh

/ In ander window	,






/ Einde DOCKER OPENSSH SUPERVISORD

/ DOCKER TMP

https://github.com/sequenceiq/hadoop-docker/blob/master/Dockerfile
run service ... start
/ is centos	,
/ TODO

https://registry.hub.docker.com/u/lab41/dendrite-cdh5/dockerfile/
run service ... start
/ is ubuntu	,
/ TODO





/ DOCKER TMP

/ Einde DOCKER TMP



/ DOCKER

/ 7	.

/ 13	. 

/ stop alle containers	,
[eric@localhost ssh]$ for i in $(awk '{print $1}' <(sudo docker ps -a)); do sudo docker rm $(sudo docker stop $i); done
/ TODO (1ste line overslaan , met CONTAINER)

/ we moeten	,
$ for i in $(awk ...);do ...
/ maar 
$ cat <(awk ...)
/ TODO

/ 13	. 

/ rm <none> images	,

/ print id's, tel	,
[eric@localhost ssh]$ awk '$1 ~ "<none>"{print $3;n++}END{print n}' <(sudo docker images -a)
...
125
/ of	,
[eric@localhost ssh]$ wc <(awk '$1 ~ "<none>"{print $3}' <(sudo docker images -a))
    125     125    1625 /dev/fd/63

[eric@localhost ssh]$ for i in $(awk '$1 ~ "<none>"{print $3}' <(sudo docker images -a)); do echo $i; done 
/ OK

[eric@localhost ssh]$ wc <(for i in $(awk '$1 ~ "<none>"{print $3}' <(sudo docker images -a)); do echo $i; done)
    125     125    1625 /dev/fd/63

[eric@localhost ssh]$ for i in $(awk '$1 ~ "<none>"{print $3}' <(sudo docker images -a)); do sudo docker rmi $i; done
/ TODO
/ Lees	,
https://github.com/docker/docker/issues/6919



/ Einde DOCKER

/ 7	 

/ AWK

/ we willen alle <none> containers rm	,

/ we oef	,
[eric@localhost fedora]$ awk '!($2~/<none>/){print $2, $3}' <(sudo docker images -a )
/ of	,
[eric@localhost fedora]$ awk '$2!~/<none>/{print $2, $3}' <(sudo docker images -a )
TAG IMAGE
latest f7d22d28e38b
latest f18cfa8085d8
v1 11cd9e0cdf84
v2 66d64dc5a36c
14.04 5ba9dab47459
latest 5ba9dab47459
12.04 69c02692b0c1
latest 834629358fe2
21 834629358fe2
latest f0f4ab557f95
latest 258105bea10d
latest 31fa814ba25a
/ deze willen we houden	,

[eric@localhost fedora]$ awk '$2~/<none>/{print $2,$3}' <(sudo docker images -a )
/ of	,
[eric@localhost fedora]$ awk ' $2~"<none>"{print $2,$3}' <(sudo docker images -a )
/ deze willen we rm	,

[eric@localhost fedora]$ awk ' $2~/<none>/{count++}END{print count}' <(sudo docker images -a ) 
50
[eric@localhost fedora]$ awk ' $2!~/<none>/{count++}END{print count}' <(sudo docker images -a ) 
13
[eric@localhost fedora]$ awk '{count++}END{print count}' <(sudo docker images -a ) 
63

/ MORGEN op 1 line	,

/ 7	.

[eric@localhost own]$ cat awk.txt 
eric en foo
saskia en foo 

[eric@localhost own]$ awk '{r=gensub(/foo/,"Foo","g",$0);print r}' awk.txt 
eric en Foo
saskia en Foo 
/ of	,
[eric@localhost own]$ awk   '{gsub(/foo/,"Foo");print}' awk.txt 
eric en Foo
saskia en Foo 

/ in gensub is laatste arg default $0	, die kunnen we dus weglaten	,
/ ook doen we print gensub(...)

/ Nu alleen foo->Foo waar eric staat	,

[eric@localhost own]$ awk  '$0 ~ "eric"{print gensub("foo","Foo","g")}$0 !~ "eric"' awk.txt 
eric en Foo
saskia en foo

/ vervang	,
[eric@localhost own]$ awk -i inplace  '$0 ~ "eric"{print gensub("foo","Foo","g")}$0 !~ "eric"' awk.txt 
[eric@localhost own]$ cat awk.txt 
eric en Foo
saskia en foo




/ Einde AWK

/ SYSTEMD IN DOCKER

/ 7	. 

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/systemd/fedora
$ vi Dockerfile

from fedora:21 
env container docker
run (cd /lib/systemd/system/sysinit.target.wants/;\
for i in *; do [ $i == systemd-tmpfiles-setup.service ] || rm -f $i; done);\
rm -f /lib/systemd/system/multi-user.target.wants/*;\
rm -f /etc/systemd/system/*.wants/*;\
rm -f /lib/systemd/system/local-fs.target.wants/*;\
rm -f /lib/systemd/system/sockets.target.wants/*udev*;\
rm -f /lib/systemd/system/sockets.target.wants/*initctl*;\
rm -f /lib/systemd/system/basic.target.wants/*;\
rm -f /lib/systemd/system/anaconda.target.wants/*
volume [ "/sys/fs/cgroup" ]
cmd [ "/usr/sbin/init" ]

[eric@localhost fedora]$ sudo docker build -t test/systemd .
/ OK

[eric@localhost fedora]$  sudo docker run --rm -it test/systemd
Failed to mount tmpfs at /run: Operation not permitted

[eric@localhost httpd]$ pwd
/home/eric/Devel/Docker/httpd
[eric@localhost httpd]$ cat Dockerfile 

from test/systemd
run yum -y install httpd;yum clean all;systemctl enable httpd.service
expose 80
cmd ["/usr/sbin/init"]

[eric@localhost httpd]$ sudo docker build -t test/httpd .

Installing:
 httpd                    x86_64       2.4.10-9.fc21         fedora       1.2 M
Installing for dependencies:
 apr                      x86_64       1.5.1-3.fc21          fedora       111 k
 apr-util                 x86_64       1.5.4-1.fc21          fedora        96 k
 fedora-logos-httpd       noarch       21.0.5-1.fc21         fedora        33 k
 httpd-filesystem         noarch       2.4.10-9.fc21         fedora        23 k
 httpd-tools              x86_64       2.4.10-9.fc21         fedora        84 k
 mailcap                  noarch       2.1.43-1.fc21         fedora        36 k

Complete!
Cleaning repos: fedora updates
Cleaning up everything
Created symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service.
 ---> 7f1cfd88d586
Removing intermediate container 15a73adee8ec
Step 2 : EXPOSE 80
 ---> Running in 07a7d7bc69c2
 ---> a79dcfe7af58
Removing intermediate container 07a7d7bc69c2
Step 3 : CMD /usr/sbin/init
 ---> Running in 2485780ad316
 ---> eaa8f378b3e0
Removing intermediate container 2485780ad316
Successfully built eaa8f378b3e0

[eric@localhost httpd]$  sudo docker run --privileged -t  -v /sys/fs/cgroup:/sys/fs/cgroup:ro -p 80:80 test/httpd
systemd 217 running in system mode. (+PAM +AUDIT +SELINUX +IMA -APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ -LZ4 +SECCOMP +BLKID +ELFUTILS +KMOD +IDN )
Detected virtualization 'docker'.
Detected architecture 'x86-64'.

/ door -t zien we 	, (zonder -t zien we niets	, maar doet het OK)	,

Welcome to Fedora 21 (Twenty One)!

Set hostname to <61255ebe48d6>.
Running in a container, ignoring fstab device entry for /dev/disk/by-uuid/3574b897-0550-4274-8d31-089b244b7476.
[  OK  ] Reached target Paths.
[  OK  ] Created slice Root Slice.
[  OK  ] Listening on Delayed Shutdown Socket.
[  OK  ] Listening on Journal Socket.
[  OK  ] Created slice System Slice.
[  OK  ] Reached target Slices.
[  OK  ] Reached target Swap.
[  OK  ] Reached target Local File Systems.
         Starting Create Volatile Files and Directories...
[  OK  ] Listening on Journal Socket (/dev/log).
         Starting Journal Service...
[  OK  ] Started Create Volatile Files and Directories.
[ INFO ] Update UTMP about System Boot/Shutdown is not active.
[DEPEND] Dependency failed for Update UTMP about System Runlevel Changes.
[  OK  ] Started Journal Service.
[  OK  ] Reached target System Initialization.
[  OK  ] Reached target Timers.
[  OK  ] Listening on D-Bus System Message Bus Socket.
[  OK  ] Reached target Sockets.
[  OK  ] Reached target Basic System.
         Starting The Apache HTTP Server...
         Starting Cleanup of Temporary Directories...
[  OK  ] Started Cleanup of Temporary Directories.
[  OK  ] Started The Apache HTTP Server.
[  OK  ] Reached target Multi-User System.

/ Geef in chrome
http://localhost
/ OK

[eric@localhost fedora]$ sudo docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                       PORTS               NAMES
61255ebe48d6        test/httpd:latest   "/usr/sbin/init"    4 minutes ago       Exited (-1) 35 seconds ago                       clever_hopper  

/ In ander window	,
[eric@localhost fedora]$ sudo docker stop clever_hopper
clever_hopper
/ OK
[eric@localhost fedora]$ sudo docker rm clever_hopper
clever_hopper
/ OK

/ Einde SYSTEMD IN DOCKER

/ DOCKER WITH SERVICE AND BASH


[eric@localhost centos6]$ pwd
/home/eric/Devel/Docker/openssh/centos6

[eric@localhost centos6]$ cat my.bash 
#!/bin/bash
service sshd start
/bin/bash

[eric@localhost centos6]$ cat Dockerfile 
from centos:6
run yum -y install openssh-server openssh-clients openssh 
run yum clean all

#run echo 'root:walnoot' | chpasswd

run echo "PermitRootLogin yes">>/etc/ssh/sshd_config
#run awk -i inplace '{print gensub("(#)(PermitRootLogin yes)","\\2","g")}' /etc/ssh/sshd_config

#run ssh-keygen -A
run ssh-keygen -N "" -t dsa -f /etc/ssh/ssh_host_dsa_key
run ssh-keygen -N "" -t rsa -f /etc/ssh/ssh_host_rsa_key
run ssh-keygen -N "" -t rsa -f ~/.ssh/id_rsa
run cat /root/.ssh/id_rsa.pub >>~/.ssh/authorized_keys

expose 22

#run mkdir ~/.ssh
copy id_dsa.pub /tmp/
run cat  /tmp/id_dsa.pub  >> ~/.ssh/authorized_keys

add my.bash /etc/my.bash
run chown root:root /etc/my.bash
run chmod 700 /etc/my.bash

#cmd /usr/sbin/sshd -D
#cmd /usr/bin/bash
cmd ["/etc/my.bash"]

/ ssh-keygen -A werkt niet in centos:6
/ ssh-keygen -f ~/.ssh/id_rsa creates de /root/.ssh dir	,
/ we kunnen ook: cmd /etc/my.bash

[eric@localhost centos6]$ sudo docker build -t test/openssh .
[eric@localhost centos6]$ sudo docker run --rm --name openssh -i -t test/openssh
[root@06ff76b72086 /]# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?           39 Ss       0   0:00 /bin/bash /etc/my.bash
    1    28    28    28 ?           -1 Ss       0   0:00 /usr/sbin/sshd
    1    30    30     1 ?           39 S        0   0:00 /bin/bash
   30    39    39     1 ?           39 R+       0   0:00 ps ajx
[root@06ff76b72086 /]# ssh localhost

/ 7	. 

[eric@localhost centos7]$ pwd
/home/eric/Devel/Docker/systemd/centos7

/ als we 	, dan knalt fedora eruit	,
/ TODO
$ vi Dockerfile

FROM centos:7
ENV container docker
RUN yum -y swap -- remove fakesystemd -- install systemd systemd-libs
RUN yum -y update; yum clean all
VOLUME [ "/sys/fs/cgroup" ]
CMD ["/usr/sbin/init"]


/ 7	.  

[eric@localhost centos7]$ pwd
/home/eric/Devel/Docker/systemd/centos7


/ 13	.

/ lees	,
https://registry.hub.docker.com/_/centos/

/ in systemd/
[eric@localhost centos7]$ sudo docker build --rm -t local/c7-systemd .

/ in httpd/
[eric@localhost centos7]$ sudo docker build --rm -t local/c7-systemd-httpd .
[eric@localhost centos7]$ sudo docker run --rm --name httpd --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -p 80:80 local/c7-systemd-httpd

/ als we rm	 in systemd's Dockerfile,
ENV container docker
/ dan zien we in httpd geen log  van [OK] ...	, maar hij doet het wel	,
/ TODO

/ we kunnen rm	 in systemd's Dockerfile,
VOLUME [ "/sys/fs/cgroup" ]
/ TODO

/ we kunnen rm	 in systemd's Dockerfile,
cmd ["/usr/sbin/init"]
/ omdat we toch systemd include in httpd	,


/ in httpd/	,
[eric@localhost centos7]$ vi Dockerfile 

$ sudo docker run --rm --name httpd --privileged -t -i -v /sys/fs/cgroup:/sys/fs/cgroup:ro -p 80:80 test/httpd

/we zien in centos7 container	, 
[root@2ae811386093 /]# ls /usr/sbin/init  -l
lrwxrwxrwx. 1 root root 22 Mar 20 21:24 /usr/sbin/init -> ../lib/systemd/systemd








/ Einde DOCKER WITH SERVICE AND BASH

/ DOCKER SYSTEMD 1 

/ systemd, httpd	,
/ sshd zit in httpd	, werkt OK	, maar see DOCKER SYSTEMD 2, gescheiden sshd en httpd	, BETER

/ 7	. 

/ systemd	,

[eric@localhost centos7]$ pwd
/home/eric/Devel/Docker/systemd/centos7
[eric@localhost centos7]$ cat Dockerfile

FROM centos:7
ENV container docker
RUN yum -y swap -- remove fakesystemd -- install systemd systemd-libs
RUN yum -y update; yum clean all; \
(cd /lib/systemd/system/sysinit.target.wants/; for i in *; do [ $i == systemd-tmpfiles-setup.service ] || rm -f $i; done); \
rm -f /lib/systemd/system/multi-user.target.wants/*;\
rm -f /etc/systemd/system/*.wants/*;\
rm -f /lib/systemd/system/local-fs.target.wants/*; \
rm -f /lib/systemd/system/sockets.target.wants/*udev*; \
rm -f /lib/systemd/system/sockets.target.wants/*initctl*; \
rm -f /lib/systemd/system/basic.target.wants/*;\
rm -f /lib/systemd/system/anaconda.target.wants/*;

[eric@localhost centos7]$ sudo docker build --rm -t local/c7-systemd .

/ 7	. 

/ sshd, httpd	,

[eric@localhost centos7_1]$ pwd
/home/eric/Devel/Docker/httpd/centos7_1

[eric@localhost centos7_1]$ ls
Dockerfile  id_dsa.pub

[eric@localhost centos7_1]$ cat Dockerfile 

FROM local/c7-systemd
RUN yum -y install httpd; yum -y install openssh-server openssh-clients;yum clean all; systemctl enable httpd.service; systemctl enable sshd.service

add my.bash /etc/my.bash
run chown root.root /etc/my.bash
run chmod 700 /etc/my.bash

run echo "PermitRootLogin yes">>/etc/ssh/sshd_config
run ssh-keygen -A

run ssh-keygen -N "" -t rsa -f ~/.ssh/id_rsa
run cat /root/.ssh/id_rsa.pub >>~/.ssh/authorized_keys

expose 80 22

cmd /usr/lib/systemd/systemd


[eric@localhost centos7_1]$ sudo docker build --rm -t local/c7-systemd-httpd .

Successfully built 9d5cab6634ad
[eric@localhost ]$ sudo docker run --rm --name httpd --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -P local/c7-systemd-httpd
systemd 208 running in system mode. (+PAM +LIBWRAP +AUDIT +SELINUX +IMA +SYSVINIT +LIBCRYPTSETUP +GCRYPT +ACL +XZ)
Detected virtualization 'other'.

Welcome to CentOS Linux 7 (Core)!

Set hostname to <86a471aa98d2>.
Cannot add dependency job for unit display-manager.service, ignoring: Unit display-manager.service failed to load: No such file or directory.
[  OK  ] Reached target Paths.
...

[eric@localhost ]$ sudo docker port httpd
22/tcp -> 0.0.0.0:49154
80/tcp -> 0.0.0.0:49155

[eric@localhost ]$ sudo su -
[root@localhost ~]# ssh localhost -p 49154
The authenticity of host '[localhost]:49154 ([127.0.0.1]:49154)' can't be established.
ECDSA key fingerprint is 19:32:10:fd:ed:73:42:98:85:2e:da:38:1d:55:68:02.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[localhost]:49154' (ECDSA) to the list of known hosts.
System is booting up. See pam_nologin(8)
[root@86a471aa98d2 ~]# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?           -1 Ss       0   0:00 /usr/lib/systemd/systemd
    1    14    14    14 ?           -1 Ss       0   0:00 /usr/lib/systemd/systemd-journald
    1    15    15    15 ?           -1 Ss       0   0:00 /usr/sbin/httpd -DFOREGROUND
    1    18    18    18 ?           -1 Ss       0   0:00 /usr/sbin/sshd -D
   15    19    15    15 ?           -1 S       48   0:00 /usr/sbin/httpd -DFOREGROUND
   15    20    15    15 ?           -1 S       48   0:00 /usr/sbin/httpd -DFOREGROUND
   15    21    15    15 ?           -1 S       48   0:00 /usr/sbin/httpd -DFOREGROUND
   15    22    15    15 ?           -1 S       48   0:00 /usr/sbin/httpd -DFOREGROUND
   15    23    15    15 ?           -1 S       48   0:00 /usr/sbin/httpd -DFOREGROUND
   18    26    26    26 ?           -1 Ss       0   0:00 sshd: root@pts/0
    1    28    28    28 ?           -1 Ss      81   0:00 /bin/dbus-daemon --system --address=systemd: --no
    1    29    29    29 ?           -1 Ss       0   0:00 /usr/lib/systemd/systemd-logind
   26    30    30    30 pts/0       46 Ss       0   0:00 -bash
   30    46    46    30 pts/0       46 R+       0   0:00 ps ajx
[root@86a471aa98d2 ~]#  ssh localhost
The authenticity of host 'localhost (::1)' can't be established.
ECDSA key fingerprint is 19:32:10:fd:ed:73:42:98:85:2e:da:38:1d:55:68:02.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
System is booting up. See pam_nologin(8)
Last login: Sat Mar 21 13:02:00 2015 from 172.17.42.1
[root@86a471aa98d2 ~]# exit
logout
Connection to localhost closed.
[root@86a471aa98d2 ~]#  ssh localhost
System is booting up. See pam_nologin(8)
Last login: Sat Mar 21 13:59:59 2015 from localhost


/ Einde DOCKER SYSTEMD 1

/ DOCKER SYSTEMD 2

/ 7	.

[eric@localhost centos7]$ sudo docker stop $(awk 'NR>1{print $1}' <(sudo docker ps -a))
[eric@localhost centos7]$ sudo docker rm $(awk 'NR>1{print $1}' <(sudo docker ps -a))

/ 7	. 

/ systemd

[eric@localhost centos7]$ pwd
/home/eric/Devel/Docker/systemd/centos7

[eric@localhost centos7]$ cat Dockerfile

FROM centos:7

ENV container docker

RUN yum -y swap -- remove fakesystemd -- install systemd systemd-libs
RUN yum -y update; yum clean all; \

(cd /lib/systemd/system/sysinit.target.wants/; for i in *; do [ $i == systemd-tmpfiles-setup.service ] || rm -f $i; done); \
rm -f /lib/systemd/system/multi-user.target.wants/*;\
rm -f /etc/systemd/system/*.wants/*;\
rm -f /lib/systemd/system/local-fs.target.wants/*; \
rm -f /lib/systemd/system/sockets.target.wants/*udev*; \
rm -f /lib/systemd/system/sockets.target.wants/*initctl*; \
rm -f /lib/systemd/system/basic.target.wants/*;\
rm -f /lib/systemd/system/anaconda.target.wants/*;

cmd /usr/lib/systemd/systemd

[eric@localhost centos7]$ sudo docker build --rm -t local/c7-systemd .

/ 7	. 

/ sshd

[eric@localhost centos7]$ pwd
/home/eric/Devel/Docker/sshd/centos7
[eric@localhost centos7]$ ls
Dockerfile  id_dsa.pub
[eric@localhost centos7]$ cat Dockerfile 

from local/c7-systemd 

run yum -y install openssh-server openssh-clients
run yum clean all
run systemctl enable sshd.service

run echo "PermitRootLogin yes">>/etc/ssh/sshd_config

run ssh-keygen -A

expose 22

run ssh-keygen -N "" -t rsa -f ~/.ssh/id_rsa
run cat /root/.ssh/id_rsa.pub >>~/.ssh/authorized_keys

run if [ ! -e ~/.ssh ];then mkdir ~/.ssh;fi
copy id_dsa.pub /tmp/
run cat  /tmp/id_dsa.pub  >> ~/.ssh/authorized_keys

cmd /usr/lib/systemd/systemd

[eric@localhost centos7]$ sudo docker build --rm -t local/c7-systemd-sshd .

[eric@localhost ]$ sudo docker run --rm --name sshd --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -P local/c7-systemd-sshd
systemd 208 running in system mode. (+PAM +LIBWRAP +AUDIT +SELINUX +IMA +SYSVINIT +LIBCRYPTSETUP +GCRYPT +ACL +XZ)
Detected virtualization 'other'.

Welcome to CentOS Linux 7 (Core)!

Set hostname to <dcd2b9e72266>.
Cannot add dependency job for unit display-manager.service, ignoring: Unit display-manager.service failed to load: No such file or directory.
[  OK  ] Reached target Paths.
...

[eric@localhost centos7]$ sudo docker port sshd
22/tcp -> 0.0.0.0:49157
[root@localhost ~]# ssh localhost -p 49157
The authenticity of host '[localhost]:49157 ([127.0.0.1]:49157)' can't be established.
ECDSA key fingerprint is 29:54:35:c1:a4:05:44:9c:9b:9b:ba:6d:5a:36:a6:16.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[localhost]:49157' (ECDSA) to the list of known hosts.
System is booting up. See pam_nologin(8)
[root@dcd2b9e72266 ~]# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?           -1 Ss       0   0:00 /usr/lib/systemd/systemd
    1    13    13    13 ?           -1 Ss       0   0:00 /usr/lib/systemd/systemd-journald
    1    16    16    16 ?           -1 Ss       0   0:00 /usr/sbin/sshd -D
   16    19    19    19 ?           -1 Rs       0   0:00 sshd: root@pts/0
    1    21    21    21 ?           -1 Ss      81   0:00 /bin/dbus-daemon --system --address=systemd
    1    22    22    22 ?           -1 Ss       0   0:00 /usr/lib/systemd/systemd-logind
   19    23    23    23 pts/0       39 Ss       0   0:00 -bash
   23    39    39    23 pts/0       39 R+       0   0:00 ps ajx

/ 7	.

[eric@localhost centos7]$ pwd
/home/eric/Devel/Docker/httpd/centos7
$ vi Dockerfile

FROM local/c7-systemd-sshd

RUN yum -y install httpd; yum clean all; systemctl enable httpd.service

expose 80

cmd /usr/lib/systemd/systemd

[eric@localhost centos7]$ sudo docker build --rm -t local/c7-systemd-sshd-httpd .

[eric@localhost ]$ sudo docker run --rm --name sshd-httpd --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -P local/c7-systemd-sshd-httpd
systemd 208 running in system mode. (+PAM +LIBWRAP +AUDIT +SELINUX +IMA +SYSVINIT +LIBCRYPTSETUP +GCRYPT +ACL +XZ)
Detected virtualization 'other'.

Welcome to CentOS Linux 7 (Core)!

Set hostname to <5e51cd73b10e>.
Cannot add dependency job for unit display-manager.service, ignoring: Unit display-manager.service failed to load: No such file or directory.
[  OK  ] Reached target Paths.
...

[eric@localhost ]$ sudo docker port sshd-httpd
22/tcp -> 0.0.0.0:49158
80/tcp -> 0.0.0.0:49159

[eric@localhost ]$ sudo su -
[root@localhost ~]# ssh localhost -p 49158
The authenticity of host '[localhost]:49158 ([127.0.0.1]:49158)' can't be established.
ECDSA key fingerprint is 29:54:35:c1:a4:05:44:9c:9b:9b:ba:6d:5a:36:a6:16.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[localhost]:49158' (ECDSA) to the list of known hosts.
System is booting up. See pam_nologin(8)
[root@5e51cd73b10e ~]# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?           -1 Ss       0   0:00 /usr/lib/systemd/systemd
    1    14    14    14 ?           -1 Ss       0   0:00 /usr/lib/systemd/systemd-journald
    1    15    15    15 ?           -1 Ss       0   0:00 /usr/sbin/httpd -DFOREGROUND
    1    18    18    18 ?           -1 Ss       0   0:00 /usr/sbin/sshd -D
   15    19    15    15 ?           -1 S       48   0:00 /usr/sbin/httpd -DFOREGROUND
   15    20    15    15 ?           -1 S       48   0:00 /usr/sbin/httpd -DFOREGROUND
   15    21    15    15 ?           -1 S       48   0:00 /usr/sbin/httpd -DFOREGROUND
   15    22    15    15 ?           -1 S       48   0:00 /usr/sbin/httpd -DFOREGROUND
   15    23    15    15 ?           -1 S       48   0:00 /usr/sbin/httpd -DFOREGROUND
   18    24    24    24 ?           -1 Rs       0   0:00 sshd: root@pts/0
    1    26    26    26 ?           -1 Ss      81   0:00 /bin/dbus-daemon --system --address=systemd
    1    27    27    27 ?           -1 Ss       0   0:00 /usr/lib/systemd/systemd-logind
   24    28    28    28 pts/0       44 Ss       0   0:00 -bash
   28    44    44    28 pts/0       44 R+       0   0:00 ps ajx

/ en geef in chrome	,
http://localhost:49159/
/ OK

/ 7	. 

/ als we de cmd in Dockerfile een script willen geven , en als de cmd systemd is, moeten we in het script exec 	,
/ anders krijgen we 	,

$ vi my.bash
#!/usr/bin/bash
/usr/sbin/init

/ we zien	,
Couldn't find an alternative telinit implementation to spawn.

$ vi my.bash
#!/usr/bin/bash
/usr/lib/systemd/systemd

/ we zien	,
Trying to run as user instance, but the system has not been booted with systemd.

[eric@localhost centos7]$ pwd
/home/eric/Devel/Docker/sshd/centos7
$ vi Dockerfile

cmd /usr/lib/systemd/systemd
/ OK	, maar willen vervangen door 	,

add my.bash /etc/my.bash
run chown root.root /etc/my.bash
run chmod 700 /etc/my.bash
cmd ["/etc/my.bash"]

$ vi my.bash
#!/usr/bin/bash
exec /usr/lib/systemd/systemd

[eric@localhost centos7]$ sudo docker build --rm -t local/c7-systemd-sshd .
[eric@localhost centos7]$ sudo docker run --rm --name sshd --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -P local/c7-systemd-sshd


[eric@localhost tmp]$ sudo su -
[root@localhost ~]# docker port sshd
22/tcp -> 0.0.0.0:49186
[root@localhost ~]# ssh localhost -p 49186
The authenticity of host '[localhost]:49186 ([127.0.0.1]:49186)' can't be established.
ECDSA key fingerprint is 29:54:35:c1:a4:05:44:9c:9b:9b:ba:6d:5a:36:a6:16.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[localhost]:49186' (ECDSA) to the list of known hosts.
System is booting up. See pam_nologin(8)
[root@1ebeb0195590 ~]# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?           -1 Ss       0   0:00 /usr/lib/systemd/systemd
    1    13    13    13 ?           -1 Ss       0   0:00 /usr/lib/systemd/systemd-journald
    1    16    16    16 ?           -1 Ss       0   0:00 /usr/sbin/sshd -D
   16    17    17    17 ?           -1 Rs       0   0:00 sshd: root@pts/0
    1    19    19    19 ?           -1 Ss      81   0:00 /bin/dbus-daemon --system --address=systemd: --
    1    20    20    20 ?           -1 Ss       0   0:00 /usr/lib/systemd/systemd-logind
   17    21    21    21 pts/0       37 Ss       0   0:00 -bash
   21    37    37    21 pts/0       37 R+       0   0:00 ps ajx

[eric@localhost tmp]$ sudo docker stop $(awk 'NR>1{print $1}' <(sudo docker ps -a))
d67f13be4a51


/ Einde DOCKER SYSTEMD 2

/ DOCKER NO SYSTEMD

/ 7		.

/ sshd

[eric@localhost centos7_2]$ pwd
/home/eric/Devel/Docker/sshd/centos7_2

[eric@localhost centos7_2]$ ls
Dockerfile  id_dsa.pub

[eric@localhost centos7_2]$ cat Dockerfile 

from centos:7 

run yum -y install openssh-server openssh-clients
run yum clean all

run echo "PermitRootLogin yes">>/etc/ssh/sshd_config

run ssh-keygen -A

run ssh-keygen -N "" -t rsa -f ~/.ssh/id_rsa
run cat /root/.ssh/id_rsa.pub >>~/.ssh/authorized_keys

run if [ ! -e ~/.ssh ];then mkdir ~/.ssh;fi
copy id_dsa.pub /tmp/
run cat  /tmp/id_dsa.pub  >> ~/.ssh/authorized_keys

expose 22

cmd /usr/sbin/sshd -D

[eric@localhost centos7_2]$ sudo docker build --rm -t local/c7-sshd .

[eric@localhost centos7_2]$ sudo docker run --rm --name sshd  -ti  -P local/c7-sshd
/ we zien niets	,

/ in ander window	, 
[eric@localhost centos7]$ sudo docker port sshd
22/tcp -> 0.0.0.0:49164
[eric@localhost centos7]$ sudo su -
[root@localhost ~]# ssh localhost -p 49164
The authenticity of host '[localhost]:49164 ([127.0.0.1]:49164)' can't be established.
ECDSA key fingerprint is 4e:aa:fc:49:69:3c:b2:a0:1b:0c:1b:9a:7f:4b:ff:6b.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[localhost]:49164' (ECDSA) to the list of known hosts.
[root@ac4e1a92f96f ~]# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?            1 Ss+      0   0:00 /usr/sbin/sshd -D
    1     7     7     7 ?           -1 Ss       0   0:00 sshd: root@pts/0
    7     9     9     9 pts/0       25 Ss       0   0:00 -bash
    9    25    25     9 pts/0       25 R+       0   0:00 ps ajx

/ 7	. 

/ httpd	,

/ we doen eerst in fedora (laptop)	,
[eric@localhost centos7_2]$ sudo yum install httpd
[eric@localhost centos7_2]$ sudo systemctl enable httpd.service 
Created symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service.
[eric@localhost centos7_2]$ sudo systemctl start httpd.service 

[eric@localhost centos7_2]$ pwd
/home/eric/Devel/Docker/httpd/centos7_2
$ vi Dockerfile

FROM local/c7-sshd
RUN yum -y install httpd; yum clean all

expose 80
cmd ["/usr/sbin/httpd","-DFOREGROUND"]

[eric@localhost centos7_2]$ sudo docker run --rm --name sshd-httpd  -ti  -P local/c7-sshd-httpd
AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.17.0.90. Set the 'ServerName' directive globally to suppress this message

 /7	. 

AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.17.0.90. Set the 'ServerName' directive globally to suppress this message

/ Lees,	
http://stackoverflow.com/questions/9541460/httpd-could-not-reliably-determine-the-servers-fully-qualified-domain-name-us

 /Lees	,
https://github.com/docker-library/httpd/tree/047048112cb4f8997b1a51f4295f44584b436a83/2.2
/ exec in bash script	,


/ 13	. 

/ we doen	,
[eric@localhost centos7_2]$  sudo docker run --rm --name sshd  -ti  -P local/c7-sshd

[eric@localhost Docker]$ sudo su -
[root@localhost ~]# docker port sshd
22/tcp -> 0.0.0.0:49175
[root@localhost ~]# ssh localhost -p 49175
[root@856da7f441a0 ~]# yum -y install hostname
[root@856da7f441a0 ~]# hostname
856da7f441a0
[root@856da7f441a0 ~]#  cat /etc/hosts
172.17.0.99	856da7f441a0
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters

/ 13	. 

/ we spieken bij c7-sshd-httpd	,

[eric@localhost centos7]$ sudo docker run --rm --name sshd-httpd --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -P local/c7-systemd-sshd-httpd

[eric@localhost Docker]$ sudo su -
[root@localhost ~]# docker port sshd-httpd
22/tcp -> 0.0.0.0:49176
80/tcp -> 0.0.0.0:49177
[root@localhost ~]# ssh localhost -p 49176
[root@c54c763f96fc ~]# yum install hostname
[root@c54c763f96fc ~]# hostname
c54c763f96fc
[root@c54c763f96fc ~]# cat /etc/hosts
172.17.0.100	c54c763f96fc
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters


/ 7	.

/ we hebben httpd image	, zonder sshd	,

[eric@localhost centos7_3]$ pwd
/home/eric/Devel/Docker/httpd/centos7_3

[eric@localhost centos7_3]$ sudo docker build --rm -t local/c7-httpd .

/ we kunnen link met sshd	, maar WH geen zin	, we kunnen alleen ssh naar de sshd-container	,
[eric@localhost centos7_3]$ sudo docker run --rm --name httpd --link sshd:mysshd   -ti  -P local/c7-httpd

/ 13	. 

/ wel zien we	,
[eric@localhost centos7_2]$ sudo docker run --rm --name httpd  --link sshd:mysshd -ti  -P local/c7-httpd env

/ of	,

[eric@localhost centos7_2]$ sudo docker run --rm --name httpd  --link sshd:mysshd -ti  -P local/c7-httpd
/ en	,
eric@localhost system]$ sudo  docker exec -it httpd env

PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=3997f2de12d9
TERM=xterm
MYSSHD_PORT=tcp://172.17.0.166:22
MYSSHD_PORT_22_TCP=tcp://172.17.0.166:22
MYSSHD_PORT_22_TCP_ADDR=172.17.0.166
MYSSHD_PORT_22_TCP_PORT=22
MYSSHD_PORT_22_TCP_PROTO=tcp
MYSSHD_NAME=/httpd/mysshd
HOME=/root

[eric@localhost system]$ sudo  docker exec -it sshd ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?            1 Ss+      0   0:00 /usr/sbin/sshd -D
    0     6     0     0 ?            0 R+       0   0:00 ps ajx
[eric@localhost system]$ sudo  docker exec -it httpd ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?            1 Ss+      0   0:00 /usr/sbin/httpd -D FOREGROUND
    1     6     1     1 ?            1 S+      48   0:00 /usr/sbin/httpd -D FOREGROUND
    1     7     1     1 ?            1 S+      48   0:00 /usr/sbin/httpd -D FOREGROUND
    1     8     1     1 ?            1 S+      48   0:00 /usr/sbin/httpd -D FOREGROUND
    1     9     1     1 ?            1 S+      48   0:00 /usr/sbin/httpd -D FOREGROUND
    1    10     1     1 ?            1 S+      48   0:00 /usr/sbin/httpd -D FOREGROUND
    0    11     0     0 ?            0 R+       0   0:00 ps ajx

[eric@localhost system]$ sudo  docker exec -it httpd "ssh $MYSSHD_PORT_22_TCP_ADDR -p $MYSSHD_PORT_22_TCP_PORT"
2015/03/22 13:17:57 docker-exec: failed to exec: exec: "ssh  -p ": executable file not found in $PATH
/ TODO










/ 7	.	. 

/ als we 	,
[eric@localhost centos7_3]$ pwd
/home/eric/Devel/Docker/httpd/centos7_3
[eric@localhost centos7_3]$ cat Dockerfile 
...
cmd ["/my.bash"]
$ cat my.bash
#!/usr/bin/bash
...
exec /usr/sbin/apachectl -D FOREGROUND

[eric@localhost centos7_3]$ sudo docker build --rm -t local/c7-httpd .

[eric@localhost centos7_3]$ sudo docker run --rm --name httpd  -ti  -P local/c7-httpd
Passing arguments to httpd using apachectl is no longer supported.
You can only start/stop/restart httpd using this script.
If you want to pass extra arguments to httpd, edit the
/etc/sysconfig/httpd config file.

AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.17.0.153. Set the 'ServerName' directive globally to suppress this message

/ Op onze laptop zien we	, maar dat is WH omdat we systemd hebben, en deze de orig /etc/sysconfig/httpd heeft overwrite,	 

$ vi /etc/sysconfig/httpd

#
# This file is no longer used to configure additional environment variables
# for the httpd process.
#
# It has been replaced by systemd. If you want to customize, the best
# way is to create a file "/etc/systemd/system/httpd.service",
# containing
#   .include /lib/systemd/system/httpd.service
#   ...make your changes here...

/ want in local/c7-httpd zien we	, build in httpd/centos7_2	,
[root@d829871ea922 /]# vi /etc/sysconfig/httpd       
#
# This file can be used to set additional environment variables for
# the httpd process, or pass additional options to the httpd
# executable.
#
...

/ 13	. 

/ Hoe rm ERR	,
AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.17.0.153. Set the 'ServerName' directive globally to suppress this message

[eric@localhost centos7_2]$ cat Dockerfile 
FROM local/c7-sshd
RUN yum -y install httpd; yum clean all

run echo "ServerName localhost">>/etc/httpd/conf/httpd.conf

expose 80 

cmd ["/usr/sbin/httpd","-D","FOREGROUND"] 

[eric@localhost centos7_2]$ sudo docker build --rm -t local/c7-httpd .
[eric@localhost centos7_2]$ sudo docker run --rm --name httpd  -ti  -P local/c7-httpd

/ De ERR is rm	, 
/ Hoe doet systemd het	, want we zien in http/centos7/Dockerfile	dat /etc/httpd/conf/httpd.conf niet wordt edit, en toch zien we de ERR NIET	,
/ TODO








/ Einde DOCKER NO SYSTEMD

/ DOCKER STOP RM ALL CONTAINERS IMAGES

[eric@localhost centos7]$ echo $(awk 'NR>1{print $1}' <(sudo docker ps -a))
CONTAINER be7d239d5cfa ebad6278f4fe 599f5f433fdc d95e10ac0058 7b5caf57e905 5f8f45b02bc0 2551d25ba6cc dae2c9461457

/ rm all containers	,
[eric@localhost centos7]$ sudo docker rm $(sudo docker stop $(awk 'NR>1{print $1}' <(sudo docker ps -a)))

/ rm all <none> images	,
[eric@localhost tmp]$ sudo docker rmi $(awk 'NR>1 && $1~/<none>/{print $3}' <(sudo docker images))




/ Einde DOCKER STOP RM ALL CONTAINERS

/ DOCKER ENTRYPOINT

/ Lees over Dockerfile in docs	, bij entrypoint	,

/ 7	. 

/ we doen 1ste 

[eric@localhost tmp]$ sudo  docker run -i -t --rm -p 80:80 nginx
Unable to find image 'nginx:latest' locally
nginx:latest: The image you are pulling has been verified
e977d53b9210: Pull complete 
c9fa20ecce88: Pull complete 
184d60f5cc4f: Pull complete 
96d31e36bd8a: Pull complete 
fd2224b9a216: Pull complete 
ddf43d50cf9c: Pull complete 
9313885bc17d: Pull complete 
a58e8918cbbe: Pull complete 
a796366df375: Pull complete 
0f52d644e5c0: Pull complete 
837726e15051: Pull complete 
3f72b0ae3e59: Pull complete 
511136ea3c5a: Already exists 
Status: Downloaded newer image for nginx:latest
FATA[0131] Error response from daemon: Cannot start container cb053f2ef5f7f15fd4863f06dcf7fa5fbea42fc3fe91d22131fc2e7d2a254f89: Error starting userland proxy: listen tcp 0.0.0.0:80: bind: address already in use 

[eric@localhost Docker]$ awk 'NR>1 && $3 ~ "3f72b0ae3e59"' <(sudo docker images -a)
/ of	,
[eric@localhost Docker]$ awk 'NR>1 && $3 == "3f72b0ae3e59"' <(sudo docker images -a)
/ of	,
[eric@localhost Docker]$ awk 'NR>1 && $3 ~ /3f72b0ae3e59/' <(sudo docker images -a)
nginx                         latest              3f72b0ae3e59        2 days ago          93.54 MB
[eric@localhost Docker]$ awk 'NR>1 && $3 ~ /837726e15051/' <(sudo docker images -a)
<none>                        <none>              837726e15051        2 days ago          93.54 MB
/ TODO

/ we moeten httpd op onze laptop stop	,
[eric@localhost tmp]$ sudo systemctl stop httpd.service 

/ nu	,
[eric@localhost tmp]$ sudo  docker run -i -t --rm -p 80:80 nginx
/ OK
/ Geef in chrome: localhost	,

/ 7	. 

/ we zien de Dockerfile van nginx	,
https://registry.hub.docker.com/_/nginx/

$ vi Dockerfile
FROM debian:wheezy
...

/ Op	,
https://registry.hub.docker.com/_/debian/
/ we zien dat wheezy = latest	,

/ we zien dat als we	,
$ sudo docker run nginx	,
/ dan is WH debian:latest een van de <none> images	, die we zien in docker images -a	,

/ we download debian expliciet	,

[eric@localhost centos7_3]$ sudo docker pull debian:wheezy
debian:wheezy: The image you are pulling has been verified
511136ea3c5a: Already exists 
e977d53b9210: Already exists 
c9fa20ecce88: Already exists 
Status: Image is up to date for debian:wheezy

/ Nu staat hij erbij bij 'docker images'	, WH was eerst <none> toen we nginx run	, 
/ TODO

/ 7	. 

[eric@localhost tmp]$  pwd
/home/eric/Devel/Docker/tmp
[eric@localhost tmp]$  cat Dockerfile 

from centos:7
entrypoint ["top","-b"]
cmd ["-c"]

/ entrypoint is de base	, wat bij cmd staat komt erbij	,
/ als we bij docker run -H geven	, vervangt deze -c	, maar dat is altijd al zo bij docker run	: als je docker run een arg geeft	, vervangt deze wat er achter cmd staat	,

/ 13	. 

[eric@localhost system]$ sudo docker run --rm -it --name tmp local/tmp

top - 11:02:47 up 1 day, 13:28,  0 users,  load average: 0.13, 0.20, 0.21
Tasks:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie
%Cpu(s):  5.1 us,  0.9 sy,  0.0 ni, 93.9 id,  0.1 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:  16310540 total,  8864476 used,  7446064 free,   284596 buffers
KiB Swap:  8191996 total,        0 used,  8191996 free.  2109576 cached Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
    1 root      20   0   15596   2124   1880 R   0.3  0.0   0:00.04 top -b -c


/ we zien dat draait	, 
top -b -c

/ dan kunnen we ook zo zien	, met docker exec	, waarmee je een cmd kunt geven	, in dit geval dus ps aux	,

[eric@localhost tmp]$  sudo docker exec  -it tmp ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.1  0.0  15596  2120 ?        Ss+  11:04   0:00 top -b -c
root         7  0.0  0.0  19756  2128 ?        R+   11:05   0:00 ps aux

[eric@localhost Docker]$ sudo docker stop $(awk 'NR>1{print $1}' <(sudo docker ps -a))


/ 13	. 

/ geef een extra arg mee	,
/ Deze vervangt die van cmd	, -c hier 	, we zien dus alleen -H	,

[eric@localhost system]$ sudo docker run --rm -it --name tmp local/tmp -H

top - 11:06:05 up 1 day, 13:32,  0 users,  load average: 0.29, 0.20, 0.21
Threads:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie
%Cpu(s):  3.6 us,  0.6 sy,  0.0 ni, 95.7 id,  0.1 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:  16310540 total,  8864804 used,  7445736 free,   285064 buffers
KiB Swap:  8191996 total,        0 used,  8191996 free.  2109000 cached Mem

  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND
    1 root      20   0   15596   2084   1828 R  0.0  0.0   0:00.01 top

[eric@localhost tmp]$  sudo docker exec  -it tmp ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.6  0.0  15596  2124 ?        Ss+  11:07   0:00 top -b -H
root         6  0.0  0.0  19756  2152 ?        R+   11:07   0:00 ps aux

[eric@localhost Docker]$ sudo docker stop $(awk 'NR>1{print $1}' <(sudo docker ps -a))

/ 13	. 










/ Einde DOCKER ENTRYPOINT


/  Mesos, Marathon en Kubernetes

/ POSTGRESQL

/ Lees	,
https://wiki.postgresql.org/wiki/YUM_Installation

/ Lees	,
http://www.postgresql.org/
/ click 'Download'
http://www.postgresql.org/download/
/ click 'Redhat'
http://www.postgresql.org/download/linux/redhat/
/ we kunnen doen wat bovenaan staat
/ of wat bij 'PostgreSQL Yum Repository' staat	,

/ Dat laatste doen we	,
/ click 'repository RPM listing'	,
http://yum.postgresql.org/repopackages.php

/ Ga daarna weer terug naar	, en ga verder met	,
http://www.postgresql.org/download/linux/redhat/

/  je kunt ook	,
http://yum.postgresql.org/
/ click 9.4
/ download	,

/ Kijk wat er in zit	,
[eric@localhost sinatra]$ rpm -qlp /home/eric/Downloads/pgdg-fedora94-9.4-2.noarch.rpm
warning: /home/eric/Downloads/pgdg-fedora94-9.4-2.noarch.rpm: Header V4 DSA/SHA1 Signature, key ID 442df0f8: NOKEY
/etc/pki/rpm-gpg
/etc/pki/rpm-gpg/RPM-GPG-KEY-PGDG-94
/etc/yum.repos.d/pgdg-94-fedora.repo

[eric@localhost sinatra]$ sudo yum install ~/Downloads/pgdg-fedora94-9.4-2.noarch.rpm 
...
[eric@localhost sinatra]$ rpm -ql pgdg-fedora94-9.4-2
/etc/pki/rpm-gpg
/etc/pki/rpm-gpg/RPM-GPG-KEY-PGDG-94
/etc/yum.repos.d/pgdg-94-fedora.repo

[eric@localhost sinatra]$ cat /etc/pki/rpm-gpg/RPM-GPG-KEY-PGDG-94 
-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: GnuPG v1.4.7 (GNU/Linux)

mQGiBEeD8koRBACC1VBRsUwGr9gxFFRho9kZpdRUjBJoPhkeOTvp9LzkdAQMFngr
...
 

[eric@localhost sinatra]$ cat /etc/yum.repos.d/pgdg-94-fedora.repo 
[pgdg94]
name=PostgreSQL 9.4 $releasever - $basearch
baseurl=http://yum.postgresql.org/9.4/fedora/fedora-$releasever-$basearch
enabled=1
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-PGDG-94

[pgdg94-source]
name=PostgreSQL 9.4 $releasever - $basearch - Source
failovermethod=priority
baseurl=http://yum.postgresql.org/srpms/9.4/fedora/fedora-$releasever-$basearch
enabled=0
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-PGDG-94



[eric@localhost sinatra]$ yum list postgres*
Loaded plugins: langpacks
pgdg94                                                       | 3.6 kB  00:00:00     
(1/2): pgdg94/21/x86_64/group_gz                             |  331 B  00:00:00     
(2/2): pgdg94/21/x86_64/primary_db                           |  76 kB  00:00:01     
google-chrome/primary                                        | 1.9 kB  00:00:00     
google-chrome                                                                   3/3
Available Packages
postgresql.i686                               9.3.5-5.fc21                   updates
postgresql.x86_64                             9.3.5-5.fc21                   updates
postgresql-contrib.x86_64                     9.3.5-5.fc21                   updates
postgresql-dbi-link.noarch                    2.0.0-11.fc21                  fedora 
postgresql-devel.i686                         9.3.5-5.fc21                   updates
postgresql-devel.x86_64                       9.3.5-5.fc21                   updates
postgresql-docs.x86_64                        9.3.5-5.fc21                   updates
postgresql-ip4r.x86_64                        2.0.2-5.fc21                   fedora 
postgresql-jdbc.noarch                        9.3.1102-1.fc21                fedora 
postgresql-jdbc-javadoc.noarch                9.3.1102-1.fc21                fedora 
postgresql-libs.i686                          9.3.5-5.fc21                   updates
postgresql-libs.x86_64                        9.3.5-5.fc21                   updates
postgresql-odbc.x86_64                        09.03.0400-3.fc21              updates
postgresql-odbc-tests.x86_64                  09.03.0400-3.fc21              updates
postgresql-pgpool-II.i686                     3.3.3-3.fc21                   fedora 
postgresql-pgpool-II.x86_64                   3.3.3-3.fc21                   fedora 
postgresql-pgpool-II-devel.i686               3.3.3-3.fc21                   fedora 
postgresql-pgpool-II-devel.x86_64             3.3.3-3.fc21                   fedora 
postgresql-pgpool-II-recovery.x86_64          3.3.3-3.fc21                   fedora 
postgresql-pgpoolAdmin.noarch                 3.1.1-6.fc21                   fedora 
postgresql-plperl.x86_64                      9.3.5-5.fc21                   updates
postgresql-plpython.x86_64                    9.3.5-5.fc21                   updates
postgresql-plpython3.x86_64                   9.3.5-5.fc21                   updates
postgresql-plruby.x86_64                      0.5.4-4.fc21                   fedora 
postgresql-plruby-doc.x86_64                  0.5.4-4.fc21                   fedora 
postgresql-pltcl.x86_64                       9.3.5-5.fc21                   updates
postgresql-server.x86_64                      9.3.5-5.fc21                   updates
postgresql-test.x86_64                        9.3.5-5.fc21                   updates
postgresql-upgrade.x86_64                     9.3.5-5.fc21                   updates
postgresql94.x86_64                           9.4.1-1PGDG.f21                pgdg94 
postgresql94-contrib.x86_64                   9.4.1-1PGDG.f21                pgdg94 
postgresql94-debuginfo.x86_64                 9.4.1-1PGDG.f21                pgdg94 
postgresql94-devel.x86_64                     9.4.1-1PGDG.f21                pgdg94 
postgresql94-docs.x86_64                      9.4.1-1PGDG.f21                pgdg94 
postgresql94-jdbc.noarch                      9.3.1101-1.f21                 pgdg94 
postgresql94-jdbc-javadoc.noarch              9.3.1101-1.f21                 pgdg94 
postgresql94-libs.x86_64                      9.4.1-1PGDG.f21                pgdg94 
postgresql94-odbc.x86_64                      09.03.0300-1PGDG.f21           pgdg94 
postgresql94-odbc-debuginfo.x86_64            09.03.0300-1PGDG.f21           pgdg94 
postgresql94-plperl.x86_64                    9.4.1-1PGDG.f21                pgdg94 
postgresql94-plpython.x86_64                  9.4.1-1PGDG.f21                pgdg94 
postgresql94-pltcl.x86_64                     9.4.1-1PGDG.f21                pgdg94 
postgresql94-python.x86_64                    4.1.1-1PGDG.f21                pgdg94 
postgresql94-python-debuginfo.x86_64          4.1.1-1PGDG.f21                pgdg94 
postgresql94-server.x86_64                    9.4.1-1PGDG.f21                pgdg94 
postgresql94-tcl.x86_64                       2.0.0-1.f21                    pgdg94 
postgresql94-tcl-debuginfo.x86_64             2.0.0-1.f21                    pgdg94 
postgresql94-test.x86_64                      9.4.1-1PGDG.f21                pgdg94 
postgresql_autodoc.noarch                     1.41-4.fc21                    fedora 

/ we zien dus fedora repo postgresql-9.3 en postgres repo postgres-9.4	, 

[eric@localhost sinatra]$ sudo yum install postgresql94-server
Dependencies Resolved

====================================================================================
 Package                   Arch         Version                  Repository    Size
====================================================================================
Installing:
 postgresql94-server       x86_64       9.4.1-1PGDG.f21          pgdg94       3.9 M
Installing for dependencies:
 postgresql94              x86_64       9.4.1-1PGDG.f21          pgdg94       1.0 M
 postgresql94-libs         x86_64       9.4.1-1PGDG.f21          pgdg94       210 k

Transaction Summary
====================================================================================
Install  1 Package (+2 Dependent packages)

Total download size: 5.1 M
Installed size: 22 M
Is this ok [y/d/N]:y
Downloading packages:
warning: /var/cache/yum/x86_64/21/pgdg94/packages/postgresql94-libs-9.4.1-1PGDG.f21.x86_64.rpm: Header V4 DSA/SHA1 Signature, key ID 442df0f8: NOKEY
Public key for postgresql94-libs-9.4.1-1PGDG.f21.x86_64.rpm is not installed
(1/3): postgresql94-libs-9.4.1-1PGDG.f21.x86_64.rpm          | 210 kB  00:00:01     
(2/3): postgresql94-9.4.1-1PGDG.f21.x86_64.rpm               | 1.0 MB  00:00:04     
(3/3): postgresql94-server-9.4.1-1PGDG.f21.x86_64.rpm        | 3.9 MB  00:00:12     
------------------------------------------------------------------------------------
Total                                                  372 kB/s | 5.1 MB  00:14     
Retrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-PGDG-94
Importing GPG key 0x442DF0F8:
 Userid     : "PostgreSQL RPM Building Project <pgsqlrpms-hackers@pgfoundry.org>"
 Fingerprint: 68c9 e2b9 1a37 d136 fe74 d176 1f16 d2e1 442d f0f8
 Package    : pgdg-fedora94-9.4-2.noarch (@/pgdg-fedora94-9.4-2.noarch)
 From       : /etc/pki/rpm-gpg/RPM-GPG-KEY-PGDG-94
Is this ok [y/N]: y
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction (shutdown inhibited)
  Installing : postgresql94-libs-9.4.1-1PGDG.f21.x86_64                         1/3 
  Installing : postgresql94-9.4.1-1PGDG.f21.x86_64                              2/3 
  Installing : postgresql94-server-9.4.1-1PGDG.f21.x86_64                       3/3 
  Verifying  : postgresql94-libs-9.4.1-1PGDG.f21.x86_64                         1/3 
  Verifying  : postgresql94-9.4.1-1PGDG.f21.x86_64                              2/3 
  Verifying  : postgresql94-server-9.4.1-1PGDG.f21.x86_64                       3/3 

Installed:
  postgresql94-server.x86_64 0:9.4.1-1PGDG.f21                                      

Dependency Installed:
  postgresql94.x86_64 0:9.4.1-1PGDG.f21  postgresql94-libs.x86_64 0:9.4.1-1PGDG.f21 

Complete!

[eric@localhost sinatra]$ sudo ls /var/lib/pgsql/9.4/data
[eric@localhost sinatra]$ 
/ Helemaal leeg	,

[eric@localhost sinatra]$ yum list installed "*postgres*"
Loaded plugins: langpacks
Installed Packages
postgresql94.x86_64                         9.4.1-1PGDG.f21                  @pgdg94
postgresql94-libs.x86_64                    9.4.1-1PGDG.f21                  @pgdg94
postgresql94-server.x86_64                  9.4.1-1PGDG.f21                  @pgdg94

[eric@localhost hadoop]$ which psql
/usr/bin/psql
[eric@localhost hadoop]$ ls -l /usr/bin/psql
lrwxrwxrwx. 1 root root 28 Feb  7 14:06 /usr/bin/psql -> /etc/alternatives/pgsql-psql
[eric@localhost hadoop]$ ls -l /etc/alternatives/pgsql-psql
lrwxrwxrwx. 1 root root 23 Feb  7 14:06 /etc/alternatives/pgsql-psql -> /usr/pgsql-9.4/bin/psql

/ er is ook	,
[eric@localhost hadoop]$ ls -l /bin/psql
lrwxrwxrwx. 1 root root 28 Feb  7 14:06 /bin/psql -> /etc/alternatives/pgsql-psql
[eric@localhost hadoop]$ sudo su - postgres -c "which psql"
/bin/psql
[eric@localhost hadoop]$ sudo su - postgres -c psql
psql (9.4.1)
Type "help" for help.

postgres=# \q
/ OK



[eric@localhost sinatra]$ sudo /usr/pgsql-9.4/bin/postgresql94-setup initdb
Initializing database ... OK
[eric@localhost sinatra]$ sudo ls /var/lib/pgsql/9.4
backups  data  initdb.log

[eric@localhost sinatra]$ sudo systemctl start postgresql-9.4
[eric@localhost sinatra]$ sudo systemctl enable postgresql-9.4
Created symlink from /etc/systemd/system/multi-user.target.wants/postgresql-9.4.service to /usr/lib/systemd/system/postgresql-9.4.service.

/ we lezen nog steeds	,
https://wiki.postgresql.org/wiki/YUM_Installation
/ we gaan naar	,
https://wiki.postgresql.org/wiki/First_steps

/ 13	. 

[eric@localhost sinatra]$ sudo su - postgres -c psql 
psql (9.4.1)
Type "help" for help.

postgres=# \q
[eric@localhost sinatra]$ sudo su - postgres
-bash-4.3$ psql 
psql (9.4.1)
Type "help" for help.

postgres=# \q
-bash-4.3$ exit
logout
[eric@localhost sinatra]$ sudo su - postgres -c pwd
/var/lib/pgsql

/13	. 
[eric@localhost sinatra]$ sudo su - postgres -c psql 
psql (9.4.1)
Type "help" for help.

postgres=# \l
                                  List of databases
   Name    |  Owner   | Encoding |   Collate   |    Ctype    |   Access privileges   
-----------+----------+----------+-------------+-------------+-----------------------
 postgres  | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | 
 template0 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
           |          |          |             |             | postgres=CTc/postgres
 template1 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
           |          |          |             |             | postgres=CTc/postgres
(3 rows)

postgres=# \dg
                             List of roles
 Role name |                   Attributes                   | Member of 
-----------+------------------------------------------------+-----------
 postgres  | Superuser, Create role, Create DB, Replication | {}

postgres=# select rolname from pg_roles;
 rolname  
----------
 postgres
(1 row)

/ 13. 

/ lees	,
http://www.postgresql.org/docs/9.4/interactive/role-attributes.html

postgres=# create role eric login;
CREATE ROLE
postgres=# select*from pg_roles
postgres-# ;
 rolname  | rolsuper | rolinherit | rolcreaterole | rolcreatedb | rolcatupdate | rolcanlogin | 
rolreplication | rolconnlimit | rolpassword | rolvaliduntil | rolconfig |  oid  
----------+----------+------------+---------------+-------------+--------------+-------------+-
---------------+--------------+-------------+---------------+-----------+-------
 postgres | t        | t          | t             | t           | t            | t           | 
t              |           -1 | ********    |               |           |    10
 eric     | f        | t          | f             | f           | f            | t           | 
f              |           -1 | ********    |               |           | 16384
(2 rows)
postgres=# \dg
                             List of roles
 Role name |                   Attributes                   | Member of 
-----------+------------------------------------------------+-----------
 eric      |                                                | {}
 postgres  | Superuser, Create role, Create DB, Replication | {}

/ dus postgres heeft ook de login attribute	, wat we al wisten	,

/ 13	 

/ postgres hoeft alleen een user eric te create	, en eric kan psql	, via peer	,

postgres=# drop role if exists eric;
DROP ROLE
postgres=# create role eric login;
CREATE ROLE

/ maar eric kan niet via tcp	, dus met 'psql -h localhost' , omdat hij dan om een password vraagt	, 
/ we kunnen eric dan ook een password geven	,
/ eric kan altijd ook 'psql -p 5432'	,

/ in ander window	,
[eric@localhost sinatra]$ psql postgres
psql (9.4.1)
Type "help" for help.

postgres=> 

/ Waarom kan eric login en in db postgres terecht komen waarvan postgres de owner is	?
/ TODO

/ 13	. 

/ Lees	,
http://www.postgresql.org/docs/9.1/static/sql-grant.html

rolename=xxxx -- privileges granted to a role
        =xxxx -- privileges granted to PUBLIC

            r -- SELECT ("read")
            w -- UPDATE ("write")
            a -- INSERT ("append")
            d -- DELETE
            D -- TRUNCATE
            x -- REFERENCES
            t -- TRIGGER
            X -- EXECUTE
            U -- USAGE
            C -- CREATE
            c -- CONNECT
            T -- TEMPORARY
      arwdDxt -- ALL PRIVILEGES (for tables, varies for other objects)
            * -- grant option for preceding privilege

        /yyyy -- role that granted this privilege

/ 13.	 

/ Nadat postgres 'create role eric login' , 

postgres=> create table my(i int);
CREATE TABLE

/ Waarom kan eric table create is db postgres, waarvan postgres de owner is?
/ TODO

postgres=> create database eric ;
ERROR:  permission denied to create database

/ Daarom doet postgres	,
postgres=#  alter role eric createdb createrole;
ALTER ROLE

/ terug naar eric	,
postgres=> create database eric ;
CREATE DATABASE

/ 13	. 

postgres=# create role foo login createdb createrole;
CREATE ROLE
postgres=# alter role foo password 'foo';
ALTER ROLE

[eric@localhost sinatra]$ psql -U foo postgres
psql: FATAL:  Peer authentication failed for user "foo"
/ Dat geeft niet, en wil je niet , want foo is geen Linux account	,

[eric@localhost sinatra]$ psql -U foo postgres -h localhost
psql: FATAL:  Ident authentication failed for user "foo"

$ sudo vi /var/lib/pgsql/9.4/data/pg_hba.conf

#host    all             all             127.0.0.1/32            ident
host    all             all             127.0.0.1/32            md5 

[eric@localhost sinatra]$ sudo systemctl reload postgresql-9.4
[eric@localhost sinatra]$ psql -U foo postgres -h localhost
Password for user foo: foo 
psql (9.4.1)
Type "help" for help.

postgres=> 

/ 13	. 

/ SAMENVATTING POSTGRES AUTHENTICATION

/ Voor Linux account eric , een user met create role hoeft alleen	,
postgres=# create role eric login;
postgres=# create role root login;
/ Dit gaat met peer 	,
/ peer=CURRENT login	,
$ vi pg_hba.conf
local   all             all                                     peer
/ al	,
[eric@localhost ]$ psql postgres
/ eric logs in , naar db postgres
/ OK
[eric@localhost hadoop]$ sudo su - -c psql postgres
/ root logs in , naar db postgres
/ OK
/ we geven postgres als database	, omdat er geen eric of root databases zijn	, 
/ ( er is nu wel een db eric)	,

 / eric kan	, 
[eric@localhost hadoop]$ psql
/ of	,
[eric@localhost hadoop]$ psql -U eric 
psql (9.4.1)
Type "help" for help.

eric=> \q

[eric@localhost hadoop]$ psql -U root postgres
psql: FATAL:  Peer authentication failed for user "root"
[eric@localhost hadoop]$ sudo su - -c psql postgres
psql (9.4.1)
Type "help" for help.

postgres=#
/ peer==kijkt naar current login	,

[eric@localhost hadoop]$ psql
psql (9.4.1)
Type "help" for help.

eric=> \q
/ OK
[eric@localhost hadoop]$ psql -h localhost
Password: 
/ we hebben geen role eric create
/ TODO


/ Voor niet Linux account	,
postgres=# create role foo login password 'foo';
$ vi pg_hba.conf
host    all             all             127.0.0.1/32            md5 
/ ident -> md5
[eric@localhost ]$ psql -U foo postgres -h localhost
Password for user foo: foo 

/ Einde SAMENVATTING POSTGRES AUTHENTICATION

/13	.

/ eric creates in db eric een table first	, 
/ dan kan foo daar niet in insert	,

/ eric doet	,
eric=> create table first(s text);
CREATE TABLE

/ foo doet	,
eric=> insert into first values('foo');
ERROR:  permission denied for relation first

/ eric doet	,
eric=> grant insert on first to public;
GRANT
eric=> \dp
                           Access privileges
 Schema | Name  | Type  | Access privileges | Column access privileges 
--------+-------+-------+-------------------+--------------------------
 public | first | table | eric=arwdDxt/eric+| 
        |       |       | =a/eric           | 
(1 row)

/ Nu pas zien we de permissions van eric zelf	, nl alles	,
/ TODO

/ foo doet	, 
eric=> select current_user;
 current_user 
--------------
 foo
(1 row)
eric=> insert into first values('foo');
INSERT 0 1

/ eric doet	,
eric=> revoke insert on first from public;
REVOKE
eric=> grant all on first to public;
GRANT
eric=> \dp
                           Access privileges
 Schema | Name  | Type  | Access privileges | Column access privileges 
--------+-------+-------+-------------------+--------------------------
 public | first | table | eric=arwdDxt/eric+| 
        |       |       | =arwdDxt/eric     | 
(1 row)

/ 13	. 

eric=> \db
       List of tablespaces
    Name    |  Owner   | Location 
------------+----------+----------
 pg_default | postgres | 
 pg_global  | postgres | 
/ TODO

/ 13	. 

/ eric doet	,
eric=> revoke all on first from public;
REVOKE
eric=> grant all on database eric to public;
GRANT

/ foo doet	,
eric=> create table second(s text);
CREATE TABLE

/ eric doet	,
eric=> insert into second values('eric');
ERROR:  permission denied for relation second

eric=> \l
                                  List of databases
   Name    |  Owner   | Encoding |   Collate   |    Ctype    |   Access privileges   
-----------+----------+----------+-------------+-------------+-----------------------
 eric      | eric     | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =CTc/eric            +
           |          |          |             |             | eric=CTc/eric

/ foo doet	,
eric=> revoke all on second from public;
REVOKE
/ zodat foo's privileges zichtbaar zijn	,
/ deze revoke doet niets	, want dat is nu toch al zo (vlak nadat foo de table heeft create)	,

eric=> \z
                           Access privileges
 Schema |  Name  | Type  | Access privileges | Column access privileges 
--------+--------+-------+-------------------+--------------------------
 public | first  | table | eric=arwdDxt/eric | 
 public | second | table | foo=arwdDxt/foo   | 

/ 13	. 

/ WH moeten we voor postgres.conf 
[eric@localhost postgresql]$ sudo systemctl restart postgresql-9.4.service 

/ WH moeten we voor pg_hba.conf 
[eric@localhost postgresql]$ sudo systemctl reload postgresql-9.4.service 

[eric@localhost sinatra]$ sudo psql -U foo postgres -h 192.168.123.182
psql: could not connect to server: Connection refused
	Is the server running on host "192.168.123.182" and accepting
	TCP/IP connections on port 5432?
/ ERR uit postgresql.conf	,

[eric@localhost sinatra]$ sudo psql -U foo postgres -h 192.168.123.182
psql: FATAL:  no pg_hba.conf entry for host "192.168.123.182", user "foo", database "postgres", SSL off
/ ERR uit pg_hba.conf

/ SAMENVATTING CONNECTIONS

$ sudo vi /var/lib/pgsql/9.4/data/postgresql.conf
listen_addresses='*'
[eric@localhost ]$ sudo systemctl restart postgresql-9.4.service 

$ sudo vi /var/lib/pgsql/9.4/data/pg_hba.conf
local   all             all                                     peer
host    all             all             127.0.0.1/32            md5
host    all             all             192.168.123.182/24      md5
#host    all             all             0.0.0.0/0      md5
[eric@localhost postgresql]$ sudo systemctl reload postgresql-9.4.service 

/ listen_addresses='*' moeten we set voor 	,
[eric@localhost ]$ psql -U foo postgres -h 192.168.123.182

/ we kunnen ook  
host    all             all             0.0.0.0/0      md5
/ voor	,
[eric@localhost ]$ psql -U foo postgres -h 192.168.123.182

/ we kunnen ook	,
host    all             all             192.168.123.182/32	md5
/ voor	,
[eric@localhost ]$ psql -U foo postgres -h 192.168.123.182



/ Einde SAMENVATTING CONNECTIONS

/ 13	. 

/ we kunnen ook	,
[eric@localhost ]$ sudo su - postgres -c "/usr/pgsql-9.4/bin/pg_ctl restart/reload/stop/start"

/ maar hetzelfde geldt: modifications in postgresql.conf moeten restart	, 
/ modifications in pg_hba.conf kunnen reload	, 

/ Maar als we eerst
pg_ctr restart 
/ en daarna 
systemctl restart wer 
/ dan ERR

/ we kunnen WEL	,
pg_ctr stop 
systemctl start wer 

/ TODO

/ systemctl start/... calls /usr/pgsql-9.4/bin/pg_ctl ...
/ TODO 

/ Lees	,
[eric@localhost postgresql]$ vi /usr/lib/systemd/system/postgresql-9.4.service


/ Lees	,
https://fedoraproject.org/wiki/PostgreSQL

/ Lees	,
http://www.postgresql.org/docs/9.1/static/functions-info.html


/ 13	.

/ 7	.

[eric@almond tmp]$ PGPASSWORD=foo psql -U foo -h almond
psql (9.4.1)
Type "help" for help.

foo=> \q
/ OK

/ eric heeft geen password	, 
/ TODO

/ 7	.

[eric@almond Manual]$ pwd
/home/eric/Devel/Postgres/PL/Manual












 





/ Einde POSTGRESQL

/ SELINUX

/ we download	,
[eric@localhost ~]$ ls ~/Documents/
The_SELinux_Notebook-4th_Edition.pdf


/ Einde SELINUX

/ VIM

[eric@localhost ~]$ sudo yum install vim-enhanced

[eric@localhost ~]$  repoquery -f /usr/bin/vim
vim-enhanced-2:7.4.475-2.fc21.x86_64
[eric@localhost ~]$  repoquery -f /usr/bin/vi
vim-minimal-2:7.4.475-2.fc21.x86_64



/ Einde VIM

/ DATABASES

/ Geef in google	, 
why relational database not scalable
relational database sharding
 


/ Einde DATABASES

/ CASSANDRA

/ Lees	,
http://www.datastax.com/documentation/cassandra/2.1/cassandra/gettingStartedCassandraIntro.html



/ Einde CASSANDRA

/ BASH

/7	.

/ subshells

/ 13	

$ cat s.bash
a=20

$ a=3
$ bash s.bash
$ echo $a
20

/ 13	.

$ (a=20)
$ echo $a
3

/ 13	. 

/ | pipes	,

$ vi u.bash

lines=0

cat myfile.txt | while read line
/ of	,
while read line

do
    echo $line
    ((lines++))
done
echo $lines

cat myfile.txt | while read line	, 
/ geeft een subshell	, WH door de |	,
/ we zien lines=0
while read line
/ geeft GEEN subshell	,
/ we zien lines=2	, als we 2 lines geven	,
/ TODO

/ we kunnen oplossen door	,

lines=0
exec 3<&0 0<myfile.txt
while read line
do
    echo $line
    ((lines++))
done
echo $lines
exec 0<&3 3<&-

/ dan zien we 2	(als er 2 lines in myfile.txt staan)	,





/ Einde BASH

/ BASHDB

/ 7	.


bash-4.3# yum -y install bashdb
bash-4.3# bash --debugger /root/foo.bash ""

/ Geef altijd een arg aan het script 	, dan debug	,

/ 7	. 

/ we kunnen bashdb ook zelf install	, maar dan moeten we een soft link maken	,

bash-4.3#  yum erase bashdb
-bash-4.3# curl -O http://heanet.dl.sourceforge.net/project/bashdb/bashdb/4.3-0.91/bashdb-4.3-0.91.tar.gz
bash-4.3# yum -y install tar
bash-4.3# tar xvzf bashdb-4.3-0.91.tar.gz 
/ ./configure, make, make install	,
bash-4.3# which bash     
/usr/bin/bash
bash-4.3# strings /usr/bin/bash | grep main
/usr/share/bashdb/bashdb-main.inc
/ bashdb is in /usr/share/share	, dus
bash-4.3# (cd /usr/share ;ln -s /usr/share/share/bashdb)
bash-4.3# bash --debugger /root/foo.bash ""
bash debugger, bashdb, release 4.3-0.91

Copyright 2002, 2003, 2004, 2006-2012, 2014 Rocky Bernstein
This is free software, covered by the GNU General Public License, and you are
welcome to change it and/or distribute copies of it under certain conditions.

(/root/foo.bash:1):
1:	echo
...

/ Einde BASHDB

/ DEBUG BASH

/ 7	. 

/ Lees	,
http://yum.baseurl.org/

bash-4.3# yum grouplist 
...
   C Development Tools and Libraries

bash-4.3#  yum groupinstall "C Development Tools and Libraries"
======================================================================================================
 Package                     Arch       Version                                     Repository   Size
======================================================================================================
Installing for group install "C Development Tools and Libraries":
 autoconf                    noarch     2.69-17.fc21                                updates     708 k
 automake                    noarch     1.14.1-5.fc21                               fedora      693 k
 binutils                    x86_64     2.24-30.fc21                                updates     5.0 M
 bison                       x86_64     3.0.2-3.fc21                                fedora      665 k
 byacc                       x86_64     1.9.20130925-4.fc21                         fedora       69 k
 ccache                      x86_64     3.1.10-1.fc21                               fedora      161 k
 cscope                      x86_64     15.8-10.fc21                                fedora      208 k
 ctags                       x86_64     5.8-15.fc21                                 fedora      160 k
 elfutils                    x86_64     0.161-6.fc21                                updates     276 k
 flex                        x86_64     2.5.37-7.fc21                               fedora      300 k
 gcc                         x86_64     4.9.2-6.fc21                                updates      18 M
 gcc-c++                     x86_64     4.9.2-6.fc21                                updates     8.3 M
 gdb                         x86_64     7.8.2-38.fc21                               updates     2.7 M
 glibc-devel                 x86_64     2.20-8.fc21                                 updates     902 k
 indent                      x86_64     2.2.11-15.fc21                              fedora      154 k
 libtool                     x86_64     2.4.2-31.fc21                               fedora      593 k
 ltrace                      x86_64     0.7.91-13.fc21                              updates     153 k
 make                        x86_64     1:4.0-3.fc21                                fedora      446 k
 oprofile                    x86_64     0.9.9-8.54.gb7e9a57.fc21                    updates     1.7 M
 oprofile-gui                x86_64     0.9.9-8.54.gb7e9a57.fc21                    updates      97 k
 strace                      x86_64     4.10-1.fc21                                 updates     354 k
 valgrind                    x86_64     1:3.10.1-1.fc21                             updates      16 M
Installing for dependencies:
 alsa-lib                    x86_64     1.0.28-2.fc21                               fedora      387 k
 clucene09-core              x86_64     0.9.21b-13.fc21                             updates     305 k
 cpp                         x86_64     4.9.2-6.fc21                                updates     6.9 M
 emacs-filesystem            noarch     1:24.4-3.fc21                               updates      63 k
 fontconfig                  x86_64     2.11.1-5.fc21                               fedora      235 k
 fontpackages-filesystem     noarch     1.44-10.fc21                                fedora       10 k
 glibc-headers               x86_64     2.20-8.fc21                                 updates     489 k
 groff-base                  x86_64     1.22.2-11.fc21                              fedora      947 k
 hwdata                      noarch     0.275-1.fc21                                updates     1.3 M
 jbigkit-libs                x86_64     2.1-2.fc21                                  fedora       51 k
 kernel-headers              x86_64     3.19.1-201.fc21                             updates     973 k
 lcms2                       x86_64     2.6-4.fc21                                  fedora      154 k
 libICE                      x86_64     1.0.9-2.fc21                                fedora       69 k
 libSM                       x86_64     1.2.2-2.fc21                                fedora       43 k
 libX11                      x86_64     1.6.2-2.fc21                                fedora      608 k
 libX11-common               noarch     1.6.2-2.fc21                                fedora      187 k
 libXau                      x86_64     1.0.8-4.fc21                                fedora       33 k
 libXcursor                  x86_64     1.1.14-4.fc21                               fedora       34 k
 libXdamage                  x86_64     1.1.4-6.fc21                                fedora       25 k
 libXext                     x86_64     1.3.3-2.fc21                                fedora       42 k
 libXfixes                   x86_64     5.0.1-4.fc21                                fedora       22 k
 libXi                       x86_64     1.7.4-2.fc21                                fedora       44 k
 libXinerama                 x86_64     1.1.3-4.fc21                                fedora       18 k
 libXrandr                   x86_64     1.4.2-2.fc21                                fedora       29 k
 libXrender                  x86_64     0.9.8-4.fc21                                fedora       30 k
 libXxf86vm                  x86_64     1.1.3-4.fc21                                fedora       22 k
 libbabeltrace               x86_64     1.2.1-3.fc21                                fedora      155 k
 libdrm                      x86_64     2.4.59-4.fc21                               updates     127 k
 libjpeg-turbo               x86_64     1.3.1-5.fc21                                updates     142 k
 libmng                      x86_64     2.0.2-3.fc21                                fedora      172 k
 libmpc                      x86_64     1.0.2-3.fc21                                fedora       55 k
 libpciaccess                x86_64     0.13.3-0.3.fc21                             fedora       30 k
 libstdc++-devel             x86_64     4.9.2-6.fc21                                updates     1.6 M
 libtiff                     x86_64     4.0.3-18.fc21                               fedora      172 k
 libxcb                      x86_64     1.11-3.fc21                                 updates     191 k
 libxshmfence                x86_64     1.2-1.fc21                                  updates      11 k
 lyx-fonts                   noarch     2.1.3-1.fc21                                updates     170 k
 m4                          x86_64     1.4.17-6.fc21                               fedora      264 k
 mesa-libGL                  x86_64     10.4.3-1.20150124.fc21                      updates     191 k
 mesa-libglapi               x86_64     10.4.3-1.20150124.fc21                      updates      46 k
 mpfr                        x86_64     3.1.2-8.fc21                                updates     209 k
 pciutils                    x86_64     3.3.0-1.fc21                                fedora       96 k
 pciutils-libs               x86_64     3.3.0-1.fc21                                fedora       50 k
 perl                        x86_64     4:5.18.4-306.fc21                           updates     8.1 M
 perl-Carp                   noarch     1.33.01-2.fc21                              fedora       24 k
 perl-Data-Dumper            x86_64     2.154-1.fc21                                fedora       54 k
 perl-Encode                 x86_64     1:2.70-1.fc21                               updates     1.5 M
 perl-Exporter               noarch     5.70-2.fc21                                 fedora       29 k
 perl-File-Path              noarch     2.09-293.fc21                               fedora       27 k
 perl-File-Temp              noarch     0.23.04-2.fc21                              fedora       57 k
 perl-Filter                 x86_64     1:1.54-1.fc21                               updates      83 k
 perl-Getopt-Long            noarch     2.45-1.fc21                                 updates      61 k
 perl-HTTP-Tiny              noarch     0.043-2.fc21                                fedora       45 k
 perl-Module-CoreList        noarch     1:3.13-306.fc21                             updates     104 k
 perl-PathTools              x86_64     3.47-3.fc21                                 fedora       91 k
 perl-Pod-Escapes            noarch     1:1.04-306.fc21                             updates      57 k
 perl-Pod-Perldoc            noarch     3.23-2.fc21                                 fedora       88 k
 perl-Pod-Simple             noarch     1:3.29-1.fc21                               updates     222 k
 perl-Pod-Usage              noarch     4:1.67-1.fc21                               updates      34 k
 perl-Scalar-List-Utils      x86_64     1:1.41-1.fc21                               updates      62 k
 perl-Socket                 x86_64     1:2.018-1.fc21                              updates      55 k
 perl-Storable               x86_64     2.51-2.fc21                                 fedora       84 k
 perl-Term-ANSIColor         noarch     4.03-2.fc21                                 fedora       44 k
 perl-Test-Harness           noarch     3.35-1.fc21                                 updates     309 k
 perl-Text-ParseWords        noarch     3.29-6.fc21                                 fedora       14 k
 perl-Thread-Queue           noarch     3.05-2.fc21                                 fedora       18 k
 perl-Time-HiRes             x86_64     1.9726-3.fc21                               fedora       50 k
 perl-Time-Local             noarch     1.2300-292.fc21                             fedora       25 k
 perl-constant               noarch     1.27-293.fc21                               fedora       19 k
 perl-libs                   x86_64     4:5.18.4-306.fc21                           updates     711 k
 perl-macros                 x86_64     4:5.18.4-306.fc21                           updates      50 k
 perl-parent                 noarch     1:0.228-2.fc21                              fedora       13 k
 perl-podlators              noarch     2.5.3-2.fc21                                fedora      114 k
 perl-threads                x86_64     1:1.92-3.fc21                               fedora       55 k
 perl-threads-shared         x86_64     1.46-4.fc21                                 fedora       44 k
 perl-version                x86_64     3:0.99.12-1.fc21                            updates      94 k
 qt                          x86_64     1:4.8.6-25.fc21                             updates     4.8 M
 qt-settings                 noarch     21-3.fc21                                   updates      23 k
 qt-x11                      x86_64     1:4.8.6-25.fc21                             updates      12 M
 tar                         x86_64     2:1.27.1-7.fc21                             fedora      894 k
 xemacs-filesystem           noarch     21.5.34-8.20140605hgacf1c26e3019.fc21       fedora       20 k
Updating for dependencies:
 elfutils-libelf             x86_64     0.161-6.fc21                                updates     205 k
 elfutils-libs               x86_64     0.161-6.fc21                                updates     267 k
 glibc                       x86_64     2.20-8.fc21                                 updates     3.5 M
 glibc-common                x86_64     2.20-8.fc21                                 updates      11 M
 libgcc                      x86_64     4.9.2-6.fc21                                updates      89 k
 libgomp                     x86_64     4.9.2-6.fc21                                updates     127 k
 libstdc++                   x86_64     4.9.2-6.fc21                                updates     299 k

Transaction Summary
======================================================================================================
Install  22 Packages (+81 Dependent packages)
Upgrade              (  7 Dependent packages)

-bash-4.3# pwd
/root
-bash-4.3# curl -O https://ftp.gnu.org/gnu/bash/bash-4.3.30.tar.gz
-bash-4.3# curl -O http://heanet.dl.sourceforge.net/project/bashdb/bashdb/4.3-0.91/bashdb-4.3-0.91.tar.gz
-bash-4.3# ls
anaconda-ks.cfg  bash-4.3.30  bash-4.3.30.tar.gz  bashdb-4.3-0.91  bashdb-4.3-0.91.tar.gz  foo.bash

# yum -y install tar
/ in bash	, 
# CFLAGS="-g" ./configure
# make 
# make install
/ in bashdb	, 
# ./configure
# make 				/ TODO 				
# make install	

-bash-4.3#  strings /usr/local/bin/bash | grep bashdb-main.inc
/usr/local/share/bashdb/bashdb-main.inc
/ Maak soft link	,
-bash-4.3# ls -l /usr/local/share
lrwxrwxrwx.  1 root root   24 Mar 23 16:08 bashdb -> /usr/share/share/bashdb/

-bash-4.3# pwd
/root/bash-4.3.30
-bash-4.3# gdb ./bash
(gdb) b main
(gdb) r --debugger /root/foo.bash
...
477	  arg_index = parse_shell_options (argv, arg_index, argc);
(gdb) s
parse_shell_options (argv=0x7fffffffe4d8, arg_start=2, arg_end=3) at shell.c:823

/ 7	. 

/* Special debugging helper. */
int debugging_login_shell = 0;
/* Values for the long-winded argument names. */
static int debugging;                   /* Do debugging things. */

} long_args[] = {
  { "debug", Int, &debugging, (char **)0x0 },
#if defined (DEBUGGER)
/ JA
  { "debugger", Int, &debugging_mode, (char **)0x0 },
#endif

b shell.c:684

684:  if (command_execution_string)
    {
      arg_index = bind_args (argv, arg_index, argc, 0);
      startup_state = 2;

689:      if (debugging_mode)
        start_debugger ();


723:  if (debugging_mode && locally_skip_execution == 0 && running_setuid == 0 && dollar_vars[1])
    start_debugger ();

1786: debugging = do_version = line_number = last_command_exit_value = 0;

b main
r --debugger /root/foo.bash "" 
b 723
b 1351
b 461
c
p debugging_mode
1

461	  arg_index = parse_long_options (argv, arg_index, argc);
/ hierin wordt debugging_mode=1 set	, omdat we --debugging geven	,

$ vi shell.c
static const struct {
  const char *name;
  int type;
  int *int_value;
  char **char_value;
} long_args[] = {
#if defined (DEBUGGER)
  { "debugger", Int, &debugging_mode, (char **)0x0 },
/ Inderdaad	, &debugging_mode	, 
/ later wordt debugging_mode set tot 1, als we --debugging geven	,


(gdb) s
parse_long_options (argv=0x7fffffffed48, arg_start=1, arg_end=4) at shell.c:767

/ hij gaat vergelijken met	,
(gdb) p long_args
$14 = {{
    name = 0x4da1ca "debug", 
    type = 1, 
    int_value = 0x71e6e0 <debugging>, 
    char_value = 0x0
  }, {
    name = 0x4da1d0 "debugger", 
    type = 1, 
    int_value = 0x71e6b8 <debugging_mode>, 
    char_value = 0x0
  }, {
...

      /* Make --login equivalent to -login. */
      if (arg_string[1] == '-' && arg_string[2])
        {
          longarg = 1;
          arg_string++;
        }

/ dus arg_string="-debugger"	,

      for (i = 0; long_args[i].name; i++)
        {
          if (STREQ (arg_string + 1, long_args[i].name))
/ JA	,
(gdb) p long_args[i]
$16 = {
  name = 0x4da1d0 "debugger", 
  type = 1, 
  int_value = 0x71e6b8 <debugging_mode>, 
  char_value = 0x0
}
              if (long_args[i].type == Int)
                *long_args[i].int_value = 1;
(gdb) p debugging_mode
$17 = 1



/ op 723 is debugging_mode=1	, aan begin van main NIET	,

builtins/common.c:	  dollar_vars[i] = savestring (list->word->word);
shell.c:  dollar_vars[0] = savestring (shell_name);

$ vim shell.c
static int
bind_args (argv, arg_start, arg_end, start_index)
     char **argv;
     int arg_start, arg_end, start_index;
{
  register int i;
  WORD_LIST *args;

1351:  for (i = arg_start, args = (WORD_LIST *)NULL; i < arg_end; i++)

$ vim builtins/common.c

/ in remember_args wordt dollar_vars set	,  en de debugger wordt called, hieboven, als dollar_vars[1] er is	,
dollar_vars[0]="/root/foo.bash"
dollar_vars[1]=""


void
remember_args (list, destructive)
     WORD_LIST *list;
     int destructive;
{
  register int i;

380:  for (i = 1; i < 10; i++)


$ vi shopt.def
} shopt_vars[] = {
#if defined (DEBUGGER)
  { "extdebug", &debugging_mode, (shopt_set_func_t *)NULL },
#endif

/ 13	 .

# CFLAGS="-g" ./configure

$ vi config.h
#define DEBUGGER 1
/ default	,

# shopt -s extdebug
/ Doet WH NIETS	,
/ TODO

/ CFLAGS="-g" komt in de Makefiles	,

/ 13	. 
/ Zonder CFLAGS="-g", dus ./configure	,

bash-4.3# grep CFLAGS -r * | grep O2
Makefile:CFLAGS = -g -O2
Makefile:CFLAGS_FOR_BUILD = -g -O2 
builtins/Makefile:CFLAGS = -g -O2
builtins/Makefile:CFLAGS_FOR_BUILD = -g -O2 
config.log:CFLAGS='-g -O2'
config.log:CFLAGS_FOR_BUILD='-g -O2'
config.status:S["CFLAGS"]="-g -O2"
config.status:S["CFLAGS_FOR_BUILD"]="-g -O2"
examples/loadables/Makefile:CFLAGS = -g -O2
examples/loadables/perl/Makefile:CFLAGS = -g -O2
lib/glob/Makefile:CFLAGS = -g -O2
lib/intl/Makefile:CFLAGS = -g -O2
lib/sh/Makefile:CFLAGS = -g -O2
lib/termcap/Makefile:CFLAGS = -g -O2
lib/malloc/Makefile:CFLAGS = -g -O2
lib/tilde/Makefile:CFLAGS = -g -O2
lib/readline/Makefile:CFLAGS = -g -O2
support/Makefile:CFLAGS = -g -O2
support/Makefile:CFLAGS_FOR_BUILD = -g -O2

/ Met CFLAGS="-g" staat er -g ipv -g -O2 , dus CFLAGS="-g" ./configure	,

/ 13	. 







/ Einde DEBUG BASH

/ HADOOP INSTALL RM

/ 13	. 

/ VOORLOPIG

# export JAVA_HOME=/etc/alternatives/java_sdk
-bash-4.3# curl -O http://mirror.arbitrary.nl/gnu/bash/bash-4.3.30.tar.gz
/ in /root	,
-bash-4.3# tar xvzf bash-4.3.30.tar.gz 

# yum -y install tar
Failed to set locale, defaulting to C
/ TODO
-bash-4.3# tar xvzf bash-4.3.30.tar.gz 
-bash-4.3# cd bash-4.3.30
-bash-4.3# yum -y install make

/ we hebben geen gcc	,
/ geef in google 'yum install gcc'	, 
/ we komen op,
http://www.cyberciti.biz/faq/centos-rhel-7-redhat-linux-install-gcc-compiler-development-tools/
http://yum.baseurl.org/wiki/YumGroups
-bash-4.3# yum grouplist
...
   Development Tools
   C Development Tools and Libraries
/ TODO (nog een keer: glibc , ...)
/ TODO( yum grouplist, list group, groups)

-bash-4.3#  yum groupinfo "C Development Tools and Libraries"


-bash-4.3# yum group install "Development Tools"
-bash-4.3# ./configure 
-bash-4.3# make
-bash-4.3# make install
-bash-4.3# which bash  
/usr/local/bin/bash

# cd 
-bash-4.3# curl -O http://heanet.dl.sourceforge.net/project/bashdb/bashdb/4.3-0.91/bashdb-4.3-0.91.tar.gz

/ click in chrome rechtboven 'customize ...'	, en click downloads, je ziet alle downloads	, zo komen we aan deze url	,

-bash-4.3# tar xvzf bashdb-4.3-0.91.tar.gz 
-bash-4.3# cd bashdb-4.3-0.91
-bash-4.3# ./configure 
-bash-4.3# make install

/ we hebben ook	, ipv ./configure	,
-bash-4.3# ./configure  --enable-debugger

-bash-4.3# which bash
/usr/local/bin/bash
-bash-4.3#  strings /usr/local/bin/bash | grep bashdb-main.inc
/usr/local/share/bashdb/bashdb-main.inc
-bash-4.3# cd /usr/local/share
-bash-4.3# ln /usr/share/share/bashdb/

/ Maar	,
-bash-4.3# bash --debugger foo.bash 
/ NIETS	,

/ Einde HADOOP INSTALL RM

/ BASH

/ 7	. 

/ let	,

[eric@localhost ch7.1]$ a=11
[eric@localhost ch7.1]$ a=$a+5
[eric@localhost ch7.1]$ echo $a
11+5

[eric@localhost ch7.1]$ a=11
[eric@localhost ch7.1]$ let a=a+5
[eric@localhost ch7.1]$ echo $a
16
[eric@localhost ch7.1]$ [[ $a -eq 16 ]]
[eric@localhost ch7.1]$ echo $?
0
[eric@localhost ch7.1]$ [[ $a == 16 ]]
[eric@localhost ch7.1]$ echo $?
0

$ let a=a +15
/ a=a moet aan elkaar	,
/ maar met "" mogen ze los	,
[eric@localhost ch7.1]$ let "a = a + 5"


/ bash lijkt geen types te kennen. Alleen operators: onder let wordt $a als int gezien , met -eq ook als int	, maar zonder let en met -eq als string	,

/ we zien dat ook in	,

[eric@localhost ch7.1]$ let a=11+5
[eric@localhost ch7.1]$ echo "$a"
16
[eric@localhost ch7.1]$ [[ "$a" -eq "16" ]]
[eric@localhost ch7.1]$ echo $?
0

[eric@localhost ch7.1]$ help
...
[eric@localhost ch7.1]$ help let
...

/ 7	.

[eric@localhost ch7.1]$ echo $a
16
[eric@localhost ch7.1]$ a=$a+15
[eric@localhost ch7.1]$ echo $a
16+15
[eric@localhost ch7.1]$ a= $a+15
bash: 16+15+15: command not found...
[eric@localhost ch7.1]$ a =$a+15
bash: a: command not found...
[eric@localhost ch7.1]$ "a =$a+15"
bash: a =16+15+15: command not found...
[eric@localhost ch7.1]$ "a=$a+15"
bash: a=16+15+15: command not found...

[eric@localhost ch7.1]$ $(a=$a+15)
[eric@localhost ch7.1]$ echo $a
16+15
/ WH in subshell	,
[eric@localhost ch7.1]$ $(a=$a+15;echo $a)
bash: 16+15+15: command not found...

/ 13	.

[eric@localhost ch7.1]$ chmod u+x first.bash 
[eric@localhost ch7.1]$  ls
first.bash
[eric@localhost ch7.1]$ $(ls)
bash: first.bash: command not found...
[eric@localhost ch7.1]$ ./$(ls)
1
1


[eric@localhost ch7.1]$ echo $a
16+15
[eric@localhost ch7.1]$ echo $(a=$a+15)

[eric@localhost ch7.1]$ echo $(a=$a+15;echo $a)
16+15+15

/ als we	,
[eric@localhost ch7.1]$ a=$a+15
[eric@localhost ch7.1]$
/ dan zien we ook niets	,

/ 13	. 

[eric@localhost ch7.1]$ (ls)
first.bash
[eric@localhost ch7.1]$ (a=3)
[eric@localhost ch7.1]$ echo $a
16+15+15+15

/ In de subshell wordt er naar stdout write door ls	, dus zien we het result	,
/ in de subshell wordt a=3,	 maar in onze shell is a een andere	,

/ 13	.

/ Lees	,
http://www.tldp.org/LDP/abs/html/commandsub.html
In a more technically correct sense, command substitution extracts the stdout of a command, then assigns it to a variable using the = operator.

/ als we normaal een string (command) geven achter de prompt	, en we geven enter, dan wordt dit command exec	, en result naar stdout	, 

[eric@localhost ch7.1]$ ls
first.bash

[eric@localhost ch7.1]$ $(ls)
/=
[eric@localhost ch7.1]$ first.bash
bash: first.bash: command not found...
[eric@localhost ch7.1]$ b=$(ls)
[eric@localhost ch7.1]$ echo $b
first.bash

/ Dus $ ls en $(ls) is eig. hetzelfde: beide hebben result first.bash	, maar bij $ ls , dus bij de top-shell is stdout de terminal, en bij $(ls) is stdout subshell=stdin van de top-shell, dus gaat first.bash de stdin van de top-shell in	,

[eric@localhost ch7.1]$ (ls)
first.bash
[eric@localhost ch7.1]$ $(ls)
bash: first.bash: command not found...

/ bij 1ste gaat result naar stdout van subshell=terminal	,
/ bij 2de gaat result naar stdout van subshell=stdin van top-shell	,
/ TODO


/ 13	. 

$ a=$a+15
$
/ dus result van = is ""	, er gaat een "" naar de terminal	,

[eric@localhost ch7.1]$ b=$(a=$a+15)
[eric@localhost ch7.1]$ echo $b

[eric@localhost ch7.1]$ echo -n $b
[eric@localhost ch7.1]$ [[ $b -eq "" ]]
[eric@localhost ch7.1]$ echo $?
0
/ TODO

[eric@localhost ch7.1]$ a=b=7
[eric@localhost ch7.1]$ echo $a
b=7
/ Dus niet zoals in C	,

[eric@localhost ch7.1]$ $(a=3)
[eric@localhost ch7.1]$ echo $a
b=7

/ 7	.

$ a=16
/ moet allemaal aan elkaar	,	
[eric@localhost ch7.1]$ b=$a
/ moet allemaal aan elkaar	, want	,
[eric@localhost ch7.1]$ b =$a
bash: b: command not found...
[eric@localhost ch7.1]$ b= $a
bash: 16: command not found...

/ 7	. 

/ we kunnen wel ((11+5)) geven	, niet a of $a	, 

[eric@localhost ch7.1]$ a
bash: a: command not found...
[eric@localhost ch7.1]$ $a
bash: 16: command not found...
[eric@localhost ch7.1]$ ((11+5))
[eric@localhost ch7.1]$ echo $?
0
[eric@localhost ch7.1]$ ((11-11))
[eric@localhost ch7.1]$ echo $?
1
/ de exit status is of het result 0 is of niet	,

/ bij [ of [[ moet er altijd een space , bij (( niet	, 

[eric@localhost ch7.1]$ a=$((11+5))
[eric@localhost ch7.1]$ a=$(( 11+5 ))
/ Beide OK	,

/ 7	.

/ if kijkt naar $? van de cmd achter if	,
/ bij let a++ is $? de vorige value van a	, 

[eric@localhost ch7.1]$ [ 0 ] && echo "OK"
OK
[eric@localhost ch7.1]$ if test 0 ;then echo "OK";fi
OK
[eric@localhost ch7.1]$ if [ 0 ] ;then echo "OK";fi
OK
[eric@localhost ch7.1]$ if 0 ;then echo "OK";fi
bash: 0: command not found...

[eric@localhost ch7.1]$ [ 0 ]
[eric@localhost ch7.1]$ echo $?
0

[eric@localhost ch7.1]$ ls
first.bash
[eric@localhost ch7.1]$ if ls ; then echo "OK";fi
first.bash
OK
[eric@localhost ch7.1]$ if (( 0 )) ; then echo "OK";fi
[eric@localhost ch7.1]$ if (( 1 )) ; then echo "OK";fi
OK
[eric@localhost ch7.1]$ if let a=3 ; then echo "OK";fi
OK
[eric@localhost ch7.1]$ if let a=0 ; then echo "OK";fi

[eric@localhost ch7.1]$ a=0
[eric@localhost ch7.1]$ let a++
[eric@localhost ch7.1]$ echo $?
1
[eric@localhost ch7.1]$ echo $a
1

[eric@localhost ch7.1]$ if $a -eq 1;then echo OK;fi
bash: 1: command not found...
[eric@localhost ch7.1]$ if [  $a -eq 1 ];then echo OK;fi
OK

/ 7	. 

/ Lees over exec	, ch 15	,
http://tldp.org/LDP/abs/html/internal.html

/ Lees over strings	, ch 10	,
http://tldp.org/LDP/abs/html/string-manipulation.html

/ 7	. 

/ exec

[eric@localhost My]$ help exec
exec: exec [-cl] [-a name] [command [arguments ...]] [redirection ...]
...

/ 13	 .
//////////////////////////////////////////////
/ Wat we hieronder zien zijn java problemen

[eric@localhost My]$ [[ $CLASSPATH == dist/* ]];echo $?
0
/ or	,
[eric@localhost My]$ echo "$CLASSPATH"
dist/*

[eric@localhost My]$ "java" "-Djava.tmp=/tmp" "-Djava.sub=/sub" "Foo"
/tmp
[eric@localhost My]$ "java" "-Djava.tmp=/tmp" " -Djava.sub=/sub" "Foo"
Error: Could not find or load main class  -Djava.sub=.sub
[eric@localhost My]$ "java" " -Djava.tmp=/tmp" "-Djava.sub=/sub" "Foo"
Error: Could not find or load main class  -Djava.tmp=.tmp

////////////////////////
/ als de string NIET met '-' begint , vindt java de string, die met een space begint een class name	, 

/ Ook	,
[eric@localhost My]$ "java" "-Djava.tmp=/tmp -Djava.sub=/sub" "Foo"
/tmp -Djava.sub=/sub
[eric@localhost My]$ "java" "-Djava.sub=/sub -Djava.tmp=/tmp" "Foo"
null
[eric@localhost My]$ "java" -Djava.sub=/sub -Djava.tmp=/tmp "Foo"
/tmp



/ javac heeft dit ook	,

[eric@localhost My]$ "javac" "sub/Foo.java"
[eric@localhost My]$ "javac" " sub/Foo.java"
javac: file not found:  sub/Foo.java



/ we kijken of C ook zo doet	,
/ JA	,

[eric@localhost scripts]$ sudo yum groupinstall "C Development Tools and Libraries"

[eric@localhost My]$ "cc" "-Wall" "-g" "foo.c"
[eric@localhost My]$ "cc" " -Wall" "-g" "foo.c"
cc: error:  -Wall: No such file or directory








/ 13	 

$ vi foo.bash

exec ls -l
/ OK
exec "ls -l"
./foo.bash: line 6: exec: ls -l: not found
exec "ls" "-l"
/ OK
exec "ls " "-l"
foo.bash: line 6: exec: ls : not found
exec "ls" " -l"
ls: cannot access  -l: No such file or directory
exec "ls" "-l "
ls: invalid option -- ' '
exec " ls" "-l"
foo.bash: line 6: exec:  ls: not found




/ 13	.

[eric@localhost My]$ pwd
/home/eric/Devel/Bash/My
$ vi sub/Foo.java
/ calls System.getProperty("java.tmp") , and prints it	,
$ javac sub/Foo.java
[eric@localhost My]$ ls sub
Foo.class  Foo.java
[eric@localhost My]$ (cd sub;jar cvf ../dist/foo.jar Foo.class)
[eric@localhost My]$ ls dist
foo.jar

$ vi foo.bash

javac sub/Foo.java
export CLASSPATH=dist/*
export OPTS=" -Djava.tmp=/tmp"
exec java $OPTS Foo

/  We doen steeds	,
[eric@localhost My]$ ./foo.bash 

exec java $OPTS Foo
/ OK
/tmp
exec "java" "$OPTS" "Foo"
/ OK
/tmp
exec "java " "$OPTS" "Foo"
foo.bash: line 5: exec: java : not found
exec "java" " $OPTS" "Foo"
Error: Could not find or load main class  -Djava.tmp=.tmp
exec "java" "$OPTS " "Foo"
/ OK
/tmp
exec "java" "$OPTS" " Foo"
Error: Could not find or load main class  Foo





exec "java" "$OPTS" "Foo"
Error: Could not find or load main class  -Djava.tmp=.tmp
/ Let op .tmp ipv /tmp	, dat doet java WH	,

exec "java" $OPTS "Foo"


/ 13	. 

javac sub/Foo.java
export CLASSPATH=dist/*:dist2/*
OPTS="-Djava.tmp=/tmp  -Djava.other=/other"
OTHER="-Djava.sub=/sub"
exec "java" $OTHER "$OPTS" Foo

[eric@localhost My]$ bash --debugger foo.bash
/tmp  -Djava.other=/other








/ 13	. 




/ Einde BASH

/ JAVA

/ 7	.

/ * expands niet achter =	, niet tussen [[ ]]	,
[eric@localhost My]$ CLASSPATH=dist/*
[eric@localhost My]$ [[ $CLASSPATH == "dist/*" ]]			/ NIET expanded	, TODO
/=
[eric@localhost My]$ [[ "$CLASSPATH" == "dist/*" ]]
/=
[eric@localhost My]$ [[ "$CLASSPATH" == dist/* ]]
[eric@localhost My]$ echo $?
0
[eric@localhost My]$ echo $CLASSPATH
dist/bar.jar dist/foo.jar
[eric@localhost My]$ echo "$CLASSPATH"
dist/*

/ Dus $CLASSPATH == dist/*	, maar achter echo expands * tot de files	,

/ Lees ch 18.2	, over * en ?	,
http://tldp.org/LDP/abs/html/globbingref.html

/ Klopt	, * expands niet tussen ""	,
/ dit staat in ch 5.1	,
When referencing a variable, it is generally advisable to enclose its name in double quotes. This prevents reinterpretation of all special characters within the quoted string -- except $, ` (backquote), and \ (escape).

[eric@localhost My]$ ls dist/*
dist/bar.jar  dist/foo.jar
[eric@localhost My]$ ls "dist/*"
ls: cannot access dist/*: No such file or directory

/ we moeten 
$ CLASSPATH=dist/*
of
$ CLASSPATH="dist/bar.jar:dist/foo.jar"
/en NIET,
$ CLASSPATH="dist/bar.jar dist/foo.jar"
/ dat gaan we zo zien	,

/ eerst kijken we hoe java reacts	,
[eric@localhost My]$ java -cp dist/bar.jar dist/foo.jar Foo
Error: Could not find or load main class dist.foo.jar
/ De ERR msg van java vervangt / door .

/ deze msg krijgen we als we 	,
[eric@localhost My]$ CLASSPATH="dist/*"
[eric@localhost My]$ java -classpath $CLASSPATH Foo
Error: Could not find or load main class dist.foo.jar

[eric@localhost My]$ export CLASSPATH
[eric@localhost My]$ java Foo
Foo
[eric@localhost My]$ CLASSPATH="dist/bar.jar:dist/foo.jar"
[eric@localhost My]$ java Foo
Foo
/ Ook hier moet CLASSPATH exported zijn	,

[eric@localhost My]$ CLASSPATH="dist/bar.jar dist/foo.jar"
[eric@localhost My]$ java Foo
Error: Could not find or load main class Foo

/ Ook OK is meerdere dist/*'s	, met : ertussen	,
[eric@localhost My]$ CLASSPATH=dist/*:dist2/*
[eric@localhost My]$ export CLASSPATH
[eric@localhost My]$ java Foo
Foo

/ Ook als we , dan wordt * NIET expanded	,
[eric@localhost My]$ [[ $CLASSPATH == dist/* ]]
[eric@localhost My]$ echo $?
0
[eric@localhost My]$ CLASSPATH=$CLASSPATH
[eric@localhost My]$ [[ $CLASSPATH == dist/* ]]
[eric@localhost My]$ echo $?
0





/ 7	.

[eric@localhost My]$ export CLASSPATH=dist/*
[eric@localhost My]$ java Foo
Foo
[eric@localhost My]$ export CLASSPATH="dist/bar.jar dist/foo.jar"
[eric@localhost My]$ java Foo
Error: Could not find or load main class Foo



/ 7	.

/ In CLASSPATH kunnen dirs en jars staan	,

/ Als in sub/ .class files en ze moeten in classpath, doe dan CLASSPATH=sub	, zoals we altijd doen	,
/ Als in dist/ .jar files en ze moeten in classpath, doe dan CLASSPATH=dist/*	, 

/ als CLASSPATH=dist/*	, dan is het java zelf die CLASSPATH expands tot dist/bar.jar:dist/foo.jar	, 
/ doe dus NIET: -cp $CLASSPATH	, want dan expand bash dit tot -cp dist/bar.jar dist/foo.jar	, en dit gaat ERR	,

[eric@localhost My]$ (cd sub/;jar cvf ../dist/foo.jar Foo.class)
[eric@localhost My]$ (cd sub/;jar cvf ../dist/bar.jar Foo.class)


[eric@localhost My]$ pwd
/home/eric/Devel/Bash/My
[eric@localhost My]$ ls sub/
Foo.class  Foo.java
[eric@localhost My]$ ls dist
foo.jar

/ Foo zit in default package	,

[eric@localhost My]$ cat foo.bash 
#!/usr/bin/bash
javac sub/Foo.java

export CLASSPATH=dist/*
/ of	,
export CLASSPATH=sub

java Foo

/ OK	,

/ Einde JAVA



/ ANDROID 

/ op 11/8/14 (8 nov) kregen we email van Matthew Feigal, met link naar slides van zijn presentatie	,
https://docs.google.com/presentation/d/11zUaraMhtgZitxL0Xi8NQedCVLXr0dv00mP46xVFbEQ/edit#slide=id.g1d4e015bb_551
/ we moeten dan gaan naar	,
https://github.com/GoogleCloudPlatform/endpoints-codelab-android


/ Einde ANDROID 

/ KUBERNETES

/ Lees	,
https://github.com/GoogleCloudPlatform/kubernetes
/ we zien docs	,
https://github.com/GoogleCloudPlatform/kubernetes/tree/master/docs
/ we zien	,
node.md			/ wat is een node (minion)
getting-started-guides/
/ hierin zien we fedora
https://github.com/GoogleCloudPlatform/kubernetes/tree/master/docs/getting-started-guides/fedora
/ we zien fedora_manual_config.md	,
https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/getting-started-guides/fedora/fedora_manual_config.md

/ Lees	,
http://kubernetes.io/gettingstarted/
/ daar link Fedora(manual) 	,
https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/getting-started-guides/fedora/fedora_manual_config.md

/ 7	. 

yum -y install --enablerepo=updates-testing kubernetes
Installing:
 kubernetes             x86_64             0.14.2-0.2.gitd577db9.fc21             updates-testing              17 M

[eric@localhost postgresql]$ repoquery -l kubernetes
/etc/kubernetes
/etc/kubernetes/apiserver
/etc/kubernetes/config
/etc/kubernetes/controller-manager
/etc/kubernetes/kubelet
/etc/kubernetes/proxy
/etc/kubernetes/scheduler
/usr/bin/kube-apiserver
/usr/bin/kube-controller-manager
/usr/bin/kube-proxy
/usr/bin/kube-scheduler
/usr/bin/kubectl
/usr/bin/kubelet
/usr/lib/systemd/system/kube-apiserver.service
/usr/lib/systemd/system/kube-controller-manager.service
/usr/lib/systemd/system/kube-proxy.service
/usr/lib/systemd/system/kube-scheduler.service
/usr/lib/systemd/system/kubelet.service
/usr/lib/tmpfiles.d/kubernetes.conf
/usr/share/bash-completion/completions/kubectl
/usr/share/doc/kubernetes
/usr/share/doc/kubernetes/CONTRIB.md
/usr/share/doc/kubernetes/CONTRIBUTING.md
/usr/share/doc/kubernetes/DESIGN.md
/usr/share/doc/kubernetes/LICENSE
/usr/share/doc/kubernetes/README.md
/usr/share/man/man1/kube-apiserver.1.gz
/usr/share/man/man1/kube-controller-manager.1.gz
/usr/share/man/man1/kube-proxy.1.gz
/usr/share/man/man1/kube-scheduler.1.gz
/usr/share/man/man1/kubectl-apiversions.1.gz
/usr/share/man/man1/kubectl-clusterinfo.1.gz
/usr/share/man/man1/kubectl-config-set-cluster.1.gz
/usr/share/man/man1/kubectl-config-set-context.1.gz
/usr/share/man/man1/kubectl-config-set-credentials.1.gz
/usr/share/man/man1/kubectl-config-set.1.gz
/usr/share/man/man1/kubectl-config-unset.1.gz
/usr/share/man/man1/kubectl-config-use-context.1.gz
/usr/share/man/man1/kubectl-config-view.1.gz
/usr/share/man/man1/kubectl-config.1.gz
/usr/share/man/man1/kubectl-create.1.gz
/usr/share/man/man1/kubectl-delete.1.gz
/usr/share/man/man1/kubectl-describe.1.gz
/usr/share/man/man1/kubectl-exec.1.gz
/usr/share/man/man1/kubectl-expose.1.gz
/usr/share/man/man1/kubectl-get.1.gz
/usr/share/man/man1/kubectl-label.1.gz
/usr/share/man/man1/kubectl-log.1.gz
/usr/share/man/man1/kubectl-namespace.1.gz
/usr/share/man/man1/kubectl-port-forward.1.gz
/usr/share/man/man1/kubectl-proxy.1.gz
/usr/share/man/man1/kubectl-resize.1.gz
/usr/share/man/man1/kubectl-rollingupdate.1.gz
/usr/share/man/man1/kubectl-run-container.1.gz
/usr/share/man/man1/kubectl-stop.1.gz
/usr/share/man/man1/kubectl-update.1.gz
/usr/share/man/man1/kubectl-version.1.gz
/usr/share/man/man1/kubectl.1.gz
/usr/share/man/man1/kubelet.1.gz
/var/lib/kubelet

[eric@localhost postgresql]$ sudo yum -y install etcd iptables
Installing:
 etcd                   x86_64                   2.0.8-0.1.fc21                     updates                   4.2 M

[eric@localhost postgresql]$ repoquery -l etcd
/etc/etcd
/etc/etcd/etcd.conf
/usr/bin/etcd
/usr/bin/etcd-migrate
/usr/bin/etcdctl
/usr/lib/systemd/system/etcd.service
/usr/share/doc/etcd
/usr/share/doc/etcd/Godeps.json
/usr/share/doc/etcd/LICENSE
/usr/share/doc/etcd/README.md
/usr/share/doc/etcd/internal-protocol-versioning.md
/var/lib/etcd

/ 7	.

/ we doen op de laptop	,

[eric@localhost postgresql]$ sudo hostnamectl set-hostname almond.nuts.org

[eric@localhost postgresql]$ cat /etc/hosts
127.0.0.1		localhost.localdomain localhost
::1		localhost6.localdomain6 localhost6

192.168.123.182 almond.nuts.org almond
fe80::2e44:fdff:fe68:82d4 almond.nuts.org almond

/ we kunnen	,
$ ssh almond
$ ssh almond.nuts.org





/ Einde KUBERNETES

/ AINSIBLE

/ Einde AINSIBLE


/ RECENTE SITES


http://forums.fedoraforum.org/index.php

https://github.com/nginxinc/docker-nginx/blob/e35d3022416ca5221fcf958ed8aa6a55a5b043af/Dockerfile
https://registry.hub.docker.com/u/sequenceiq/hadoop-docker/dockerfile/
https://github.com/docker-library/postgres/blob/e616341507a7beec3a161b0a366ba0d3400328fd/9.4/Dockerfile
https://github.com/docker-library/httpd/blob/9c77579dcf981f060732bf41845edea8e39a130b/2.4/Dockerfile
https://registry.hub.docker.com/_/centos/
https://docs.docker.com/examples/postgresql_service/

http://new.livestream.com/accounts/1545775/osdi12/videos/4646642

http://dev.mysql.com/doc/refman/5.6/en/innodb-performance-ro-txn.html
/ Einde RECENTE SITES

/ LINUX CONTAINERS

/ 7	. 

/ Lees	,
https://linuxcontainers.org/



/ Einde LINUX CONTAINERS

/ DOCKER POSTGRESQL 

/ 7	. 

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/postgresql/fedora
[eric@localhost fedora]$ cat Dockerfile 

from fedora:21 
run yum -y install postgresql-server
run su - postgres -c "pg_ctl initdb"
run echo "listen_addresses='*'" >>/var/lib/pgsql/data/postgresql.conf
run echo "host all postgres 172.17.0.0/16 trust">>/var/lib/pgsql/data/pg_hba.conf
run echo "host all all 172.17.0.0/16 md5">>/var/lib/pgsql/data/pg_hba.conf

run yum -y install net-tools 		
# ifconfig	,
run yum -y install iputils 			
# ping

expose 5432
volume /mydir
cmd su - postgres -c postgres

[eric@localhost fedora]$ sudo docker build --rm -t local/fd-postgres .
/ OK

[eric@localhost Docker]$ sudo docker run --rm --name db local/fd-postgres
/ OK

/ we kunnen op onze laptop	,
[eric@localhost Docker]$ sudo docker port sshd
22/tcp -> 0.0.0.0:49177
[eric@localhost Docker]$ sudo su - postgres -c "psql"
psql (9.4.1)
Type "help" for help.
postgres=# \q

/ we kunnen ook	,

[eric@localhost Docker]$ sudo docker exec -it db /bin/bash
bash-4.3# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?           -1 Ss       0   0:00 su - postgres -c postgres
    1     8     8     8 ?           -1 Ss      26   0:00 postgres
    8    19    19    19 ?           -1 Ss      26   0:00 postgres: logger process  
    8    21    21    21 ?           -1 Ss      26   0:00 postgres: checkpointer process  
    8    22    22    22 ?           -1 Ss      26   0:00 postgres: writer process  
    8    23    23    23 ?           -1 Ss      26   0:00 postgres: wal writer process  
    8    24    24    24 ?           -1 Ss      26   0:00 postgres: autovacuum launcher process  
    8    25    25    25 ?           -1 Ss      26   0:00 postgres: stats collector process  
    0    26    26     0 ?           32 S        0   0:00 /bin/bash
   26    32    32     0 ?           32 R+       0   0:00 ps ajx

bash-4.3# su - postgres -c "pg_ctl stop"
waiting for server to shut down....[eric@localhost Docker]$ 
/ of	,
bash-4.3# kill -9 7
/ De container stops	, 

/ we zien dat we geen pg_ctl stop/restart kunnen doen	, 

/ we kunnen 	,


/ 13	. 

$ vi Dockerfile
...
#cmd su - postgres -c postgres
add my.sh /tmp/
cmd /tmp/my.sh

[eric@localhost fedora]$ cat my.sh 
su - postgres -c "pg_ctl start"
/usr/bin/bash

[eric@localhost fedora]$ sudo docker build --rm -t local/fd-postgres .
/ OK
[eric@localhost Docker]$ sudo docker run --rm --name db -it local/fd-postgres
bash-4.3# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?           30 Ss       0   0:00 /bin/sh -c /tmp/my.sh
    1    21     8     8 ?           -1 S       26   0:00 /usr/bin/postgres
    1    22    22     1 ?           30 S        0   0:00 /usr/bin/bash
   21    23    23    23 ?           -1 Ss      26   0:00 postgres: logger process  
   21    25    25    25 ?           -1 Ss      26   0:00 postgres: checkpointer process  
   21    26    26    26 ?           -1 Ss      26   0:00 postgres: writer process  
   21    27    27    27 ?           -1 Ss      26   0:00 postgres: wal writer process  
   21    28    28    28 ?           -1 Ss      26   0:00 postgres: autovacuum launcher process  
   21    29    29    29 ?           -1 Ss      26   0:00 postgres: stats collector process  
   22    30    30     1 ?           30 R+       0   0:00 ps ajx
bash-4.3#  bash-4.3# su - postgres -c "pg_ctl restart"
/ OK

/13	. 

[eric@localhost fedora]$ cat my.sh 
#!/usr/bin/bash
su - postgres -c "pg_ctl start"
/usr/bin/bash

[eric@localhost fedora]$ chmod u+x my.sh 

/ Let op dat we  in my.sh	, anders ERR in container	,
#!/usr/bin/bash


$ vi Dockerfile
...
add my.sh /tmp/ 
cmd ["/tmp/my.sh"]

[eric@localhost fedora]$ sudo docker build --rm -t local/fd-postgres .

[eric@localhost Docker]$ sudo docker run --rm --name pg -it -P local/fd-postgres
bash-4.3# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?           29 Ss       0   0:00 /usr/bin/bash /tmp/my.sh
    1    20     7     7 ?           -1 S       26   0:00 /usr/bin/postgres
    1    21    21     1 ?           29 S        0   0:00 /usr/bin/bash
   20    22    22    22 ?           -1 Ss      26   0:00 postgres: logger process  
   20    24    24    24 ?           -1 Ss      26   0:00 postgres: checkpointer process  
   20    25    25    25 ?           -1 Ss      26   0:00 postgres: writer process  
   20    26    26    26 ?           -1 Ss      26   0:00 postgres: wal writer process  
   20    27    27    27 ?           -1 Ss      26   0:00 postgres: autovacuum launcher process  
   20    28    28    28 ?           -1 Ss      26   0:00 postgres: stats collector process  
   21    29    29     1 ?           29 R+       0   0:00 ps ajx

/ we hebben een bash, maar we kunnen als we willen er nog een krijgen door in een ander window	,
[eric@localhost Docker]$ sudo docker exec -it pg /bin/bash
bash-4.3# 


/ -P is voor als je vanaf laptop psql wilt doen	, naar postgres in container	,

/ we zien nu het verschil , in Dockerfile , tussen
cmd ["/tmp/my.bash"]
/ en 
cmd /tmp/bash 
/ we zien 	,
    0     1     1     1 ?           30 Ss       0   0:00 /bin/sh -c /tmp/my.sh
/ en
    0     1     1     1 ?           29 Ss       0   0:00 /usr/bin/bash /tmp/my.sh

/ 7 . 

/ pg_hba.conf 

/ 13	. 

/ default	,
# "local" is for Unix domain socket connections only
local   all             all                                     trust
# IPv4 local connections:
host    all             all             127.0.0.1/32            trust
# IPv6 local connections:
host    all             all             ::1/128                 trust

/ in container zelf
bash-4.3# su - postgres -c "pg_ctl reload" 

/ Vanwege de 1ste regel	,
/ In de container zelf kunnen we	,
bash-4.3# psql -U postgres
/ of	,
bash-4.3# su - postgres -c psql
psql (9.3.6)
Type "help" for help.
postgres=# \q

/ als we trust vervangen door md5	, moet postgres password geven	,
local   all             all                                    	md5 
/ op machine zelf
bash-4.3# su - postgres -c "pg_ctl reload" 

/ op machine zelf
bash-4.3# psql -U postgres
/ of	,
bash-4.3# su - postgres -c psql
Password: 

/ 13	. 

/ als we vanaf laptop psql -h localhost doen	, moeten we in pg_hba.conf de ipv6 regel config	,

host    all             all             ::1/128                 trust
bash-4.3# su - postgres -c "pg_ctl reload"
server signaled
bash-4.3# psql -U postgres -h localhost
/of	,
bash-4.3# su - postgres -c "psql -h localhost"
psql (9.3.6)
Type "help" for help.
postgres=# \q

host    all             all             ::1/128                 md5
bash-4.3# su - postgres -c "pg_ctl reload"
server signaled
bash-4.3# psql -U postgres -h localhost
/ of	,
bash-4.3# su - postgres -c "psql -h localhost"
Password: 
psql: fe_sendauth: no password supplied

/ 13	. 

/ voor 127.0.0.1	,

host    all             all             127.0.0.1/32           trust

bash-4.3# su - postgres -c "pg_ctl reload"
server signaled
bash-4.3# psql -U postgres -h 127.0.0.1
psql (9.3.6)
Type "help" for help.
postgres=# \q

host    all             all             127.0.0.1/32           md5
bash-4.3# su - postgres -c "pg_ctl reload"
server signaled
bash-4.3# su - postgres -c "psql -h 127.0.0.1"
/ of	,
bash-4.3# psql -U postgres -h 127.0.0.1
Password for user postgres: 



/ 13	.

bash-4.3# cat /etc/hosts
172.17.0.71	196f23b98cb1
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback


/ voor ping	,
/ Lees	,
https://bugzilla.redhat.com/show_bug.cgi?id=1142311

[eric@localhost Docker]$ sudo docker run --rm --name pg -it -P --cap-add net_raw --cap-add net_admin local/fd-postgres
/of	,
[eric@localhost Docker]$ sudo docker run --rm --name pg -it -P local/fd-postgres
bash-4.3# setcap cap_net_raw,cap_net_admin+p /usr/bin/ping 

local   all             all                                    	md5
host    all             all             127.0.0.1/32           	md5
host    all             all             ::1/128                 md5
host 	all 			postgres 		172.17.0.64/32 			trust

bash-4.3# psql -U postgres -h 172.17.0.71
/of	,
bash-4.3# PGPASSWORD=foo psql -U foo  -h 196f23b98cb1
postgres=# \dg
                             List of roles
 Role name |                   Attributes                   | Member of 
-----------+------------------------------------------------+-----------
 postgres  | Superuser, Create role, Create DB, Replication | {}

postgres=# create role foo password 'foo' superuser createrole createdb replication;
CREATE ROLE
postgres=# alter role foo login;
ALTER ROLE
postgres=# \q

/////////////////
/ de db is trouwens iets van de container	, 
/ als we een nieuwe container start	, is role foo er niet in	,

bash-4.3# setcap cap_net_raw,cap_net_admin+p /usr/bin/ping6
bash-4.3# ping6 localhost
/ OK
bash-4.3# ping6 196f23b98cb1
/ of	,
bash-4.3# ping6 fe80::42:acff:fe11:47
connect: Invalid argument
/ ERR 
/ Lees	,
https://blogs.gentoo.org/eva/2010/12/17/things-you-didnt-known-about-ipv6-link-local-address/
bash-4.3# ping6 -I eth0 fe80::42:acff:fe11:47
PING fe80::42:acff:fe11:47(fe80::42:acff:fe11:47) from fe80::42:acff:fe11:47 eth0: 56 data bytes
64 bytes from fe80::42:acff:fe11:47: icmp_seq=1 ttl=64 time=0.091 ms
/ OK
bash-4.3# ping6 -I eth0 196f23b98cb1
PING 196f23b98cb1(196f23b98cb1) from fe80::42:acff:fe11:47 eth0: 56 data bytes
64 bytes from 196f23b98cb1: icmp_seq=1 ttl=64 time=0.084 ms
/ OK
bash-4.3# ping6 -I eth0 www.google.com
connect: Network is unreachable
/ TODO

/ wat zijn	,
But it does not work on a linux client for ipv6 with ipv6 link-local addresses:
/ TODO


/ we zien geen verschil bij	, maar bij 127.0.0.1 en localhost was dat wel, want localhost = ::1	,
bash-4.3# psql -U postgres -h 172.17.0.71
bash-4.3# PGPASSWORD=foo psql -U foo  -h 196f23b98cb1
/ TODO


# ifconfig
eth0: flags=67<UP,BROADCAST,RUNNING>  mtu 1500
        inet 172.17.0.71  netmask 255.255.0.0  broadcast 0.0.0.0
        inet6 fe80::42:acff:fe11:47  prefixlen 64  scopeid 0x20<link>
        ether 02:42:ac:11:00:47  txqueuelen 0  (Ethernet)
lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 0  (Local Loopback)

/ 13	. 

host all postgres 172.17.0.71/32  trust
host all postgres fe80::42:acff:fe11:47/128  trust

bash-4.3# psql -U postgres -h fe80::42:acff:fe11:47
psql: could not connect to server: Invalid argument
/ TODO

/ 13	.

/ connectie van buiten	,

/ op de laptop	,
/ als we 2 containers	,
[eric@localhost Docker]$ ifconfig
docker0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 172.17.42.1  netmask 255.255.0.0  broadcast 0.0.0.0
        inet6 fe80::5484:7aff:fefe:9799  prefixlen 64  scopeid 0x20<link>
        ether 56:84:7a:fe:97:99  txqueuelen 0  (Ethernet)

vetha270870: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet6 fe80::e02f:ffff:fefe:40e0  prefixlen 64  scopeid 0x20<link>
        ether e2:2f:ff:fe:40:e0  txqueuelen 0  (Ethernet)

vethe96b976: flags=67<UP,BROADCAST,RUNNING>  mtu 1500
        inet6 fe80::4c32:88ff:fef6:d8c0  prefixlen 64  scopeid 0x20<link>
        ether 4e:32:88:f6:d8:c0  txqueuelen 0  (Ethernet)
/ TODO

host all postgres 172.17.42.1/32 trust

/ dit is de regel zodat we zonder pw van de laptop kunnen connect naar pg	, 
/ maar op de laptop kunnen we ook localhost geven	,

bash-4.3# su - postgres -c "pg_ctl reload"
server signaled
/ in de container	,

/ op de laptop	,
[eric@localhost Docker]$ psql -U postgres -h 172.17.42.1 -p 49184
/ of	,
[eric@localhost Docker]$ psql -U postgres -h localhost -p 49184
psql (9.4.1, server 9.3.6)
Type "help" for help.
postgres=# \q
/ OK

[eric@localhost Docker]$ psql -U postgres -h 172.17.42.1
Password for user postgres: 
/ TODO

[eric@localhost Docker]$ PGPASSWORD=foo psql -U foo -h localhost -p 49184 postgres
psql (9.4.1, server 9.3.6)
Type "help" for help.
postgres=# 
/ OK

/ Einde DOCKER POSTGRESQL

/ DOCKER POSTGRESQL SSHD

/ 7	.

/ container met postgres + sshd	,

[eric@localhost fedora2]$ pwd
/home/eric/Devel/Docker/postgresql/fedora2
$ vi Dockerfile	

from fedora:21 

run yum -y install postgresql-server
run yum -y install net-tools iputils

run su - postgres -c "pg_ctl initdb"
run echo "listen_addresses='*'" >>/var/lib/pgsql/data/postgresql.conf
run echo "host all postgres 172.17.0.0/16 trust">>/var/lib/pgsql/data/pg_hba.conf
run echo "host all all 172.17.0.0/16 md5">>/var/lib/pgsql/data/pg_hba.conf

run yum -y install openssh-server openssh-clients
run yum clean all

run awk -i inplace '{print gensub("(#)(PermitRootLogin yes)","\\2","g")}' /etc/ssh/sshd_config

run ssh-keygen -A

run ssh-keygen -N "" -t rsa -f ~/.ssh/id_rsa
run cat /root/.ssh/id_rsa.pub >>~/.ssh/authorized_keys

run if [ ! -e ~/.ssh ];then mkdir ~/.ssh;fi
copy id_dsa.pub /tmp/
run cat  /tmp/id_dsa.pub  >> ~/.ssh/authorized_keys

expose 5432 22

add my.sh /tmp/ 
cmd ["/tmp/my.sh"]

[eric@localhost fedora2]$ sudo docker build --rm -t local/fd-postgres-sshd .

[eric@localhost fedora2]$ cat my.sh 
#!/usr/bin/bash
su - postgres -c "pg_ctl start"
/usr/sbin/sshd
/usr/bin/bash

/ we doen NIET	, anders krijgen we geen bash er na	, 
/usr/sbin/sshd -D

/ we starten bijv 2 containers	,

/ Waarom 
/ TODO

[eric@localhost Docker]$ sudo docker run --rm --name pgssh -it -P local/fd-postgres-sshd
bash-4.3#  cat /etc/hosts
172.17.0.88	709bee8cf25e
127.0.0.1	localhost
...
bash-4.3# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?           31 Ss       0   0:00 /usr/bin/bash /tmp/my.sh
    1    20     7     7 ?           -1 S       26   0:00 /usr/bin/postgres
    1    22    22    22 ?           -1 Ss       0   0:00 /usr/sbin/sshd
    1    23    23     1 ?           31 S        0   0:00 /usr/bin/bash
...

[eric@localhost Docker]$ sudo docker run --rm --name pgssh2 -it -P local/fd-postgres-sshd
bash-4.3# setcap cap_net_raw,cap_net_admin+p /usr/bin/ping
bash-4.3# head /etc/hosts
172.17.0.90	12638e6def49
127.0.0.1	localhost
...

/ we kunnen op pgssh	,
bash-4.3# ssh 172.17.0.90
The authenticity of host '172.17.0.90 (172.17.0.90)' can't be established.
ECDSA key fingerprint is 82:68:48:4f:2b:2d:cd:5a:e8:5a:c8:8d:70:68:d4:79.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '172.17.0.90' (ECDSA) to the list of known hosts.
-bash-4.3# exit
logout
/ Dus aan de andere kant (.90) staat de public dsa in authorized_keys	, en  aan deze kant (.88) komt de andere kant (.90) is known_hosts	,

/ Hetzelfde kunnen we op .90 doen naar .88

/ Maar ook op de laptop	, ook dan wordt de laptop's /root/.ssh/known_hosts update	,
$ sudo su -
[root@localhost ~]# docker port pgssh
22/tcp -> 0.0.0.0:49191
5432/tcp -> 0.0.0.0:49190
[root@localhost ~]# ssh 172.17.0.88
/ of	,
[root@localhost ~]# ssh localhost -p 49191
The authenticity of host '[localhost]:49191 ([127.0.0.1]:49191)' can't be established.
ECDSA key fingerprint is 82:68:48:4f:2b:2d:cd:5a:e8:5a:c8:8d:70:68:d4:79.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[localhost]:49191' (ECDSA) to the list of known hosts.
Last login: Sun Apr  5 14:43:01 2015 from 172.17.0.90

/////////////////////////////
/ we kunnen dus of direct 172.17.0.88 of indirect 'localhost -p 49191'	,

/ 7	. 

/ password less inloggen van container naar container	,

$ vi Dockerfile

from fedora
run yum -y install openssh-server openssh-clients
run yum clean all

run awk -i inplace '{print gensub("(#)(PermitRootLogin yes)","\\2","g")}' /etc/ssh/sshd_config

run ssh-keygen -A

run ssh-keygen -N "" -t rsa -f ~/.ssh/id_rsa
run cat /root/.ssh/id_rsa.pub >>~/.ssh/authorized_keys

expose 22

add my.sh /root/
cmd ["/root/my.sh"]

[eric@localhost fedora_3]$ sudo docker build --rm -t local/fd-sshd-3 .

/ Door in de Dockerfile	, kunnen de containers passwordless inloggen bij elkaar	,  
run ssh-keygen -N "" -t rsa -f ~/.ssh/id_rsa
run cat /root/.ssh/id_rsa.pub >>~/.ssh/authorized_keys

/ Op beide containers	,
# cat /root/.ssh/authorized_keys
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDhCdQ3IfAvlzRkAHsSixKAdDVbzLpLIQ/MgyhyD+8zuwgR9cR+lj0HPgcauvdfcyKlWaMy2vzI8nsWrfN2i2n1UJTYqgu92mgxa3BYA6Bxv4FnrESuogqIc4vAJSgxMHnwKSR0GyTd401ke+g3nbTC56/332W0S9hwVFlkwehCu42SdGRWRwirdlxZJLcMqzYHrE6M9rnKYm30KIb2gx2SWEoDlrzhQ7DWIraGGcnzhgHmg9M8dhLbhAtGCg+rpeJTIjxIGKh+Cc3ndpWHK75wBma59gLzHQEOGsYpBVo4LkPzQWt1EYnX3pqsbtWFERWfxXK9idOFisZCouBBEuQV root@c4e263145f81

/ Maar c4e263145f81 is niet de hostname van een van de containers	,
/ TODO

/ 13	. 

/ we gaan het zelf doen	,

[eric@localhost fedora_3]$ pwd
/home/eric/Devel/Docker/sshd/fedora_3

$ vi Dockerfile

from fedora
run yum -y install openssh-server openssh-clients
run yum clean all

run echo 'root:walnoot' | chpasswd

run awk -i inplace '{print gensub("(#)(PermitRootLogin yes)","\\2","g")}' /etc/ssh/sshd_config

run ssh-keygen -A

#run ssh-keygen -N "" -t rsa -f ~/.ssh/id_rsa
#run cat /root/.ssh/id_rsa.pub >>~/.ssh/authorized_keys

expose 22

add my.sh /root/
cmd ["/root/my.sh"]

/ Let op dat we	, anders kan root in een container niet inlog in een andere container	,
run echo 'root:walnoot' | chpasswd

[eric@localhost Docker]$ sudo docker run --rm --name ssh -it -P local/fd-sshd-3

[eric@localhost Docker]$ sudo docker run --rm --name ssh2 -it -P local/fd-sshd-3

bash-4.3# ssh-keygen -N "" -t rsa -f ~/.ssh/id_rsa

bash-4.3# ssh-keyscan -H -t ecdsa 172.17.0.107  >>.ssh/known_hosts
bash-4.3#  cat .ssh/id_rsa.pub | ssh 172.17.0.107 "cat >>.ssh/authorized_keys"
/ of	,
bash-4.3# (exec 5<&0;exec 0<.ssh/id_rsa.pub;ssh 172.17.0.107 "cat >>.ssh/authorized_keys";exec 0<&5)
root@172.17.0.107's password: walnoot
/ OK
bash-4.3# ssh 172.17.0.107
Last login: Mon Apr  6 13:56:36 2015 from 172.17.0.108
-bash-4.3# exit
logout
/ dus password less inlog	,


/ Intermezzo

/ we doen eerst	,
bash-4.3# ssh-keyscan -H -t ecdsa 172.17.0.107  >>.ssh/known_hosts
/ anders krijgen we vraag	,
The authenticity of host '172.17.0.109 (172.17.0.109)' can't be established.
ECDSA key fingerprint is 79:db:34:ac:91:76:8b:c3:f8:28:2d:8a:aa:fd:23:8c.
Are you sure you want to continue connecting (yes/no)? no

/ Dit kunnen we ook doen, als deze dir er aan de andere kant nog niet is	,
bash-4.3# ssh 172.17.0.107 "mkdir .ssh"
root@172.17.0.107's password: 

/ Deze is ook OK, maar copies file , komt niet in authorized_keys	, 
bash-4.3# scp .ssh/id_rsa.pub 172.17.0.107:.ssh/
root@172.17.0.107's password: 
id_rsa.pub                                                            100%  399     0.4KB/s   00:00    

bash-4.3# cat >>~/.ssh/authorized_keys <(cat .ssh/id_rsa.pub )
/ OK in dezelfde container, 
/ TODO

/ Einde Intermezzo


/ Einde DOCKER POSTGRESQL SSHD

/ DOCKER LINK 

[eric@localhost fedora2]$ pwd
/home/eric/Devel/Docker/postgresql/fedora2
[eric@localhost fedora2]$ sudo docker build --rm -t local/fd-postgres-sshd .



[eric@localhost Docker]$ sudo docker run --rm --name pg -it -P local/fd-postgres-sshd 
bash-4.3# cat /etc/hosts
172.17.0.128	498647fc8373
127.0.0.1	localhost

[eric@localhost Docker]$ sudo docker run --rm --name pg2 --link pg:mypg -it -P local/fd-postgres-sshd
bash-4.3# cat /etc/hosts
172.17.0.129	3e154c738b88
127.0.0.1	localhost
172.17.0.128	mypg

bash-4.3# env
HOSTNAME=3e154c738b88
TERM=xterm
MYPG_PORT=tcp://172.17.0.128:22
MYPG_PORT_22_TCP_PORT=22
MYPG_PORT_5432_TCP_PORT=5432
MYPG_PORT_22_TCP=tcp://172.17.0.128:22
MYPG_PORT_5432_TCP=tcp://172.17.0.128:5432
MYPG_PORT_22_TCP_PROTO=tcp
MYPG_PORT_5432_TCP_PROTO=tcp
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PWD=/
MYPG_NAME=/pg2/mypg
HOME=/root
SHLVL=2
MYPG_PORT_22_TCP_ADDR=172.17.0.128
MYPG_PORT_5432_TCP_ADDR=172.17.0.128
_=/usr/bin/env



/ Einde DOCKER LINK 

/ MONGODB

/ 7		.

/ we doen eerst met de hand	,

/ ook op de laptop	,

/ we oef. in een local/fd-systemd container, hoe te start mongod	, en hoe mongod te start zonder systemctl	,

/ Lees	,
https://docs.mongodb.org/manual/tutorial/install-mongodb-on-red-hat/

/ Kies verschillend versies	, 2.4,	 2.6	, 3.0	, 

/ we zien verschillende repo's	, 

[eric@localhost fedora_3]$ sudo vi  /etc/yum.repos.d/mongodb.repo
[mongodb]
name=MongoDB Repository
baseurl=http://downloads-distro.mongodb.org/repo/redhat/os/x86_64/
gpgcheck=0
enabled=1

[eric@localhost fedora]$ sudo docker run --rm --name fd --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -P local/fd-systemd
[eric@localhost Docker]$ sudo docker exec -it fd /usr/bin/bash

bash-4.3# yum -y erase $(awk "NR>1{print \$1}" <(yum list installed "*mongo*"))
/ gooi weg wat er is	,

bash-4.3# yum -y install mongodb-org

Installing:
 mongodb-org                         x86_64                  2.6.9-1                    mongodb                  4.6 k
Installing for dependencies:
 mongodb-org-mongos                  x86_64                  2.6.9-1                    mongodb                  6.9 M
 mongodb-org-server                  x86_64                  2.6.9-1                    mongodb                  9.1 M
 mongodb-org-shell                   x86_64                  2.6.9-1                    mongodb                  4.3 M
 mongodb-org-tools                   x86_64                  2.6.9-1                    mongodb                   90 M

# systemctl start mongod

/ Dit lukt wel op de laptop, maar NIET in een container	, 
/ Het lijkt alsof de mongo rpm's toch sysv init doen	, maar op de laptop lukt het toch	, 
/ we zien op de laptop	,
    1 16326 16325 16325 ?           -1 Sl     986   0:14 /usr/bin/mongod -f /etc/mongod.conf

/ we doen in de container	,
bash-4.3# mongod -f /etc/mongod.conf 
about to fork child process, waiting until server is ready for connections.
forked process: 294
child process started successfully, parent exiting

bash-4.3# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0   294   293   293 ?           -1 Sl       0   0:01 mongod -f /etc/mongod.conf


bash-4.3# mongod -f /etc/mongod.conf  --shutdown
/ Maakt mongod een zombie	, 
/ Je moet hem ctrl+c	, 
/ Je kunt hierna	,
bash-4.3# mongod -f /etc/mongod.conf  
/ Maar ga niet helemaal OK	,

/ we kunnen ook	,
bash-4.3# vi /etc/mongod.conf 
#fork=true
bash-4.3#  mongod -f /etc/mongod.conf   
/ blijft run	,
/ in ander window	,
[eric@localhost fedora]$ sudo docker exec -it fd /usr/bin/bash
bash-4.3# mongo
MongoDB shell version: 2.6.9
connecting to: test
> 
/ we kunnen in server-window	, 
bash-4.3#  mongod -f /etc/mongod.conf   
ctrl+c
/ cleans up correctly	,

/ Wat ook OK	,
/ in client,	
> use admin
switched to db admin
> db.shutdownServer()
2015-04-11T18:07:24.555-0400 DBClientCursor::init call() failed
server should be down...
2015-04-11T18:07:24.559-0400 trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed
2015-04-11T18:07:24.559-0400 warning: Failed to connect to 127.0.0.1:27017, reason: errno:111 Connection refused
2015-04-11T18:07:24.559-0400 reconnect 127.0.0.1:27017 (127.0.0.1) failed failed couldn't connect to server 127.0.0.1:27017 (127.0.0.1), connection attempt failed
>
/ ERR msg, maar toch OK gedaan	,
/ TODO

/ 7	. 

/ Dockerfile	

[eric@localhost fedora]$ pwd
/home/eric/Devel/Docker/mongod/fedora
[eric@localhost fedora]$ cp /etc/yum.repos.d/mongodb.repo .

[eric@localhost fedora]$ cat mongodb.repo 
[mongodb]
name=MongoDB Repository
baseurl=http://downloads-distro.mongodb.org/repo/redhat/os/x86_64/
gpgcheck=0
enabled=1

$ cat my.sh
#!/usr/bin/bash
mongod -f /etc/mongod.conf
bash
$ chmod u+x my.sh
/ TODO

[eric@localhost fedora]$ cat Dockerfile 
from fedora:21 

add mongodb.repo /etc/yum.repos.d/ 

run yum -y install mongodb-org 
run yum clean all

expose 27017 27018 27019 20017 

add my.sh /root/
cmd ["/root/my.sh"]

[eric@localhost fedora]$ sudo docker build --rm -t local/fd-mongod .

[eric@localhost fedora]$ sudo docker run --rm --name mongod -it local/fd-mongod
forked process: 9
child process started successfully, parent exiting
bash-4.3# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0     1     1     1 ?           21 Ss       0   0:00 /usr/bin/bash /root/my.sh
    1     9     8     8 ?           -1 Sl       0   0:02 mongod -f /etc/mongod.conf
    1    20    20     1 ?           21 S        0   0:00 bash
   20    21    21     1 ?           21 R+       0   0:00 ps ajx
bash-4.3# mongo
/ OK

/ 7	. 

/ we install ook met de tar.gz	,

[eric@localhost fedora2]$ pwd
/home/eric/Devel/Docker/mongod/fedora2

[eric@localhost fedora2]$ cat Dockerfile 
from fedora:21 

run curl -O http://downloads.mongodb.org/linux/mongodb-linux-x86_64-3.0.2.tgz;\
	yum -y install tar;\
	cd;\
	tar -xvzf mongodb-linux-x86_64-3.0.2.tgz;\
	mkdir /opt/mongodb;\
	cp -a mongodb-linux-x86_64-3.0.2/* /opt/mongodb;\
 	mkdir -p /data/db	

expose 27017 27018 27019 20017 

add my.sh /root/
cmd ["/root/my.sh"]

[eric@localhost fedora2]$ cat my.sh 
#!/usr/bin/bash
export PATH=$PATH:/opt/mongodb/bin

mongod
/ of	,
mkdir /var/log/mongodb
mongod --fork --logpath=/var/log/mongodb/mongod.log
bash

[eric@localhost fedora2]$ sudo docker build -t local/fd-mongod-tar .

[eric@localhost fedora2]$ sudo docker run --rm --name mongod -it local/fd-mongod-tar
/ bij de 1ste mog in my.sh moeten we extra	, in ander window	,
$ sudo docker exec -it mongod /usr/bin/bash
# export PATH=$PATH:/opt/mongodb/bin

/ bij 2de mog moeten we bash	, anders returns docker run 	,
/ dan	,
bash-4.3# mongo
connecting to: test
> exit
 
/ 7	.

[eric@localhost fedora2]$ sudo docker run --rm --name mongod -it local/fd-mongod
bash-4.3# ps ajx
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    1     8     7     7 ?           -1 Sl       0   0:04 mongod -f /etc/mongod.conf


/ we kunnen daarnaast ook 	,
[eric@localhost fedora2]$ sudo docker exec -it mongod /usr/bin/bash
...
bash-4.3# mongod --shutdown --dbpath=/var/lib/mongo/
killing process with pid: 8
/ OK
/ we moeten het dbpath erbij geven als we shutdown	,

/ onthoud	,
http://docs.mongodb.org/manual/reference/sql-comparison/

/ 7	.

> var bulk=db.inventory.initializeUnorderedBulkOp()
/ of	,
> var bulk=db.inventory.initializeOrderedBulkOp()

> bulk.find({item:"ABC2"}).remove()
> bulk.execute()
BulkWriteResult({
	"writeErrors" : [ ],
	"writeConcernErrors" : [ ],
	"nInserted" : 0,
	"nUpserted" : 0,
	"nMatched" : 0,
	"nModified" : 0,
	"nRemoved" : 3,
	"upserted" : [ ]
})

> bulk.find({item:"ABC2"}) kan niet: verwacht bijv. .remove()

/ 7	.

> db.inventory.find()
{ "_id" : ObjectId("553261640e183dc7265dcac3"), "item" : "MNO2", "details" : { "model" : "14Q3", "manufacturer" : "ABC Company" }, "stock" : [ { "size" : "S", "qty" : 5 }, { "size" : "M", "qty" : 5 }, { "size" : "L", "qty" : 1 } ], "category" : "clothing" }
{ "_id" : ObjectId("553264db0e183dc7265dcac5"), "item" : "BE10", "details" : { "model" : "14Q2", "manufacturer" : "XYZ Company" }, "stock" : [ { "size" : "L", "qty" : 5 } ], "category" : "clothing" }
...

> db.inventory.find({item:"ABC1"})

> db.inventory.find({item :{$in:["ABC2","IJK2"]}})

/ and is met , or is met $or	,

> db.inventory.find({item :{$in:["ABC2","IJK2"]},category:"houseware"})

> db.inventory.find({$or:[{item :"ABC2"},{item:"IJK2"}]}) 
/ hetzelfde als met $in hierboven	,

/ 7	. 

/ embedded documents gebruiken we al lang	, bijv in een inventory document	,
"details" : { "model" : "14Q2", "manufacturer" : "XYZ Company" }

/ Lees	,
http://docs.mongodb.org/manual/core/data-model-design/#data-modeling-embedding

> db.inventory.find({"details" : { "model" : "14Q2", "manufacturer" : "XYZ Company" }})
/ OK

> db.inventory.find({"details" : { "model" : "14Q2" }})
/ NIETS	,
/ Arrays werken wel ongeveer zo	(je moet de [] weglaten, hier laten we de {} niet weg_, maar docs blijkbaar dus niet	,
> db.inventory.find({"details.model" : "14Q2"})
/ OK

/ 7	.

/ arrays of docs	,

> db.inventory.find({"stock" : [ { "size" : "S", "qty" : 5 }, { "size" : "L", "qty" : 1 } ]})
/ 3 docs	,
> db.inventory.find({"stock" :{ "size" : "S", "qty" : 5 }})
/ 8 docs	,
/ of	,
> db.inventory.find({"stock.0" :{ "size" : "S", "qty" : 5 }})
/ of	,
> db.inventory.find({"stock.1" :{ "size" : "L", "qty" : 1 }})
/ of	,
> db.inventory.find({stock:{$elemMatch :{ "size" : "S", "qty" : 5 }}})

/als er een getal staat in de key, moet er "" om de key	,

> db.inventory.find({"stock.1.qty" : 1 })
/ 3 docs	,
> db.inventory.find({"stock.1.qty" : {$gt:0,$lt:7 }})
/ 8 docs	,

/ 7	. 

/ modify	,

> db.inventory.find({item:"MNO2"})
/ 5 docs	,
> db.inventory.update(     { item: "MNO2" },     {       $set: {         category: "apparel",         details: { model: "14Q3", manufacturer: "XYZ Company" }       },       $currentDate: { lastModified: true }     } )
WriteResult({ "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 })
/ Doet er steeds maar 1	,

/ we lezen	,
By default, update() updates a single document. To update multiple documents, use the multi option.

/ we set {multi:true} erbij	,
> db.inventory.update({ item: "MNO2" },{$set: {category: "apparel",details: {model:"14Q3", manufacturer: "XYZ Company" }},$currentDate: { lastModified: true }} ,{multi:true})
WriteResult({ "nMatched" : 5, "nUpserted" : 0, "nModified" : 5 })

> db.inventory.find({item:"MNO2"})
{ "_id" : ObjectId("553261640e183dc7265dcac3"), "item" : "MNO2", "details" : { "model" : "14Q3", "manufacturer" : "XYZ Company" }, "stock" : [ { "size" : "S", "qty" : 5 }, { "size" : "M", "qty" : 5 }, { "size" : "L", "qty" : 1 } ], "category" : "apparel", "lastModified" : ISODate("2015-04-19T11:38:13.847Z") }
...
/ 5 docs	,
/ we zien een lastModified field	,
/ TODO (Was deze er eerst ook ?)

/ 13	. 

> db.inventory.find({},{item:1,_id:0})
{ "item" : "BE10" }
{ "item" : "IJK2" }
{ "item" : "ABC2" }
{ "item" : "IJK2" }
{ "item" : "ABC2" }
{ "item" : "IJK2" }
{ "item" : "MNO2" }
{ "item" : "MNO2" }
{ "item" : "MNO2" }
{ "item" : "MNO2" }
{ "item" : "MNO2" }
/ of	,
> db.inventory.distinct("item")
[ "BE10", "IJK2", "ABC2", "MNO2" ]

> db.inventory.update(   { item: "ABC2" },   { $set: { "details.model": "14Q2" } },{multi:true} )
WriteResult({ "nMatched" : 2, "nUpserted" : 0, "nModified" : 1 })
/ Die andere had al "details.model": "14Q2"	, dus die verandert niet	,

/ 13	. 

/ vervang het hele doc	,

> db.inventory.find({item:"BE10"})
{ "_id" : ObjectId("553264db0e183dc7265dcac5"), "item" : "BE10", "details" : { "model" : "14Q2", "manufacturer" : "XYZ Company" }, "stock" : [ { "size" : "L", "qty" : 5 } ], "category" : "clothing" }
db.inventory.update(
   { item: "BE10" },
   {
     item: "BE05",
     stock: [ { size: "S", qty: 20 }, { size: "M", qty: 5 } ],
     category: "apparel"
   }
)
> db.inventory.find({item:"BE10"})
> db.inventory.find({item:"BE05"})
{ "_id" : ObjectId("553264db0e183dc7265dcac5"), "item" : "BE05", "stock" : [ { "size" : "S", "qty" : 20 }, { "size" : "M", "qty" : 5 } ], "category" : "apparel" }

/ 13	. 

/ test	,

> db.my.insert({item:"ABC"})
WriteResult({ "nInserted" : 1 })
> db.my.find();
{ "_id" : ObjectId("553397cfba02c9f8f92c6d45"), "item" : "ABC" }

> db.my.update({item:"ABC"},{$currentDate:{mod:true}})
WriteResult({ "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 })
> db.my.find();
{ "_id" : ObjectId("553397cfba02c9f8f92c6d45"), "item" : "ABC", "mod" : ISODate("2015-04-19T11:57:29.820Z") }

> db.my.update({item:"ABC"},{name:"Eric J."})
WriteResult({ "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 })
> db.my.find();
{ "_id" : ObjectId("553397cfba02c9f8f92c6d45"), "name" : "Eric J." }

/ Dus groot VERSCHIL {$currentDate:{mod:true}} en {name:"Eric J."}	,
/we lezen op ,
http://docs.mongodb.org/manual/reference/operator/update/currentDate/

If the field does not exist, $currentDate adds the field to a document.

/ Met {name:"Eric J."} wordt het hele doc vervangen door {name:"Eric J."}	,

/ 13	. 

/ upsert	,

However, by specifying upsert: true, the update() method either updates matching document or documents, or inserts a new document using the update specification if no matching document exists.


>  db.inventory.update(
...    { item: "TBD1" },
...    {
...      item: "TBD1",
...      details: { "model" : "14Q4", "manufacturer" : "ABC Company" },
...      stock: [ { "size" : "S", "qty" : 25 } ],
...      category: "houseware"
...    },
...    { upsert: true }
... )
WriteResult({
	"nMatched" : 0,
	"nUpserted" : 1,
	"nModified" : 0,
	"_id" : ObjectId("5533d80958c4e2dfb98e5b99")
})

> db.inventory.update(    { item: "TBD1" },    {      item: "TBD1",      details: { "model" : "14Q4", "manufacturer" : "ABC Company" },      stock: [ { "size" : "S", "qty" : 25 } ],      category: "houseware"    },    { upsert: true } )
WriteResult({ "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 })

/ Waarom "nModified" : 1 	, want er is niets modified ?
/ TODO

/ 13	. 

> db.inventory.update(
...    { item: "TBD2" },
...    {
...      $set: {
...         details: { "model" : "14Q3", "manufacturer" : "IJK Co." },
...         category: "houseware"
...      }
...    },
...    { upsert: true }
... )
WriteResult({
	"nMatched" : 0,
	"nUpserted" : 1,
	"nModified" : 0,
	"_id" : ObjectId("5533da3558c4e2dfb98e5b9a")
})
> db.inventory.find({item:"TBD2"})
{ "_id" : ObjectId("5533da3558c4e2dfb98e5b9a"), "item" : "TBD2", "details" : { "model" : "14Q3", "manufacturer" : "IJK Co." }, "category" : "houseware" }

/ Wat is verschil met vorige?
/ TODO

/ 7	.

/ remove	,

db.inventory.remove({})
/ removes NIET de indexes	,

db.inventory.remove() 
/ is ERR: needs a query	,


To remove all documents from a collection, it may be more efficient to use the drop() method to drop the entire collection, including the indexes, and then recreate the collection and rebuild the indexes.

>  db.my.drop()
true

> db.inventory.remove({item:"TBD2"})
WriteResult({ "nRemoved" : 1 })

> db.my.remove({},1)
/ of
> db.my.remove({},2)
/ of
> db.my.remove({},true)
/ verwijdert er maar 1	,

/ 13	.

> db.my.insert({item:"ABC"})
WriteResult({ "nInserted" : 1 })
> db.my.insert({item:"ABC"})
WriteResult({ "nInserted" : 1 })
> db.my.count()
26
> var cursor=db.my.find()
> cursor
{ "_id" : ObjectId("5533dfc3ba02c9f8f92c6d49"), "item" : "ABC" }
{ "_id" : ObjectId("5533dfc3ba02c9f8f92c6d4a"), "item" : "ABC" }
{ "_id" : ObjectId("5533f033ba02c9f8f92c6d4b"), "item" : "ABC" }
{ "_id" : ObjectId("5533f084ba02c9f8f92c6d4c"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1a7ba02c9f8f92c6d4d"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1b0ba02c9f8f92c6d4e"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1b2ba02c9f8f92c6d4f"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1b2ba02c9f8f92c6d50"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1b3ba02c9f8f92c6d51"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1b4ba02c9f8f92c6d52"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1b4ba02c9f8f92c6d53"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1b5ba02c9f8f92c6d54"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1b5ba02c9f8f92c6d55"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1b6ba02c9f8f92c6d56"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1b7ba02c9f8f92c6d57"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1b8ba02c9f8f92c6d58"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1b8ba02c9f8f92c6d59"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1b9ba02c9f8f92c6d5a"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1b9ba02c9f8f92c6d5b"), "item" : "ABC" }
{ "_id" : ObjectId("5533f1baba02c9f8f92c6d5c"), "item" : "ABC" }
Type "it" for more
> 

/ dus 
> cursor
/ is hetzelfde als wat we altijd doen	, met db.my.find()	,

> while(cursor.hasNext()){
... print(cursor.next())
... }
[object Object]
[object Object]
[object Object]
[object Object]
[object Object]
[object Object]

/ Klopt	, er waren er nog 6 over	,

/ opnieuw cursor maken	,
> var cursor=db.my.find()
> while(cursor.hasNext()){ print(cursor.next()) }
[object Object]
[object Object]
...
/ alle 26	,

/ wat doet tojson	?

> var cursor=db.my.find()
> while(cursor.hasNext()){ print(tojson(cursor.next())) }
/ of	,
> while(cursor.hasNext()){ printjson(cursor.next()) }
/ of	,
> cursor.forEach(printjson)
{ "_id" : ObjectId("5533dfc3ba02c9f8f92c6d49"), "item" : "ABC" }
{ "_id" : ObjectId("5533dfc3ba02c9f8f92c6d4a"), "item" : "ABC" }
/ alle 26	

/ 13	. 

> var cursor=db.my.find()
> var arr=cursor.toArray()
> var doc=arr[3]
> doc
{ "_id" : ObjectId("5533f084ba02c9f8f92c6d4c"), "item" : "ABC" }

/ we kunnen nu nog een keer	,
> var doc=arr[3]
> doc
{ "_id" : ObjectId("5533f084ba02c9f8f92c6d4c"), "item" : "ABC" }
> cursor[3]
{ "_id" : ObjectId("5533f084ba02c9f8f92c6d4c"), "item" : "ABC" }

/ 13	. 

/ examine query	,

> db.inventory.find({item:"ABC2"})
{ "_id" : ObjectId("55326f9c0e183dc7265dcace"), "item" : "ABC2", "details" : { "model" : "14Q2", "manufacturer" : "M1 Corporation" }, "stock" : [ { "size" : "M", "qty" : 50 } ], "category" : "clothing" }
{ "_id" : ObjectId("55326f9f0e183dc7265dcad1"), "item" : "ABC2", "details" : { "model" : "14Q2", "manufacturer" : "M1 Corporation" }, "stock" : [ { "size" : "M", "qty" : 50 } ], "category" : "clothing" }

> db.inventory.find({item:"ABC2"}).explain("executionStats")
{
	"queryPlanner" : {
		"plannerVersion" : 1,
		"namespace" : "test.inventory",
		"indexFilterSet" : false,
		"parsedQuery" : {
			"item" : {
				"$eq" : "ABC2"
			}
		},
		"winningPlan" : {
			"stage" : "COLLSCAN",
			"filter" : {
				"item" : {
					"$eq" : "ABC2"
				}
			},
			"direction" : "forward"
		},
		"rejectedPlans" : [ ]
	},
	"executionStats" : {
		"executionSuccess" : true,
		"nReturned" : 2,
		"executionTimeMillis" : 0,
		"totalKeysExamined" : 0,
		"totalDocsExamined" : 12,
		"executionStages" : {
			"stage" : "COLLSCAN",
			"filter" : {
				"item" : {
					"$eq" : "ABC2"
				}
			},
			"nReturned" : 2,
			"executionTimeMillisEstimate" : 0,
			"works" : 14,
			"advanced" : 2,
			"needTime" : 11,
			"needFetch" : 0,
			"saveState" : 0,
			"restoreState" : 0,
			"isEOF" : 1,
			"invalidates" : 0,
			"direction" : "forward",
			"docsExamined" : 12
		}
	},
	"serverInfo" : {
		"host" : "a464828db77e",
		"port" : 27017,
		"version" : "3.0.2",
		"gitVersion" : "6201872043ecbbc0a4cc169b5482dcf385fc464f"
	},
	"ok" : 1
}

> db.inventory.createIndex({item:1})
{
	"createdCollectionAutomatically" : false,
	"numIndexesBefore" : 1,
	"numIndexesAfter" : 2,
	"ok" : 1
}
/ WH al index op _id	,

> db.inventory.find({item:"ABC2"}).explain("executionStats")
{
	"queryPlanner" : {
		"plannerVersion" : 1,
		"namespace" : "test.inventory",
		"indexFilterSet" : false,
		"parsedQuery" : {
			"item" : {
				"$eq" : "ABC2"
			}
		},
		"winningPlan" : {
			"stage" : "FETCH",
			"inputStage" : {
				"stage" : "IXSCAN",
				"keyPattern" : {
					"item" : 1
				},
				"indexName" : "item_1",
				"isMultiKey" : false,
				"direction" : "forward",
				"indexBounds" : {
					"item" : [
						"[\"ABC2\", \"ABC2\"]"
					]
				}
			}
		},
		"rejectedPlans" : [ ]
	},
	"executionStats" : {
		"executionSuccess" : true,
		"nReturned" : 2,
		"executionTimeMillis" : 0,
		"totalKeysExamined" : 2,
		"totalDocsExamined" : 2,
		"executionStages" : {
			"stage" : "FETCH",
			"nReturned" : 2,
			"executionTimeMillisEstimate" : 0,
			"works" : 3,
			"advanced" : 2,
			"needTime" : 0,
			"needFetch" : 0,
			"saveState" : 0,
			"restoreState" : 0,
			"isEOF" : 1,
			"invalidates" : 0,
			"docsExamined" : 2,
			"alreadyHasObj" : 0,
			"inputStage" : {
				"stage" : "IXSCAN",
				"nReturned" : 2,
				"executionTimeMillisEstimate" : 0,
				"works" : 3,
				"advanced" : 2,
				"needTime" : 0,
				"needFetch" : 0,
				"saveState" : 0,
				"restoreState" : 0,
				"isEOF" : 1,
				"invalidates" : 0,
				"keyPattern" : {
					"item" : 1
				},
				"indexName" : "item_1",
				"isMultiKey" : false,
				"direction" : "forward",
				"indexBounds" : {
					"item" : [
						"[\"ABC2\", \"ABC2\"]"
					]
				},
				"keysExamined" : 2,
				"dupsTested" : 0,
				"dupsDropped" : 0,
				"seenInvalidated" : 0,
				"matchTested" : 0
			}
		}
	},
	"serverInfo" : {
		"host" : "a464828db77e",
		"port" : 27017,
		"version" : "3.0.2",
		"gitVersion" : "6201872043ecbbc0a4cc169b5482dcf385fc464f"
	},
	"ok" : 1
}

/ we zien verschil	,

	"executionStats" : {
		"executionSuccess" : true,
		"nReturned" : 2,
		"executionTimeMillis" : 0,
		"totalKeysExamined" : 0,
		"totalDocsExamined" : 12,
		"executionStages" : {
			"stage" : "COLLSCAN",

	"executionStats" : {
		"executionSuccess" : true,
		"nReturned" : 2,
		"executionTimeMillis" : 0,
		"totalKeysExamined" : 2,
		"totalDocsExamined" : 2,
		"executionStages" : {
			"stage" : "FETCH",
			"nReturned" : 2,

/ .hint
/ TODO 

/ 13	. 
















	


























 


/ Einde MONGODB

/ POSTGRESQL REPLICATION

/ Lees	,
https://wiki.postgresql.org/wiki/Replication,_Clustering,_and_Connection_Pooling

/ geef in google: postgresql replication	,
/ Kijk	,
https://www.youtube.com/watch?v=GobQw9LMEaw



/ Einde POSTGRESQL REPLICATION

/ ECLIPSE CPP INSTALL CONFIGURE

/ Click onder Welcome docs	,
http://127.0.0.1:41386/help/index.jsp?topic=%2Forg.eclipse.cdt.doc.user%2Fgetting_started%2Fcdt_o_tutorial.htm

/ bekijk	,
https://www.youtube.com/watch?v=6WFSEPDS2KI

/ Lees	,
http://web.eecs.umich.edu/~sugih/courses/eecs487/glut-howto/#linux

glibc.x86_64                                          2.20-7.fc21                                           @updates
gcc.x86_64                                           4.9.2-6.fc21                                           @updates


/ 7	.

/ bekijk	,
https://www.youtube.com/watch?v=6WFSEPDS2KI

window, preferences, general, editors, text editors	,
/ check allemaal, behalve 
insert spaces for tabs
show print margin

window, preferences, c/c++, new c/c++ project wizard , preferred toolchains
executable
	Empty project
	Hello World C++ project
	Hello World ANSI C project
/ Kies Linux GCC, click Make toolchains preferred (bij 1  is genoeg)	,

window, preferences, c/c++, editor, folding	,
/ check de bovenste 3, de anderen niet	,

/ see hier beneden	, waarom	,
/ In eclipse	, per project	,

/ project, properties	,  c/c++ build	, settings
GCC C++ compiler, Miscellaneous
	Other flags: c -fmessage-length=0 -std=c++11
GCC C compiler, Miscellaneous
	Other flags: c -fmessage-length=0 -std=gnu11

/ als veranderd, right click project, 'Build project',	 dan is de src/subdir.mk aangepast	,

/ we zien bij 'GCC C++ Compiler' alle opties	,
-O0 -g3 -Wall -c -fmessage-length=0 -std=c++11
/ Eronder kunnen we ze aanpassen	,
'Optimization': staat nu op -O0
/ Debugging: staat nu op maximum (-g3)
...



/ Lees	,
http://help.eclipse.org/luna/index.jsp
/ we zien

/ Lees	,
/ geef in google: eclipse c++ std=c++11	,
http://stackoverflow.com/questions/17457069/enabling-c11-in-eclipse-juno-kepler-luna-cdt
http://stackoverflow.com/questions/9131763/eclipse-cdt-c11-c0x-support

/ geef in google: eclipse c++ toolchain	, 
http://help.eclipse.org/luna/index.jsp?topic=%2Forg.eclipse.cdt.doc.user%2Freference%2Fcdt_u_prop_build_toolchain.htm
/ we zijn in	,
C/C++ Development User Guide > Reference > C/C++ Properties > C/C++ File Properties > C/C++ Build


/ 7	. 

/ we lezen	,

/ help 	,
http://help.eclipse.org/luna/index.jsp
C/C++ Developmet User Guide
Getting started

/ 13	

create a simple application

file, new	, c++ project
executable
Hello world project
/ als dit gedaan	, dan hebben we project first	, we zien	
Includes
src
/ right click project, Build project	, we zien	,
Binaries
Includes
src
Debug
/ ga nu naar Run , Run configurations
/ click New	, hij pakt vanzelf first	,
Name: first Debug
C/C++ application: Debug/first
project: first
/ click Run
/ OK

/ we kijken in de ws	,
[eric@almond first]$ pwd
/home/eric/Devel/Eclipse/cpp/workspace/first
[eric@almond first]$ find
.
./src
./src/first.cpp
./Debug
./Debug/makefile
./Debug/objects.mk
./Debug/src
./Debug/src/subdir.mk
./Debug/src/first.o
./Debug/src/first.d
./Debug/first
./Debug/sources.mk
./.project
./.cproject

/ wat je in eclipse, in project explorer ziet onder Binaries: first = Deploy/first	,

/ 13	. 

/ make

file, new	, c++ project
makefile project
Hello world project
/click advanced settings: we zien: elf parser	,
/ maar er is ook gnu elf parser (TODO)
/ als dit gedaan	, dan hebben we project fourth, we zien	
Includes
fourth.cpp
Makefile
/ right click project, Build project	, we zien	,
Binaries
Includes
fourth.cpp
fourth
fourth.o
Makefile

/ ga nu naar Run , Run configurations
/ click New	, hij pakt vanzelf first	,
Name: fourth Debug
C/C++ application: fourth 
project: fourth 
/ click Run
/ OK

/ we kijken in de ws	,
[eric@almond fourth]$ pwd
/home/eric/Devel/Eclipse/cpp/workspace/fourth
[eric@almond fourth]$ find
[eric@almond fourth]$ find
.
./fourth.cpp
./fourth.o
./.settings
./fourth
./.project
./.cproject
./Makefile

/ 13	. 

 /verschillen executable, makefile

/ bij makefile is de Makefile simple	,

$ vi fourth/Makefile
CXXFLAGS =  -O2 -g -Wall -fmessage-length=0
OBJS =      fourth.o
LIBS =
TARGET =    fourth
$(TARGET):  $(OBJS)
    $(CXX) -o $(TARGET) $(OBJS) $(LIBS)
all:    $(TARGET)
clean:
    rm -f $(OBJS) $(TARGET)

/ De bedoeling is WH dat we deze zelf write	, 

/ bij executable heb je bij create project , advanced settings	, meer tabs	,
/ we kunnen precies deze later bekijken door right click project, properties	,
/ we zien tab bij executable	,
Tool settings
/ waar we de opties geven aan g++	, bijv -std=c++11
/ de tab 'Binary parsers (elf) zien we altijd	,

/ TODO

/ 7	. 

/ 13	. 

/ executable proj	,

/ right click 'clean project'	,
/ we zien in de 'CDT Global build console'	,
make clean 
rm -rf  ./src/first.o  ./src/first.d  first

/ makefile proj	,

make clean 
rm -f fourth.o fourth

/ 13	. 

/ executable proj	,

/ right click 'build project'	,
/ we zien in de 'CDT Global build console'	,
make all 
Building file: ../src/first.cpp
Invoking: GCC C++ Compiler
g++ -O0 -g3 -Wall -c -fmessage-length=0 -MMD -MP -MF"src/first.d" -MT"src/first.d" -o "src/first.o" "../src/first.cpp"
Finished building: ../src/first.cpp
 
Building target: first
Invoking: GCC C++ Linker
g++  -o "first"  ./src/first.o   
Finished building target: first

/ Waar komt dit vandaan	?

/ Alles komt in Debug/	, ook de src/	, 
/ een .cpp file zien we NIET in Debug/src , wel in src/ , de subdir.mk zien we WEL in Debug/src	, 

[eric@almond first]$ pwd
/home/eric/Devel/Eclipse/cpp/workspace/first
[eric@almond first]$ vi Debug/src/subdir.mk 
...
src/%.o: ../src/%.cpp
    @echo 'Building file: $<'
    @echo 'Invoking: GCC C++ Compiler'
    g++ -O0 -g3 -Wall -c -fmessage-length=0 -MMD -MP -MF"$(@:%.o=%.d)" -MT"$(@:%.o=%.d)" -o "$@" "$<"

/ dit zien we inderdaad	,

/ we zien dus dat wat bij project first, properties, Tool settings staat: de options bij g++ , we hier zien	, in de subdir.mk	,

/ er is ook	,
[eric@almond first]$ vi Debug/makefile 
all: first

# Tool invocations
first: $(OBJS) $(USER_OBJS)
    @echo 'Building target: $@'
    @echo 'Invoking: GCC C++ Linker'
    g++  -o "first" $(OBJS) $(USER_OBJS) $(LIBS)
...

/ Dit zien we ook	,
 

/ 13	. 
 
/ makefile proj	,

make all 
g++ -O2 -g -Wall -fmessage-length=0   -c -o fourth.o fourth.cpp
g++ -o fourth fourth.o 





/ Einde ECLIPSE CPP INSTALL CONFIGURE

/ CPP

/ 7	. 

/ Hoe debug C++ itself?

/ Lees	,
https://gcc.gnu.org/onlinedocs/libstdc++/manual/debug_mode_using.html
/ -D_GLIBCXX_DEBUG
/ TODO

/ Lees	,
https://gcc.gnu.org/onlinedocs/libstdc++/manual/debug.html
/ build c++ met --enable-libstdcxx-debug
/ Heeft fedora dit al?
 /TODO

/ Bekijk C Unit test	,
https://www.youtube.com/watch?v=y-tBjj9OmdI

/ Bekijk C++ Google test
https://www.youtube.com/watch?v=TS2CTf11k1U&list=PL5jc9xFGsL8GyES7nh-1yqljjdTvIFSsh

/ Bekijk cgreen	, op cppcon
https://www.youtube.com/watch?v=Y8YVSohnlgY

/ Bekijk CUTE,  op cppcon
https://www.youtube.com/watch?v=YrGSQXZmAXs

/ Ga naar	,
http://www.cute-test.com/projects/cute/wiki/CUTE_Installation_and_System_Requirements

/ Einde CPP

/ CPP CUTE

/ Ga naar	,
http://www.cute-test.com/projects/cute/wiki/CUTE_Installation_and_System_Requirements

/ we update cdt	,

ga in eclipse naar help, install new software, click 'available software sites'	,
/ we zien	,
cdt
linux tools
luna
the eclipse project updates

/ achter work with: we kunnen ze select uit de dropdown list	, kies cdt: we upgrade cdt naar 8.4.0
/ TODO (de andere 3)

/ ga weer naar http://www.cute-test.com/projects/cute/wiki/CUTE_Installation_and_System_Requirements, en we lezen de install url http://www.cute-test.com/updatesite, geef deze achter work with, en geef een name: cute, en click OK	, we install CUTE	,

/ click op cute site, rechts menu 'User guide'

/ we zien installation, waar we net waren	,

/ 7	.

/ we zien 'using the plugin'	,
http://www.cute-test.com/projects/cute/wiki/Using_the_Cute_Eclipse_Plugin

/ file, new , c++ project,
/ naast executable, ..., makefile project, zien we nu ook  CUTE	,
/ kies CUTE project
/ als we -std=c++11 add aan compiler options	, hoeven we NIET: copy Boost headers into Project	,
/ anders build het proj niet	,

/ De 1ste keer, toen we Run as -> CUTE test kozen, komt er een window (deze komt maar 1 keer )	, 
Launch Debug configuration selection
gdb/mi
gdbserver
remote gdb/mi
/ In tmp/ laten we het tot zo	, in cpp/ hebben we gdb/mi gekozen.
/ TODO

/ 7	. 

/ we edit	, 

void thisIsATest() {
    std::string first, second, expected;
    first = "Hello";
    second = "World";
    expected = "Hello World";
    ASSERT_EQUAL(expected, first + " " + second);
}
/ we zien in een aparte view 'Test results' de groene balk, maar in Console zien we de text	,
#beginning AllTests 1

#starting thisIsATest

#success thisIsATest OK

#ending AllTests

185

/ 7	. 

/ Video	,
https://www.youtube.com/watch?v=YrGSQXZmAXs

/ 13	. 

 /we maken een hello world appl	,

File, New , C++ Project	, 
Executable, HelloWorld C++ Project	,
Project Name: HelloWorld 
/ click hamer icon: build	,
/ right click , Run as Local C/C++ program	,
/ OK

/ we zien	,
int main() {
	cout << "!!!Hello World!!!" << endl; 
	return 0;
}

/ 13	. 

/ we rm global var cout	, omdat we deze straks willen vervangen	,  het wordt dan arg van fct	,

int main() {
	ostream&out=cout;
	out << "!!!Hello World!!!" << endl; 
	return 0;
}

/ we kunnen main niet test	, 

/ we select de line	,
	out << "!!!Hello World!!!" << endl;
/ refactor, extract function
function name: sayHello

/ we zien	,
void sayHello(ostream&out) {
	out << "!!!Hello World!!!" << endl;
}

int main() {
	ostream&out=cout;
	sayHello(out);
	return 0;
}
 
/13	. 

/ 1313	,

/ om de een of andere reden is deze main fct een probleem voor de test	,
/ TODO
/ we brengen sayHello in library	,

/ eerst rm	,
using namespace std;

/ select std	, right click	, refactor, inline using...	,
 /we zien	,

$ vi HellWorld.cpp

#include <iostream>

void sayHello(std::ostream& out) {
	out << "!!!Hello World!!!" << std::endl;
}

int main() {
	std::ostream&out = std::cout;
	sayHello(out);
	return 0;
}

/ 1313	,

/select de fct sayHello	, right click , refactor, to new header file	,

/ we zien	, 

$ vi HellWorld.cpp


#include <iostream>

#include "sayHello.h"

int main() {
	std::ostream&out = std::cout;
	sayHello(out);
	return 0;
}

$ vi sayHello.h

#ifndef SAYHELLO_H_
#define SAYHELLO_H_



void sayHello(std::ostream& out) {
	out << "!!!Hello World!!!" << std::endl;
}


#endif /* SAYHELLO_H_ */

/ 1313	,

/ Hij maakt een libary proj, new C++ proj	, static libary, empty project	, 
/ project name: LibHello	,

/ moves de header file uit src/ uit HelloWorld proj naar LibHello	, bovenin	, er is geen src/	, TODO
/ we zien in de header file ERRs	, omdat we daarin #include<iostream> 	, we set deze er in	, 

/ right click HelloWorld proj, properties	, Project References, check LibHello	,

/ In de header file is de fct sayHello def	, we kunnen hem in de header file laten, maar dan moet inline fct worden, of in een .cpp file set	,

/ we doen eerst inline	,

/ in LibHello	, 

$ vi sayHello.h

#ifndef SAYHELLO_H_
#define SAYHELLO_H_

#include<iostream>

inline void sayHello(std::ostream& out) {

	out << "!!!Hello World!!!" << std::endl;
}


#endif /* SAYHELLO_H_ */

/ we geven in HelloWorld bij properties	, c/c++ build, settings, gcc c++ compiler , includes, ${workspace_loc:/LibHello}
/ Niets in properties , project reference	,
/ Niets in properties , c/c++ build, settings, gcc c++ compiler , gcc c++ linker	,



/ 1313	.

/ als we buiten eclipse zelf willen compile	,
[eric@almond Debug]$ pwd
/home/eric/Devel/Eclipse/cpp/workspace/HelloWorld/Debug
Building file: ../src/HelloWorld.cpp
Invoking: GCC C++ Compiler
g++ -I"/home/eric/Devel/Eclipse/cpp/workspace/LibHello" -O0 -g3 -Wall -c -fmessage-length=0 -MMD -MP -MF"src/HelloWorld.d" -MT"src/HelloWorld.d" -o "src/HelloWorld.o" "../src/HelloWorld.cpp"
Finished building: ../src/HelloWorld.cpp
 
Building target: HelloWorld
Invoking: GCC C++ Linker
g++  -o "HelloWorld"  ./src/HelloWorld.o   
Finished building target: HelloWorld

[eric@almond Debug]$ make -f makefile all
[eric@almond Debug]$ ./HelloWorld 
!!!Hello World!!!
Hello


/ 1313	.

/ nu met sayHello.cpp	, 
/ rm eerst inline	,

/ select fct	, right click refactor, toggle function definition	, 
/ eclipse: create a new file 'sayHello.cpp and moven sayHello(ostream&out)?
/ JA

/ we zien in LibHello	,

$ vi sayHello.h

#ifndef SAYHELLO_H_
#define SAYHELLO_H_

#include<iostream>

void sayHello(std::ostream& out);

#endif

$ vi sayHello.cpp

#include "sayHello.h"

void sayHello(std::ostream& out) {
	out << "!!!Hello World!!!" << std::endl;
}

/ 1313	. 

/ Het verschil tussen static lib en shared lib voor HelloWorld proj is dat je 
/ * in de run config van HelloWorld2 (die een shared lib links) moet in de env LD_LIBRARY_PATH set	, tot ${workspace_loc:/LibHello2/Debug}
/ * -I, -L en -l in HelloWorld en HelloWorld2 zijn hetzelfde	,
/ * In gcc c++ compiler settings bij LibHello2 (shared lib) moet (via miscellaneous) -fPIC  , 	

/ 1313	 

/ HelloWorld Met LibHello static library	,

file, new, c++ project,	static library, empty project	,
project name: LibHello
/ cp sayHello.h/.cpp in LibHello/ root dir	,
/ build LibHello, 

/ we zien	,
make all 
Building file: ../sayHello.cpp
Invoking: GCC C++ Compiler
g++ -O0 -g3 -Wall -c -fmessage-length=0 -MMD -MP -MF"sayHello.d" -MT"sayHello.d" -o "sayHello.o" "../sayHello.cpp"
Finished building: ../sayHello.cpp
 
Building target: libLibHello.a
Invoking: GCC Archiver
ar -r  "libLibHello.a"  ./sayHello.o   
ar: creating libLibHello.a
Finished building target: libLibHello.a

/ we zien	,
[eric@almond LibHello]$ find
.
./Debug
./Debug/makefile
./Debug/sayHello.o
./Debug/subdir.mk
./Debug/objects.mk
./Debug/libLibHello.a
./Debug/sayHello.d
./Debug/sources.mk
./.settings
./.settings/org.eclipse.core.resources.prefs
./.project
./sayHello.h
./.cproject
./sayHello.cpp

/ In HelloWorld	,
/ right click project,	 properties	,
Project references: niets doen	,
c/c++ build	, settings
gcc c++ compiler 
includes 
Include paths (-I) 
${workspace_loc:/LibHello}
gcc c++ linker
libraries
libraries (-l)
LibHello
library search path (-L)
${workspace_loc:/LibHello/Debug}
/ build HelloWorld	,

/ we zien	,
make all 
Building file: ../src/HelloWorld.cpp
Invoking: GCC C++ Compiler
g++ -I"/home/eric/Devel/Eclipse/cpp/workspace/LibHello" -O0 -g3 -Wall -c -fmessage-length=0 -MMD -MP -MF"src/HelloWorld.d" -MT"src/HelloWorld.d" -o "src/HelloWorld.o" "../src/HelloWorld.cpp"
Finished building: ../src/HelloWorld.cpp
 
Building target: HelloWorld
Invoking: GCC C++ Linker
g++ -L"/home/eric/Devel/Eclipse/cpp/workspace/LibHello/Debug" -o "HelloWorld"  ./src/HelloWorld.o   -lLibHello
Finished building target: HelloWorld

/ we zien	,
[eric@almond HelloWorld]$ find
.
./src
./src/HelloWorld.cpp
./Debug
./Debug/makefile
./Debug/objects.mk
./Debug/src
./Debug/src/HelloWorld.o
./Debug/src/subdir.mk
./Debug/src/HelloWorld.d
./Debug/sources.mk
./Debug/HelloWorld
./.project
./.cproject

/ Debug/HelloWorld is de executable	,

  

/ 1313

/ HelloWorld2 met LibHello2 shared library	,

file, new, c++ project,	shared library, empty project	,
project name: LibHello2
/ cp sayHello.h/.cpp in LibHello/ root dir	,
/ we moeten een compiler optie add	,  -fPIC
/ right click LibHello2, properties	, c/c++ build, settings, gcc c++ compiler, miscellaneous, 
other flags: -c -fmessage-length=0 -fPIC
/ build LibHello2	, 
/ we zien	,
make all 
Building file: ../sayHello.cpp
Invoking: GCC C++ Compiler
g++ -O0 -g3 -Wall -c -fmessage-length=0 -fPIC -MMD -MP -MF"sayHello.d" -MT"sayHello.d" -o "sayHello.o" "../sayHello.cpp"
Finished building: ../sayHello.cpp
 
Building target: libLibHello2.so
Invoking: GCC C++ Linker
g++ -shared -o "libLibHello2.so"  ./sayHello.o   
Finished building target: libLibHello2.so

/ we zien	,
[eric@almond LibHello2]$ find
.
./Debug
./Debug/makefile
./Debug/sayHello.o
./Debug/subdir.mk
./Debug/objects.mk
./Debug/libLibHello2.so
./Debug/sayHello.d
./Debug/sources.mk
./.project
./sayHello.h
./.cproject
./sayHello.cpp

/ In HelloWorld	,
/ right click project,	 properties	,
Project references: niets doen	,
c/c++ build	, settings
gcc c++ compiler 
includes 
Include paths (-I) 
${workspace_loc:/LibHello2}
gcc c++ linker
libraries
libraries (-l)
LibHello2
library search path (-L)
${workspace_loc:/LibHello2/Debug}
/ build HelloWorld2

/ we zien	,
make all 
Building file: ../src/HelloWorld.cpp
Invoking: GCC C++ Compiler
g++ -I"/home/eric/Devel/Eclipse/cpp/workspace/LibHello2" -O0 -g3 -Wall -c -fmessage-length=0 -MMD -MP -MF"src/HelloWorld.d" -MT"src/HelloWorld.d" -o "src/HelloWorld.o" "../src/HelloWorld.cpp"
Finished building: ../src/HelloWorld.cpp
 
Building target: HelloWorld2
Invoking: GCC C++ Linker
g++ -L"/home/eric/Devel/Eclipse/cpp/workspace/LibHello2/Debug" -o "HelloWorld2"  ./src/HelloWorld.o   -lLibHello2
Finished building target: HelloWorld2

/ In HelloWorld's run config	,
environment
LD_LIBRARY_PATH ${workspace_loc:/LibHello2/Debug}

/ run	, 
/ we zien	,
!!!Hello World!!!

/ 1313

/ een static lib wordt create met	,
ar -r  "libLibHello.a"  ./sayHello.o   
/ een shared lib wordt create met	,
g++ -shared -o "libLibHello2.so"  ./sayHello.o   

/ 13	.

/ new C++ project,	 CUTE library test project	, TestHello	,
/ we zien LibHello staan: check	,
/ we add -std=c++11 option aan c++ compiler	, anders moeten we Boost headers include into project	,  
/ click TestHello proj	, compile , 
/ we zien	,
make all 
Building file: ../src/Test.cpp
Invoking: GCC C++ Compiler
g++ -I"/home/eric/Devel/Eclipse/cpp/workspace/LibHello" -I"/home/eric/Devel/Eclipse/cpp/workspace/TestHello/cute" -O0 -g3 -Wall -c -fmessage-length=0 -std=c++11 -MMD -MP -MF"src/Test.d" -MT"src/Test.d" -o "src/Test.o" "../src/Test.cpp"
Finished building: ../src/Test.cpp
 
Building target: TestHello
Invoking: GCC C++ Linker
g++ -L"/home/eric/Devel/Eclipse/cpp/workspace/LibHello/Debug" -o "TestHello"  ./src/Test.o   -lLibHello
Finished building target: TestHello

////////////
/ WH omdat we toen we de CUTE Library project build, we LibHello check	, zien we 	,
/ we zien bij project, c/c++ build, settings	,
gcc c++ compiler 
includes
${workspace_loc:/LibHello}
${workspace_loc:/TestHello/cute}
misc
other flags:   -c -fmessage-length=0 -std=c++11
gcc c++ linker
libraries
libraries (-l)
LibHello
library search path (-L)
${workspace_loc:/LibHello/Debug}
/ we zien bij project , properties , project references	, LibHello check	,
/ TODO

///////////////////
/ WH 
/ Bij de test proj	, als we LibHello check	, als we de test proj create	, dan vult hij -l, -L, -I in	, 
/ bij een gewoon proj , HelloWorld	, moeten we dat zelf	, 

/ Run as CUTE test

/ 1313	.

$ vi Test.cpp
#include "sayHello.h"
void thisIsATest() {
	std::ostringstream out;
	sayHello(out);
	ASSERT_EQUAL("Hello, world\n",out.str());
}
/ #include "sayHello.h" moeten we zelf type	, 
/ wanneer kent hij sayHello	? Als we Test.cpp save na de #include	?
/ TODO

/ OK, maar we zien dat de strings niet kloppen	,
/ In LibHello (de static lib)	,
$ vi sayHello.cpp
void sayHello(std::ostream& out) {
	out << "Hello, world" << std::endl;
}
/ Run de test opnieuw	, 
/ OK

/ als we rerun test kiezen in Test console, 	dan gaat hij 	,

/ In CDT Global Build Console	,

20:54:01 **** Incremental Build of configuration Debug for project LibHello ****
make all 
Building file: ../sayHello.cpp
Invoking: GCC C++ Compiler
g++ -O0 -g3 -Wall -c -fmessage-length=0 -MMD -MP -MF"sayHello.d" -MT"sayHello.d" -o "sayHello.o" "../sayHello.cpp"
Finished building: ../sayHello.cpp
 
Building target: libLibHello.a
Invoking: GCC Archiver
ar -r  "libLibHello.a"  ./sayHello.o   
ar: creating libLibHello.a
Finished building target: libLibHello.a
 

20:54:02 Build Finished (took 267ms)

20:54:02 **** Incremental Build of configuration Debug for project TestHello ****
make all 
Building target: TestHello
Invoking: GCC C++ Linker
g++ -L"/home/eric/Devel/Eclipse/cpp/workspace/LibHello/Debug" -o "TestHello"  ./src/Test.o   -lLibHello
Finished building target: TestHello

/ 1313	. 

/ we test de shared library	,

/ we veranderen de settings van TestHello , van LibHello naar libHello2	, 
/ we moeten in de run config de LD_LIBRARY PATH=${workspace_loc:/LibHello2/Debug} set	,

/ rerun test	, 
/ Maar moesten we touch LibHello2 opnieuw build?
/ TODO

/ 13	. 

/ video 27:00 test loops	,

/ 13	.
/ Einde CPP CUTE

/ CPP CUTE

/ we maken c++ shared lib project LibBook	, 

/ we maken cute lib test project TestBook	, 
/ Als we check LibBook	, wordt -I -L -l set , en als we Run as -> CUTE test	, wordt ook in de run config LD_LIBRARY_PATH set	,

/ 13	. 

/ In LibBook	,

$ vi my.h
int sum(int,int);

$ vi my.cpp
int sum(int a,int b){
	return a+b;
}


/ 13	. 

/ In TestBook's Test.cpp	,

#include "my.h"
void testSetGet() {
	my my1 { };
	int a { 7 };
	my1.set_a(a);
	int r=my1.get_a();
	ASSERT_EQUALM("set/get a",a,r);
}

/ we moeten van eclipse my1{} en a{7}, ipv my1 en a=7	, 
/ TODO

void testSum(){
	int a{13},b{7};
	int c{sum(a,b)};
	ASSERT_EQUALM("sum",20,c);
}
/ eclipse  zei dat hij sum niet kende (terwijl de test OK),	 maar doen deden we	op TestHello,
project, c/c++ index, rebuild
/ Toen OK	,

/ 13	. 

/ register bij cute	,
ericjvandervelden@gmail.com
vlWalnoot27
/ Werkt NIET	,

/ 13	. 

/ In c++ is WARN	, 
	char*s="Foo Bar";
/ we moeten 
	const char*s="Foo Bar";
/ en ERR is	, 
	char*t=s;

/ Dit is dus ook ERR	,
$ vi my.h
char*types1(const char*);
$ vi my.cpp
char*types1(const char*s){
->	return s;
}

/ 13	 

	char const*s="Foo Bar";
	s="Eric J.";			/ OK
	s[0]='C';				/ ERR
	char*const t="Foo Bar";
	t="Eric J.";			/ ERR

	char*const t="Foo Bar";
	t[0]='C'
/ ERR	, segm fault	,
/ TODO

		

const char*s	, 
/ of 
char const*s
/ waar s naar points is constant	,
/ je mag dus s="Eric J"	, want waar s naar points ("Foo Bar") verandert niet	,

char*const s
/ s zelf is const	, wat er gebeurt is dat op dezelfde plaats (s) een andere string verschijnt	,

/ 7	. 

/ proj. TestBook, file Test.cpp	,

	const char*s=m2.get_s();
	ASSERT_EQUALM("equals",s,"foo bar");
/ passes	, 
/ dus we kunnen een char* assign aan een const char*	,

/ 7	.

	my m3=retref(m2);
	ASSERT_EQUALM("equals",&m2,&m3);

/ we zien	,
#failure testRefs ../src/Test.cpp:66 testRefs: equals expected:	no operator<<(ostream&, my*)	but was:	no operator<<(ostream&, my*)	
/ TODO

/ Maar &m2 en &m3 zijn verschillend	,

 7	. 
	ASSERT_EQUALM("equals",&m2,&m3);

/ geeft compile errors	,
/ TODO




/ Einde CPP CUTE

/ CPP CH8 IO

/ proj. TestBook

/ 7	. 

/ we moeten inderdaad eerst ios::dec unsetf	,

$ vi main.cpp

void testIo(){
	ios_base::fmtflags flags=ios::dec;
	cout.unsetf(flags);
	flags=ios::hex;
	cout.setf(flags);
	cout<<100<<endl;
}
/ OK
/ we zien	, 
64

/ als we 
	flags=ios::hex|ios::showbase;
/ dan zien we	,
0x64

/ als we 	,
	flags=ios::hex|ios::showbase|ios::uppercase;
/ dan zien we	,
0X64

/ 7	.

/ unsetf, setf (un)sets alleen die flags die genoemd	, 
/ flags sets alle flags: updates die genoemd, en unsets die niet genoemd	,

 Lees	,
http://www.cplusplus.com/reference/ios/ios_base/flags/

/ HIER HIER HIER

/ 7	.

/ Einde CPP CH8 IO

/ ECLIPSE CPP BOOK 

/ 7	. 

[eric@localhost C++]$ pwd
/home/eric/Devel/C/C++
[eric@localhost C++]$ g++ -v second.cpp 
/ doet	,
 /usr/libexec/gcc/x86_64-redhat-linux/4.9.2/cc1plus -quiet -v -D_GNU_SOURCE second.cpp -quiet -dumpbase second.cpp -mtune=generic -march=x86-64 -auxbase second -version -o /tmp/ccgI2CyW.s
 as -v --64 -o /tmp/cc0uiCwh.o /tmp/ccgI2CyW.s
 /usr/libexec/gcc/x86_64-redhat-linux/4.9.2/collect2 -plugin /usr/libexec/gcc/x86_64-redhat-linux/4.9.2/liblto_plugin.so -plugin-opt=/usr/libexec/gcc/x86_64-redhat-linux/4.9.2/lto-wrapper -plugin-opt=-fresolution=/tmp/ccUp3pvC.res -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lgcc --build-id --no-add-needed --eh-frame-hdr --hash-style=gnu -m elf_x86_64 -dynamic-linker /lib64/ld-linux-x86-64.so.2 /usr/lib/gcc/x86_64-redhat-linux/4.9.2/../../../../lib64/crt1.o /usr/lib/gcc/x86_64-redhat-linux/4.9.2/../../../../lib64/crti.o /usr/lib/gcc/x86_64-redhat-linux/4.9.2/crtbegin.o -L/usr/lib/gcc/x86_64-redhat-linux/4.9.2 -L/usr/lib/gcc/x86_64-redhat-linux/4.9.2/../../../../lib64 -L/lib/../lib64 -L/usr/lib/../lib64 -L/usr/lib/gcc/x86_64-redhat-linux/4.9.2/../../.. /tmp/cc0uiCwh.o -lstdc++ -lm -lgcc_s -lgcc -lc -lgcc_s -lgcc /usr/lib/gcc/x86_64-redhat-linux/4.9.2/crtend.o /usr/lib/gcc/x86_64-redhat-linux/4.9.2/../../../../lib64/crtn.o

$ vi second.cpp

#include <iostream>
#include<vector>
using namespace std;

int main() {

    cout << "!!!Hello World!!!" << endl;
    vector<string>v={"one"};
    return 0;
}

/ 7	.

/ we moeten zelf
#include<vector>

/ Hier door wordt vector gekend, in
    vector<string>v={"one"};

[eric@localhost C++]$ g++ -std=c++11 second.cpp
/ OK
/ we moeten -std=c++11 geven	, anders vindt hij {"one"} niet OK	,

/ Daarom moeten we in eclipse (per project) doen, wat hierboven staat , met -std=c++11	.
 /in C proj moeten we -std=gnu11

/ 7	. 

/ we hebben book proj, daarin ch1 folder	, daarin my.h en my.cpp	,
/ we moeten 1 keer project build	, 
/ als we my.cpp veranderen, en we click run		, >	, dan doet hij ook opnieuw proj build	,
/ TODO

/ 7	.

/ Verschillen C, C++

/ 13	. 

$ vi main.cpp

int sum(int,int);

int main(int argc, char **argv) {

	int a,b,c;
	cout<<"Enter two numbers:";
	cin>>a>>b;
	c=sum(a,b);
	cout<<"Sum is: "<<c<<endl;
	return 0;
}

int sum(int a,int b){
	return a+b;
}

/ Inderdaad	, we MOETEN in C++  voor use in main	, prototype geven van sum	,
int sum(int,int);
/ en we MOETEN return type geven, al is dat int	,

/ In C	,

$ vi main.c

int main() {
	int a,b,c;

	puts("!!!Hello World!!!"); /* prints !!!Hello World!!! */

	scanf("%d",&a);
	scanf("%d",&b);
	c=sum(a,b);
	printf("%d\n",c);

	return EXIT_SUCCESS;
}

sum(int a,int b){
	return a+b;
}

/ is OK

/ 13	. 

/ In C betekent 
int main(){
...
}
/ dat we niets zeggen over de params	, 
/ met main(void) wel	,
/ we kunnen dus ook main() use als we geen params hebben	,

/ in C++ is main()==main(void) dus met main() hebben we geen params	,

/ Hoe call main met params en main() def	,
/ TODO

/  7	.

/ In overloading voorbeeld moeten we de fcts _abs noemen	, 
/ met abs(long) ERR,
call of overloaded ‘abs(long int)’ is ambiguous
/ Er is er blijkbaar nog een def	,
/ Dit komt door
using namespace std;
/ TODO
/ See ook manual proj, main.cpp	,

/ Lees	,
http://stackoverflow.com/questions/7549874/how-do-i-fix-an-ambiguous-function-call
http://stackoverflow.com/questions/17189063/stdabs-for-unsigned-long-long-int-data-type

/ In cstdlib zijn abs voor int , long, long long def	, 
 /in project manual, main.cpp

using namespace std;
#include <cstdlib>
int main() {

	int a=abs(7);
	long b=abs(7l);

/ we kunnen ze gewoon use	,

/ 13	. 

/ Kijk in eclipse in console	, daar zien we alle ERRs	, die we ook op de cmd line zien	,

/ In proj book, main.cpp	, als we -std=c++11 use	, zien 	we	,

$ vi main.cpp

#include "my.h"
#include <iostream>
using namespace std;
#include<cmath>

long abs(long);

int main(int argc, char **argv) {
	long l=abs(-7l);
	...
}

long abs(long n){
	if(n<0)return(-n);
	return(n);
}

/ Omdat via iostream er ook een abs(long) is is de call abs(-7l) ambiguous	,


21:39:51 **** Incremental Build of configuration Debug for project book ****
make all 
Building file: ../src/ch1/main.cpp
Invoking: GCC C++ Compiler
g++ -O0 -g3 -Wall -c -fmessage-length=0 -std=c++11 -MMD -MP -MF"src/ch1/main.d" -MT"src/ch1/main.d" -o "src/ch1/main.o" "../src/ch1/main.cpp"
../src/ch1/main.cpp: In function ‘int main(int, char**)’:
../src/ch1/main.cpp:34:16: error: call of overloaded ‘abs(long int)’ is ambiguous
src/ch1/subdir.mk:21: recipe for target 'src/ch1/main.o' failed
  long l=abs(-7l);
                ^
../src/ch1/main.cpp:34:16: note: candidates are:
In file included from /usr/include/c++/4.9.2/cstdlib:72:0,
                 from /usr/include/c++/4.9.2/ext/string_conversions.h:41,
                 from /usr/include/c++/4.9.2/bits/basic_string.h:2850,
                 from /usr/include/c++/4.9.2/string:52,
                 from /usr/include/c++/4.9.2/bits/locale_classes.h:40,
                 from /usr/include/c++/4.9.2/bits/ios_base.h:41,
                 from /usr/include/c++/4.9.2/ios:42,
                 from /usr/include/c++/4.9.2/ostream:38,
                 from /usr/include/c++/4.9.2/iostream:39,
                 from ../src/ch1/main.cpp:9:
/usr/include/stdlib.h:774:12: note: int abs(int)
 extern int abs (int __x) __THROW __attribute__ ((__const__)) __wur;
            ^
../src/ch1/main.cpp:15:6: note: long int abs(long int)
 long abs(long);
      ^
In file included from /usr/include/c++/4.9.2/ext/string_conversions.h:41:0,
                 from /usr/include/c++/4.9.2/bits/basic_string.h:2850,
                 from /usr/include/c++/4.9.2/string:52,
                 from /usr/include/c++/4.9.2/bits/locale_classes.h:40,
                 from /usr/include/c++/4.9.2/bits/ios_base.h:41,
                 from /usr/include/c++/4.9.2/ios:42,
                 from /usr/include/c++/4.9.2/ostream:38,
                 from /usr/include/c++/4.9.2/iostream:39,
                 from ../src/ch1/main.cpp:9:
/usr/include/c++/4.9.2/cstdlib:174:3: note: long long int std::abs(long long int)
   abs(long long __x) { return __builtin_llabs (__x); }
   ^
/usr/include/c++/4.9.2/cstdlib:166:3: note: long int std::abs(long int)
   abs(long __i) { return __builtin_labs(__i); }
   ^
make: *** [src/ch1/main.o] Error 1

21:39:52 Build Finished (took 263ms)

/ Wat er precies staat met stdlib.h	,
/ TODO

/ Wat is abs(int)	, want de debugger valt nergens in	,
/ TODO

/ als we zelf geen abs(long), en wel #include<iostream> 	, dan de debugger valt in	,

$ vi cstdlib
#ifndef __CORRECT_ISO_CPP_STDLIB_H_PROTO
  inline long
  abs(long __i) { return __builtin_labs(__i); }

/ de debugger valt niet in abs(int)	, 
/ wel met cmath




/ 7	.

/ Als we geen #include<iostream>	, en zelf geen abs(long)	, dan een template uit <cmath>	, en ook vallen we in abs(int)	, dat is hetzelfde template als voor abs(long)	,

$ vi cmath

  template<typename _Tp>
    inline _GLIBCXX_CONSTEXPR
    typename __gnu_cxx::__enable_if<__is_integer<_Tp>::__value,
                                    double>::__type
    abs(_Tp __x)
    { return __builtin_fabs(__x); }

/ 7	. 

/ proj book

/ 13	. 

$ vi my.cpp
void my::fct(my o){
	o.set_a(13);
}

$ vi main.cpp
	my my,my2;
	my2.set_a(21);
	my.fct(my2);
	cout<<my2.get_a()<<endl;

/ we zien	,
my()
my()
~my()
21
~my()
~my()
/ we zien geen ctor voor o	,
/ Klopt: lees boek(99)	, het is een copy, geen new	obj	
/ copy ctor? TODO
/ De dtor wordt wel called: als de orig heeft pointer naar mem, en de dtor frees deze	, dan points orig na fct call niet meer naar mem, want is vrijgegeven	, 


/ 13	. 

/ met pointers	,

$ vi my.cpp
void my::fct(my* o){
	o->set_a(13);
}

$ vi main.cpp
	my my,my2;
	my2.set_a(21);
	my.fct(my2);
	cout<<my2.get_a()<<endl;

/ we zien	,
my()
my()
21
~my()
~my()
 
/ 1 dtor minder	,

/ 13	. 

/ (105)

/ return een my	,

$ vi my.cpp

my::my() {
	s='\0';
	cout<<"my()"<<endl;
}

my::~my() {
	cout<<"~my()"<<endl;
	if(s)free(s);
}
void my::set_s(char*str){
	s=(char*)malloc(strlen(str)+1);
	...
	strcpy(s,str);
}

$ vi my.cpp

my input(){
	my my;
	my.set_s("a string");
	return my;
}

$ vi main.cpp
{
	cout<<"Part 4"<<endl;
	my my;
	my=input();
	cout<<my.get_s()<<endl;
}
	return 0

/ we zien 

my()	/ in main.cpp, main: my my
my()	/ in my.cpp, input: my my
~my()	/ in main.cpp, main, als my=input() done	,  dus als we op de cout line	, het is de dtor van my uit input()	,
		/ , we zien geen ctor van de my obj die wordt return	, want dat gaat net als bij een fct param	: copy	,
~my()	/ als uit {} , dus op return line	, het is de dtor van my uit het block	, 

/ we zien de string "a string" dus niet door de cout line in main	, want 	de dtor van de local my in input heeft s free	, en de returned my is een copy daarvan, dus de & is ook copy	, 

/ 7	.

/ friend function	,

/ a friend fct of a class type is not a member fct	, 
/ a friend fct can access the private fields of an instances of that type	,

/ de friend fct moet in dezelfde file def als de class , dus bij ons in my.h
/ TODO

/ in test proj	, 

$ vi you.cpp

class you {
	int a;
public:
	you();
	void set_a(int);
	friend int get_a(you);
};
you::you(){
	cout<<"my()"<<endl;
}
void you::set_a(int b){
	a=b;
}
int get_a(you you){
	return you.a;
}
/ Dit is OK voor eclipse	,

$ vi main.cpp

class you;
int get_a(you);	 / OK




void fct();
int main() {
{
	fct();
	you you; 			/ ERR
	you.set_a(13); 		
	int a=get_a(you);
	cout<<a<<endl;
}

	return 0;
}
void fct(){
	cout<<"foo"<<endl;
}

/ Hoe 
	you you
/ TODO

/ 7	. 

/ friend fct	,

/ als we in proj book	, 

$ vi my.h

#ifndef CH1_MY_H_
#define CH1_MY_H_

class my {
private:
	int a;
	char*s;
public:
	my();
	~my();
	void set_a(int);
	int get_a();
	void set_s(char*);
	char*get_s();
	friend int another_get_a(my);
	friend char*also_get_s(my);
};

#endif /* CH1_MY_H_ */

$ vi my.cpp

#include "my.h"
#include<iostream>
using namespace std;
#include<cstring>
#include<cstdlib>



my::my() {
	s='\0';
	cout<<"my()"<<endl;
}

my::~my() {
	cout<<"~my()"<<endl;
	if(s)free(s);
}

void my::set_a(int n){
	a=n;
}

int my::get_a(){
	return a;
}


void my::set_s(char*str){
	s=(char*)malloc(strlen(str)+1);
	if(s==NULL){
		cout<<"alloc error"<<endl;
		exit(7);
	}
	strcpy(s,str);
}

char*my::get_s(){
	return s;
}

int another_get_a(my my){
	return my.a;
}
/ Eclipse geeft x: member fct 'a' is not visible	, maar het gaat toch OK	,

char*also_get_s(my my){
	return my.s;
}
/ Eclipse geeft x: member fct 's' is not visible	, maar het gaat toch OK	,

$ vi main.cpp

{
	cout<<"Part 4"<<endl;
	my my;
	my=input();
	my.set_s("a string"); 		// redt als straks , als dit block exits	, ~my() wordt called	,	
	cout<<my.get_s()<<endl;
}

{
	cout<<"Part 5"<<endl;
	my my;
	my.set_a(13);
	my.set_s("Foo bar");
	cout<<also_get_s(my)<<endl;// returns "Foo bar" , maar erna wordt ~my() called, die s free	,
	cout<<another_get_a(my)<<endl;// returns 13, maar ~my() geeft ERR, omdat hij s wil free, die al free is	,

}
/ we zien	,

Part 4
my()
my()
~my()
a string
~my()

Part 5
my()
Foo bar
~my()
13
~my()

/ we set Part 4 erbij, omdat wat daar gebeurt ook een rol speelt bij Part5	,

/ de dtor van de local in input() releases s	, dus my=input() heeft s=""	, dus moeten we hem opnieuw set	,

 In Part5	,
	cout<<also_get_s(my)<<endl;// returns "Foo bar" , maar erna wordt ~my() called, die s free	,
	cout<<another_get_a(my)<<endl;// returns 13, maar ~my() geeft ERR, omdat hij s wil free, die al free is	,

/ want inderdaad, na de call & return van also_get_s wordt ~my() called, die s free	, maar na another_get_a gaat dit dus fout	,

/ we maken daarom part 5	,

{
	cout<<"Part 5"<<endl;
	my my;
	my.set_a(13);
	my.set_s("Foo bar");
	cout<<also_get_s(my)<<endl;// returns "Foo bar" , maar erna wordt ~my() called, die s free	,
	my.set_s("Foo bar");	// omdat na another_get_a ~my wordt called	,
	cout<<another_get_a(my)<<endl;// returns 13, en ~my()  free s ,
	my.set_s("Foo bar"); // omdat het exit block ~my calls	,

}

/ 7	. 

/ class arrays	,

{
	cout<<"Part 6"<<endl;
	my mys[4];
	for(int i=0;i<4;i++){
		mys[i].set_a(i);
	}
	for(int i=0;i<4;i++){
		int j=mys[i].get_a();
		cout<<j<<endl;
	}
}

/ we zien	,

Part 6
my()
my()
my()
my()
0
1
2
3
~my()
~my()
~my()
~my()

/ 7	. 

/ init class array with 1-param ctor	,

$ vi you.h

class you {
private:
	int a;
public:
	you();
	you(int);
	...

$ vi you.cpp
...
you::you(int a){
	set_a(a);
}

$ vi main.cpp

{
	cout<<"Part 7"<<endl;
	you you[4]={1,2,3,4};
	for(int i=0;i<4;i++){
		int j=you[i].get_a();
		cout<<j<<endl;
	}
}


Part 7
1
2
3
4
~you()
~you()
~you()
~you()

/ 7	. 

/ pointers	,

/ in book proj	,
$ vi main.cpp

{
	cout<<"Part 8"<<endl;
	you arr[4]={1,2,3,4};
	you*p=arr;
	for(int i=0;i<4;i++){
		int j=p->get_a();
		cout<<j<<endl;
		p++;
	}
}

/ we zien	,

Part 8
you(int)
you(int)
you(int)
you(int)
1
2
3
4
~you()
~you()
~you()
~you()

/ 7	.

/ In een member fct we kunnen 	,
	a=n;
/ or	
	this->a=n;

/ 7	.

/ Lees	,
http://en.cppreference.com/w/cpp/language/sizeof

/ 7	.

{
	int*p=new int;
	*p=13;
	cout<<*p<<endl;
	delete p;
	delete p;
/ ERR
}

/ we kunnen net als free niet 2 keer call 	,
/ we zien dan in de shell waar we eclipse/eclipse gaven de ERRs	,

/ 7	.

/ Lees	,
http://stackoverflow.com/questions/4108313/how-do-i-find-the-length-of-an-array
/ over length array in c++

	int i[3];
	cout<<sizeof(i)/sizeof(*i)<<endl;
	int*j=new int[3];
	cout<<sizeof(j)/sizeof(*j)<<endl;

/ sizeof(i)==12	,sizeof(*i)==4	,
/ sizeof(j)==8: een int* is blijkbaar 2 bytes	,

/ 7	. 

	cout<<"Part 10"<<endl;
	you*q=new you(7,"eric j");
	you*r=new you[3];
	delete q;
	delete[]r;

/ we kunnen NIET	,
	you r[]=new you[3];
/ of	,
	you r[3]=new you[3];

/ we zien	,

Part 10
13
you(int,char*)
you()
you()
you()
~you()
~you()
~you()
~you()

/ 7	.

/ references	,

/ proj book, main.cpp	,

{
	cout<<"Part 11"<<endl;
	int n=5;
	void plus1(int&);
	plus1(n);
	cout<<n<<endl;
	int&m=n;
	m+=1;
	cout<<n<<endl;
}
	return 0;
}

void plus1(int&n){
	n+=1;
}

/ we zien 
6
7

/ 7	. 

/ we willen niet cp arg	,
/ we kunnen ref use, of copy ctor	,
/ we use nu ref	,

	you y=you(13,"eric j");
	void funref(you&);
	funref(y);
	cout<<y.get_s()<<endl;
}
	return 0;
}

void funref(you&y){
	y.set_s("foo bar");
}

/ we zien	,
you(int,char*)
foo bar
~you()
/ de dtor wordt called als het block eindigt en y wordt release	, dat is altijd	,

/ 7	. 

/ return ref	,

	you&retref();
	retref().set_a(13);
}
	return 0;
}

you&retref(){
	you y(5,"eric j");
	return y;
}

/ In gdb zien we dat set_a wordt called op de you die in retref is create	,
/ In book (151) staat dat dit niet kan	,
/ c++ copies nu NIET de local 	, 
/ Toch werkt het	,
/ TODO

/ Beter is dan om te doen	,

	you z=you(13,"saskia");
	you&retref(you&);
	retref(z).set_a(13);
}
	return 0;
}

you&retref(you&y){
	return y;
}

/ 7	. 

/ proj book	, main.cpp, funp.cpp/.h

/ function pointers zoals in C werken ook in C++
 /we kunnen ook de c++ &' use	,

$ vi funp.cpp

void local_main(){
	fctarg(twice);
	cout<<fun()(8)<<endl;
	int r=fctarg2(ret_a);
	cout<<r<<endl;
}


int twice(int i){
	return 2*i;
}

int ret_a(you&y){
	return y.get_a();
}

void fctarg(int(*f)(int)){
	int r=f(13);
	cout<<r<<endl;
}

int fctarg2(int(*f)(you&)){
	you y=you(13,"eric");
	int r=f(y);
	return r;
}

int(*fun())(int){
	return twice;
}

/ en in main.cpp	,

	cout<<"Part 12"<<endl;
#include"funp.h"
	local_main();

/ we zien	,

Part 12
26
16
you(int,char*)
~you()
13

/ 7	.

/ function overloading	,

/ ctor overloading: copy ctor	,

/ book proj	, 

$ vi my.h
	my(const my&);

$ vi  my.cpp

my::my(const my&m){
	a=m.a;
	long l=strlen(m.s);
	s=new char[l];
	strcpy(s,m.s);
}
char*also_get_s(my my){
	return my.s;
}

$ vi main.cpp
	{
		cout<<"Part 13"<<endl;
		my my;
		my.set_a(13);
		my.set_s("Foo bar");
		cout<<also_get_s(my)<<endl;// returns "Foo bar" , maar erna wordt ~my() called, die s free	,

	}

/ Dit gaat nu OK	, niet meer 2 keer ~my op dezelfde inst	, vanwege dat de local in also_get_s free	, en de local in het block free	,
/ nu is de local in also_get_s een copy 	, 

/ we zien	,

Part 13
my()
Foo bar
~my()
~my()

/ 13	. 

/ we doen	,
my::my(const my&m){
	long l=strlen(m.s);
/ we wilden eig	, 
	long l=strlen(m.get_s());
/ Maar dit kan niet: strlen verwacht een const char*	,  en we kunnen my::get_s NIET const char* maken?
/ TODO

/ we lezen	,
http://www.cplusplus.com/reference/cstring/strlen/
size_t strlen ( const char * str );
char * strcpy ( char * destination, const char * source );

/ 13	. 

/ const char*s: waar s naar points mag niet veranderen	,
/ char*const t: t mag niet veranderen	,

	const char*s="Foo Bar";
	s="Eric J.";
//	s[0]='C';
	char*const t="Foo Bar";
	t[0]='C';
//	t="Eric J.";

	char*u=s;
/ ERR	,

	char*u=t;
/ OK

	char*k{"Foo Bar"};
/ deprecated conversion from string constant to char*

/ 13	. 

const char*sfun(){
	return "Foo Bar";
}
void testTypes(){
	...
	const char*k=sfun();
/ OK

/ 13	. 

/ we zien dat kan	,

const char*you::get_s(){
	return s;
}
/ TODO

/ 7	.


/ Einde ECLIPSE CPP BOOK 


/ ECLIPSE C

/ 7	. 

/ Dit kan niet in C, met deze fct	,

struct my{
	int a;
	void set_a(int);
};

/ 7	. 

/ Dit kan niet in C, met deze fct	,

struct my{
	int a;
	void set_a(int);
};

/ 7	. 

/ Dit kan niet in C, met deze fct	,

struct my{
	int a;
	void set_a(int);
};

/ 7	. 

/ Dit kan niet in C, met deze fct	,

struct my{
	int a;
	void set_a(int);
};

/ 7	. 

/ Dit kan niet in C, met deze fct	,

struct my{
	int a;
	void set_a(int);
};

/ 7	. 

/ Dit kan niet in C, met deze fct	,

struct my{
	int a;
	void set_a(int);
};

/ 7	. 

/ in book_c proj	,

/ Dit kan niet in C, met deze fct	,

struct my{
	int a;
	void set_a(int);
};

/ we doen	, 

$ vi my.h

struct my{
	int a;
	void(*set_a)(struct my*,int);
	int(*get_a)(struct my*);
};

void set_a(struct my*,int);
int get_a(struct my*);

$ vi my.c

#include "my.h"

void set_a(struct my*p,int a){
	p->a=a;
}
int get_a(struct my*p){
	return p->a;
}

$ vi main.c

int sum(int,int);

	{
		int a,b,c,n;

		a=13;b=7;
		c=sum(a,b); 				/ *
	}
	{
		int(*f)(int,int)=sum;		/ **
		int s=f(7,13);
		printf("%d\n",s);
	}

	{
		struct my*p=malloc(sizeof(struct my));
		int j;
		p->set_a=set_a;				/ ***
		p->get_a=get_a;				/ ****
		p->set_a(p,14);
		j=p->get_a(p);
		printf("%d\n",j);

	}

/ dus het link van main.o en my.o gaat OK (see set_a en get_a)	,

/ we moeten in main.c forward decl	,
int sum(int,int);
/ anders ERR bij **	,
/ maar voor * hoeft het NIET	,
/ TODO

/ we moeten om dezelfde reden in my.h	, 
void set_a(struct my*,int);
int get_a(struct my*);
/ anders gaat ERR bij *** en ****	,

/ we moeten inderdaad *** en **** doen	,

/ 7	. 

/ We oef verder met fct pointers	,

/ Lees	,
http://www.newty.de/fpt/fpt.html

/ we maken een fct pointer als arg en als return value	,

$ vi you.h

int twice(int i);

void fct(int(*f)(int));		// f is een int(*)(int)
int(*fun())(int); 			// fun() returns int(*)(int)

$ vi you.c

int twice(int i){
	return 2*i;
}

void fct(int(*f)(int)){
	int r=f(13);
	printf("%d\n",r);
}

int(*fun())(int){
	return twice;
}

$ vi main.c

	{
		fct(twice);
		int(*f)(int)=fun();
		int j=f(8);
		printf("%d\n",j);
		j=fun()(8);
		printf("%d\n",j);
	}

/we zien	,

26
16
16

 





/ Einde ECLIPSE C

/ YUM

[eric@almond Eclipse]$ sudo yum list available *maven*
Repodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fast

/ Lees	,
https://fedoraproject.org/wiki/AutoUpdates

[eric@almond Eclipse]$   sudo yum install -y yum-cron
[eric@almond Eclipse]$ sudo vi /etc/yum/yum-cron.conf 
apply_updates = yes

[eric@almond Eclipse]$ sudo yum install maven




/ Einde YUM 

/ ECLIPSE JEE

[eric@almond jee]$ tar xvzf ~/Downloads/eclipse-jee-luna-SR2-linux-gtk-x86_64.tar.gz 


/ Einde ECLIPSE JEE

/ PARI GP

/ 7	. 

[eric@localhost pari-2.7.3]$ pwd
/home/eric/Devel/Math/pari-2.7.3
$ ./Configure

Configuring pari-2.7.3 (STABLE) 
Checking echo to see how to suppress newlines...
...using -n.
Looking for some tools first ...
...gzip is /usr/bin/gzip
...cc is /usr/lib64/ccache/cc
...gcc is /usr/lib64/ccache/gcc
...ld is /usr/bin/ld
...perl is /usr/bin/perl
...zcat is /usr/bin/zcat
Choosing C compiler ...
GNU compatible compiler: gcc version 4.9.2 20150212 (Red Hat 4.9.2-6) (GCC)
Using mt engine single
Given the previous choices, sizeof(long) is 8 chars.
The internal word representation of a double is not needed (64bit).
==========================================================================
Building for: amd64 running linux (x86-64/GMP kernel) 64-bit version
==========================================================================
C compiler is          /usr/lib64/ccache/gcc -O3 -Wall -fno-strict-aliasing -fomit-frame-pointer    -fPIC
Executable linker is   /usr/lib64/ccache/gcc  -O3 -Wall -fno-strict-aliasing -fomit-frame-pointer    -Wl,--export-dynamic 
Dynamic Lib linker is  /usr/lib64/ccache/gcc  -shared  $(CFLAGS) $(DLCFLAGS) -Wl,-shared,-soname=$(LIBPARI_SONAME) 
Looking in C lib for some symbols...
...Found exp2.
...Found log2.
...Found strftime.
...Found getrusage.
...Found sigaction.
...Found TIOCGWINSZ.
...Found getrlimit.
...Found stat.
...Found vsnprintf.
...Found waitpid.
...Found setsid.
...Found getenv.
...Found isatty.
...Found alarm.
...I did not find dlopen.
Try again, with -ldl this time...
...Found dlopen.
Checking for optional libraries and headers...
###
### libgmp.so not found. Maybe install gmp development files?
### E.g.gmp-devel (RPM) or libgmp-dev (Debian) packages
###
...gmp header file not found by Configure, trying to proceed
### Your GMP library is incompatible with the compiler settings.
### Building without GNU MP support
###
### libX11.so not found. Maybe install X11 development files?
### E.g.[XFree86|xorg-x11|libx11]-devel (RPM) or libx11-dev (Debian) packages
###
### X11 not found
...X11 libraries: 
### FLTK not found. Building without FLTK support
### Qt not found. Building without Qt support
Hi-Res Graphics: ps
###
### libreadline.so not found. Maybe install readline development files?
### E.g.readline-devel (RPM) or libreadline-dev (Debian) packages
###
...readline header file not found by Configure, trying to proceed
...Linking failed. Trying with libncurses
###
### libncurses.so not found. Maybe install ncurses development files?
### E.g.ncurses-devel (RPM) or libncurses-dev (Debian) packages
###
...Linking failed. Trying with libtermcap
###
### Readline library does not seem to work. Maybe install libncurses?
###
### Building without GNU readline support
Installation prefix ? [/usr/local]
...for architecture-independent files (share-prefix) ? [/usr/local/share]
Installation directories for:
...executables (gp, gphelp) ? [/usr/local/bin]
...libraries (libpari) ? [/usr/local/lib]
...include files ? [/usr/local/include]
...manual pages ? [/usr/local/share/man/man1]
...other system-dependent data ? [/usr/local/lib/pari]
...other system-independent data ? [/usr/local/share/pari]
Default is dynamic executable and shared library
==========================================================================
Extracting examples/Makefile.linux-x86_64
Extracting Olinux-x86_64/Makefile
Extracting Olinux-x86_64/paricfg.h
Extracting Makefile
Extracting scripts and macros
...in doc
...in misc
==========================================================================
Shall we try to build pari 2.7.3 (released) now (y/n)? [n]
Ok. Type "make install" when you are ready
Bye !

/ 7	. 

[eric@localhost pari-2.7.3]$ readelf -Ws /usr/lib64/libX11.so.6
/ of	,
[eric@localhost pari-2.7.3]$ objdump -T /usr/lib64/libX11.so.6
...

[eric@localhost pari-2.7.3]$ repoquery -l libX11 | grep -- .so
/usr/lib/libX11.so.6
/usr/lib/libX11.so.6.3.0
/usr/lib64/libX11.so.6
/usr/lib64/libX11.so.6.3.0
...

/ Maar in /usr/lib zijn er geen libX11.so.6 en libX11.so.6.3.0	,

/ 7	.

$ make all
$ make bench
[eric@localhost pari-2.7.3]$ sudo make install
[eric@localhost pari-2.7.3]$ cp misc/gprc.dft ~/.gprc


/ Niet grafisch installed?
/ TODO
/ -g 
/ TODO

/ 7	. 

/ Lees	,
http://pari.math.u-bordeaux.fr/pub/pari/manuals/2.7.0/tutorial.pdf

$ gp
...





/ Einde PARI GP

/ PL/PGSQL

[eric@almond PL]$ pwd
/home/eric/Devel/Postgres/PL

/ Lees	,
http://www.postgresql.org/docs/9.4/static/plpgsql.html



/ Einde PL/PGSQL

/ OPENSSL

[eric@almond OpenSSL]$ pwd
/home/eric/Devel/OpenSSL
$ ./config -d
$ make
$ make test
$ sudo make install


/ Einde OPENSSL

/ GIT

/ Kijk	,
https://www.youtube.com/watch?v=MQ9Yv8csyMU

https://www.youtube.com/watch?v=SyMkLQLC3Kg


/ Einde GIT

/ GRADLE

/ Lees	,
https://docs.gradle.org/current/userguide/installation.html

[eric@almond Java]$ unzip  ~/Downloads/gradle-2.4-all.zip 

[eric@almond quickstart]$ pwd
/home/eric/Devel/Gradle/Java/gradle-2.4/samples/java/quickstart

/ 13	. 

[eric@almond ch6.6]$ pwd
/home/eric/Devel/Gradle/Java/my/ch6.6
[eric@almond ch6.6]$ cat build.gradle 
4.times { counter ->
	task "task$counter" << {
		println "I'm task number $counter"
	}
}
[eric@almond ch6.6]$ ../../gradle-2.4/bin//gradle task2
I'm task number 2
[eric@almond ch6.6]$ ../../gradle-2.4/bin//gradle -q task5

FAILURE: Build failed with an exception.

* What went wrong:
Task 'task5' not found in root project 'ch6.6'. Some candidates are: 'task0', 'task1', 'task2', 'task3', 'tasks'.

* Try:
Run gradle tasks to get a list of available tasks. Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

/ 13	. 


[eric@almond ch7.2]$ pwd
/home/eric/Devel/Gradle/Java/my/ch7.2

[eric@almond ch7.2]$ vi build.gradle 
apply plugin: 'java'

/ er is verder geen file	,
/ toch kunnen we	,

eric@almond ch7.2]$ gradle build
:compileJava UP-TO-DATE
:processResources UP-TO-DATE
:classes UP-TO-DATE
:jar
:assemble
:compileTestJava UP-TO-DATE
:processTestResources UP-TO-DATE
:testClasses UP-TO-DATE
:test UP-TO-DATE
:check UP-TO-DATE
:build

[eric@almond ch7.2]$ find
.
./.gradle
./.gradle/2.4
./.gradle/2.4/taskArtifacts
./.gradle/2.4/taskArtifacts/fileSnapshots.bin
./.gradle/2.4/taskArtifacts/cache.properties.lock
./.gradle/2.4/taskArtifacts/fileHashes.bin
./.gradle/2.4/taskArtifacts/cache.properties
./.gradle/2.4/taskArtifacts/taskArtifacts.bin
./.gradle/2.4/taskArtifacts/outputFileStates.bin
./build.gradle
./build
./build/tmp
./build/tmp/jar
./build/tmp/jar/MANIFEST.MF
./build/libs
./build/libs/ch7.2.jar

[eric@almond ch7.2]$ jar tvf build/libs/ch7.2.jar 
     0 Wed Jun 10 22:15:10 CEST 2015 META-INF/
    25 Wed Jun 10 22:15:10 CEST 2015 META-INF/MANIFEST.MF

/ 7	.

[eric@almond ch7.2]$ gradle 
:help

Welcome to Gradle 2.4.

To run a build, run gradle <task> ...

To see a list of available tasks, run gradle tasks

To see a list of command-line options, run gradle --help

To see more detail about a task, run gradle help --task <task>


[eric@almond ch7.2]$ gradle clean
:clean

BUILD SUCCESSFUL

Total time: 2.813 secs

This build could be faster, please consider using the Gradle Daemon: http://gradle.org/docs/2.4/userguide/gradle_daemon.html
[eric@almond ch7.2]$ find
.
./.gradle
./.gradle/2.4
./.gradle/2.4/taskArtifacts
./.gradle/2.4/taskArtifacts/fileSnapshots.bin
./.gradle/2.4/taskArtifacts/cache.properties.lock
./.gradle/2.4/taskArtifacts/fileHashes.bin
./.gradle/2.4/taskArtifacts/cache.properties
./.gradle/2.4/taskArtifacts/taskArtifacts.bin
./.gradle/2.4/taskArtifacts/outputFileStates.bin
./build.gradle

/ 7	.





/ Einde GRADLE

/ SPRING

/ 7	.

[eric@almond Spring]$ pwd
/home/eric/Devel/Java/Spring
[eric@almond Spring]$ unzip  ~/Downloads/sia2_code.zip 
[eric@almond Spring]$ cd sia2/

[eric@almond Spring]$ pwd
/home/eric/Devel/Java/Spring
[eric@almond Spring]$ git clone https://github.com/habuma/SpringInActionExamples.git sia4

/ 7	.

/ we maken de appl van 	,
http://projects.spring.io/spring-framework/


/ Lees	,
http://docs.spring.io/spring/docs/current/spring-framework-reference/html/
/ we waren in 	Beans, ch5	,
http://docs.spring.io/spring/docs/current/spring-framework-reference/html/beans.html
/ en in Testing, ch11
http://docs.spring.io/spring/docs/current/spring-framework-reference/html/testing.html

/ Lees	, 
http://docs.spring.io/docs/Spring-MVC-step-by-step
/ we waren in Business logic	, 
http://docs.spring.io/docs/Spring-MVC-step-by-step/part3.html
/ met tests	,

/  7	. 

/ we willen de main method exec	, 

/ Lees	,
http://books.sonatype.com/mvnex-book/reference/customizing-sect-custom-packaged.html

/ we add de assembly goal to de package phase	, anders moeten we de assembly:assembly	goal, en die is na de install phase	,  
/ de attached goal=assembly goal	,

   <plugin>
        <artifactId>maven-assembly-plugin</artifactId>
        <configuration>
          <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
          </descriptorRefs>
        </configuration>
        <executions>
          <execution>
            <id>simple-command</id>
            <phase>package</phase>
            <goals>
              <goal>attached</goal>
            </goals>
          </execution>
        </executions>
      </plugin>

/ zonder <executions/> moeten we de assembly:assembly goal geven,	 nu de package phase	,

/ we doen 
$ mvn clean package

/ we zien	,
./target/spring-first-jar-with-dependencies.jar
./target/spring-first.war

/ spring-first.war zien we altijd met package	, hier zit ook de Application.class in	, slaat nergens op	,
/ in spring-first-jar-with-dependencies.jar zitten alle classes uit de dependencies uit de pom	,



/ Einde SPRING

/ SPRING MVC XML

/ we lezen	,
http://docs.spring.io/docs/Spring-MVC-step-by-step/index.html
/ Dit is voor spring 2.5	,

/ 7	.

/ Lees	,
http://docs.spring.io/docs/Spring-MVC-step-by-step/part1.html

/ dit is oud	, maar lijkt veel op sia2	,

/ we maken in sts spring-mvc-step-by-step appl	,

/ 7	.

/ ch1

/ create spring managed web controller	,  in web application context	,
/ de dispatcher leest deze ook (en redirects requests op  /hello.htm naar de web controller	,
/ test web controller	,

/ WEB-INF/ is niet in src/main/resources, maar in src/main/webapp	, 

$ vi web.xml

<web-app version="2.5"
         xmlns="http://java.sun.com/xml/ns/javaee"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://java.sun.com/xml/ns/javaee 
         http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd" >

  <welcome-file-list>
    <welcome-file>
      index.jsp
    </welcome-file>
  </welcome-file-list>

</web-app>

/ we lezen	,
By default server looks for the welcome file in following order:

    welcome-file-list in web.xml
    index.html
    index.htm
    index.jsp

If none of these files are found, server renders 404 error.

/ bij ons is er alleen index.jsp, dus is niet nodig eig.	,

 7	.

/ 1ste webapp	,

/ 13	, 

$ vi pom.xml

 <build>
    <finalName>spring-mvc-step-by-step</finalName>

/ Zo heet de war	, en dus context root in de url	,

/ als we deze finalName weglaten, heet de war zo	, (de version komt erbij)	,

./target/spring-mvc-step-by-step-0.0.1-SNAPSHOT/WEB-INF/springapp-servlet.xml

/ 13	. 

/ we kijken in de XmlWebApplicationContext config file	, (springapp is de name van de DispatcherServlet, de servlets zijn config in web.xml)

$ vi springapp-servlet.xml
      <bean 
       		name="/hello.htm" 
       		class="my.own.spring_mvc_step_by_step.HelloController">      
       </bean>

/ Vergeet niet de / in name="/hello.htm"	,

/ 13 	 .

/ we maken de controller	,

$ vi HelloController.java

public class HelloController implements Controller {

	public ModelAndView handleRequest(HttpServletRequest arg0,
			HttpServletResponse arg1) throws Exception {
		ModelAndView modelAndView=new ModelAndView("hello.jsp");
		return modelAndView;
	}

}

/ en een test erop	,

$ vi HelloController.java

public class HelloControllerTest {

	@Test
	public void first() throws Exception{
		HelloController helloController=new HelloController();
		ModelAndView modelAndView=helloController.handleRequest(null, null);
		Assert.assertEquals("hello.jsp", modelAndView.getViewName());
	}
}

/ 7	.

/ ch2

/ decouple view en web controller	,
/ test de web controller	,

/ we copy proj spring-mvn-step-by-step to spring-mvn-step-by-step2	,
$ vi pom.xml
	...
  <artifactId>spring-mvc-step-by-step2</artifactId>
	...
  <build>
    <finalName>spring-mvc-step-by-step2</finalName>

/ we moeten jstl en standard add aan pom	, anders gaat uri ERR in	, 
<%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core"%>

$ vi pom.xml

   <dependency>
	    <groupId>javax.servlet</groupId>
	    <artifactId>jstl</artifactId>
	    <version>${jstl.version}</version>
	</dependency>
	<dependency>
    	<groupId>taglibs</groupId>
    	<artifactId>standard</artifactId>
    	<version>${standard.version}</version>
	</dependency>

$ vi include.jsp
	<%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core"%>
	<%@ taglib prefix="fmt" uri="http://java.sun.com/jsp/jstl/fmt"%>

/ 13	. 

/ Lees	,
https://community.oracle.com/thread/310170

$ vi index.jsp

<%@ include file="/WEB-INF/jsp/include.jsp" %>

<%-- Redirected because we can't set the welcome page to a virtual URL. --%>
<c:redirect url="/hello.htm"></c:redirect>

/ ze bedoelen dat de welcome page NIET /hello.htm kan zijn (/hello.htm is de virtual url)	, het moet index.jsp zijn bijv, en in index.jsp moeten we redirect naar de virtual url, 
/ de spring's web application config heeft	, 
	<bean name="/hello.htm" class="springapp.web.HelloController"/>
/ eig zit daar de BeanNameUrlHandlerMapping tussen, maar we kunnen /hello.htm zien als (de name van de ) controller	, 
/ eig: de dispatcher servlet kijkt naar de context, en schakelt de BeanNameUrlHandlerMapping in	, die met de controller komt aanzetten	,
die dan naar de dispatcher servlet gaat	, en daarna dus naar de controller	, 

/ 13	.

 /we set hello.jsp ook in /WEB-INF/jsp/	,
$ vi hello.jsp

<%@ include file="/WEB-INF/jsp/include.jsp" %>
<p>Greetings, it is now <c:out value="${now }"></c:out> </p>

/ als we include.jsp OK write, dan kent hij <c:out value="${...}"/>


 7	. 

/ unit test	, 

$ vi pom.xml

	<dependency>
	    <groupId>org.mockito</groupId>
	    <artifactId>mockito-core</artifactId>
	    <version>${mockito.version}</version>
	</dependency>



$ vi HelloControllerTest.java

	@Test
	public void first() throws Exception{
		HelloController helloController=new HelloController();
		ModelAndView modelAndView=helloController.handleRequest(null, null);
		Assert.assertEquals("/WEB-INF/jsp/hello.jsp", modelAndView.getViewName());
		Map<String, Object>model=modelAndView.getModel();
		Assert.assertNotNull(model);
		Object value=model.get("now");
		Assert.assertThat(model.get("now"),IsInstanceOf.instanceOf(String.class) );

// of zelf	,
		Assert.assertThat(model.get("now"), new MyIsInstanceof());
		
	}
	private class MyIsInstanceof extends ArgumentMatcher<Object>{

		@Override
		public boolean matches(Object argument) {
			return argument instanceof String;
		}
		
	}

/ ArgumentMatcher is uit Mockito	,
    java.lang.Object
        org.hamcrest.BaseMatcher<T>
            org.mockito.ArgumentMatcher<T>

public class HelloControllerTest {
	private class MyIsInstanceOf2 extends BaseMatcher<Object>{
/ TODO

/ 7	.

$ vi HelloController.java

public class HelloController implements Controller {

	public ModelAndView handleRequest(HttpServletRequest arg0,
			HttpServletResponse arg1) throws Exception {
		String now=(new Date()).toString();
		ModelAndView modelAndView=new ModelAndView("/WEB-INF/jsp/hello.jsp","now",now);
		return modelAndView;
	}

}

/ Geef http://localhost:8080/spring-mvc-step-by-step2/hello.htm	,

/  7	.

/ In de controller staat de url hard in	, willen we niet	, we willen een logical name, zodat we de view kunnen veranderen, en de controller niet	,

/ we introduce een nieuwe bean, dus we gaan naar de spring web app context  config file	, die gedeeltelijk de naam van de dispatcher servlet heeft	(de dispatcher servlet consults deze file ook), 
$ vi springapp-servlet.xml

       <bean
       		id="viewResolver"
       		class="org.springframework.web.servlet.view.InternalResourceViewResolver"
       	>
       		<property name="viewClass" value="org.springframework.web.servlet.view.JstlView"></property>
       		<property name="prefix" value="/WEB-INF/jsp/"></property>
       		<property name="suffix" value=".jsp"></property>
       </bean>

$ vi HelloController.java

		ModelAndView modelAndView=new ModelAndView("hello","now",now);

$ vi HelloControllerTest.java

		Assert.assertEquals("hello", modelAndView.getViewName());

/ OK
/ Als we de viewResolver bean out comment, passes de test ook	,
/ TODO
/ Als we deploy niet	, dan werkt de bean echt	,
/ de test is niet OK
/ TODO

/ 7	.

/ ch3 

/ proj. spring-mvc-step-by-step2	,

/ maak Product	,
/ test product,	

/ maak ProductManagerImpl,	 service layer
/ 
/ De ProductManagerTest tests de ProductManagerImpl, zonder dat hij spring managed is	, dus gewoon met new created	, 

/ Later wordt de List<Product> vervangen  door een ProductDao	, 
/ nu kan de test hem gewoon een list van products geven	, en testen hoe hij daar mee omgaat (dwz of hij de prijsverhoging goed berekent)	, 
/ later moeten we dus de product dao mock, of iets anders ... (TODO)
/ Nu is de product manager impl GEEN spring managed bean: straks pas in ch4 als hij injected wordt in de web controller	, dan is het pas nodig	, 
/ de product manager (spring managed) straks in ch4 is in feite een mock: hij heeft een product list en in de spring context wordt deze list create	, later wordt deze list vervangen door een dao, en dan is de product manager geen mock meer,	
/ het geven van een list van products in de test is eig  overbodig, maar pas in ch4 wordt de mock spring bean create, 	
/ de web controller was al meteen wel spring managed	, omdat de dispatcher servlet de web application context raadpleegt	,  WH (TODO)

/ we maken interface ProductManager  en class ProductManagerImpl	,

$ vi ProductManagerImpl.java

public class ProductManagerImpl implements ProductManager {

	public void increasePrice(int percentage) {
		throw new UnsupportedOperationException();
	}
	public List<Product> getProducts() {
		throw new UnsupportedOperationException();
	}
	public void setProducts(List<Product> products) {
		throw new UnsupportedOperationException();
	}
}

$ vi ProductManagerTest.java

public class ProductManagerTest {

	private ProductManager productManager;

	@Before
	public void setUp() throws Exception {
		productManager=new ProductManagerImpl();
	}

	@Test(expected=UnsupportedOperationException.class)
	public void test() {
		Assert.assertNull(productManager.getProducts());
	}

}
/ OK
/ passes	, 

/ 7	.

/ we run de test en we zien de unsupported operation exc	,
	@Test
	public void test2(){
		productManager.increasePrice(priceIncrease);
		
	}

/ Dat betekent dat we naar de product manager moeten, en de method impl	,

$ vi ProductManagerImpl.java

	public void increasePrice(int percentage) {
//		throw new UnsupportedOperationException();
		if(products!=null){
			for (Product product:products){
				product.setPrice(product.getPrice()*(100+percentage)/100);
				
			}
		}

	}

	public List<Product> getProducts() {
//		throw new UnsupportedOperationException();
		return products;
	}

	public void setProducts(List<Product> products) {
//		throw new UnsupportedOperationException();
		this.products=products;
	}

/ In de test geven de de product manager gewoon een list van products, en kijken hoe hij daarmee omgaat: of hij de prijsverhoging goed berekent	,


	

$ vi ProductManagerTest{

public class ProductManagerTest {

	private ProductManager productManager;
	private List<Product>products;
	
	private final static int productCount=2;
	private final static double chairPrice=20.50;
	private final static String chairDescription="Chair";
	private final static double tablePrice=105.10;
	private final static String tableDescription="Table";
	private final static int priceIncrease=10;
	
	
	@Before
	public void setUp() throws Exception {
		productManager=new ProductManagerImpl();
		products=new ArrayList<Product>();
		
		// stub a list of products
		Product product=new Product();;
		product.setDescription(chairDescription);
		product.setPrice(chairPrice);
		products.add(product);
		
		product=new Product();
		product.setDescription(tableDescription);
		product.setPrice(tablePrice);
		products.add(product);
		
		productManager.setProducts(products);
	}

	@Test(expected=UnsupportedOperationException.class) @Ignore
	public void test() {
		Assert.assertNull(productManager.getProducts());
	}
	
	@Test
	public void testWithNullList(){
		productManager=new ProductManagerImpl();
		productManager.increasePrice(priceIncrease);
		
	}
	
	@Test
	public void testWithEmptyList(){
		productManager=new ProductManagerImpl();
		productManager.setProducts(new ArrayList<Product>());
		productManager.increasePrice(priceIncrease);
		
	}
	
	@Test
	public void testWithList(){
		productManager.increasePrice(10);
		List<Product>products=productManager.getProducts();
		
		for (Product product:products){
			double increase=product.getPrice();
			if(product.getDescription()=="Chair"){
				Assert.assertEquals(22.55, product.getPrice(),0);
			}else if(product.getDescription()=="Table"){
				Assert.assertEquals(115.61, product.getPrice(),0);
			}
		}
	}
	

}
/ OK

/ 7	. 

/ ch3

/ proj. spring-mvc-step-by-step2	,



$ vi ProductManagerTest{

public class ProductManagerTest {

	private ProductManager productManager;
	private List<Product>products;
	
	private final static int productCount=2;
	private final static double chairPrice=20.50;
	private final static String chairDescription="Chair";
	private final static double tablePrice=105.10;
	private final static String tableDescription="Table";
	private final static int priceIncrease=10;
	
	
	@Before
	public void setUp() throws Exception {
		productManager=new ProductManagerImpl();
		products=new ArrayList<Product>();
		
		// stub a list of products
		Product product=new Product();;
		product.setDescription(chairDescription);
		product.setPrice(chairPrice);
		products.add(product);
		
		product=new Product();
		product.setDescription(tableDescription);
		product.setPrice(tablePrice);
		products.add(product);
		
		productManager.setProducts(products);
	}

	@Test(expected=UnsupportedOperationException.class) @Ignore
	public void test() {
		Assert.assertNull(productManager.getProducts());
	}
	
	@Test
	public void testWithNullList(){
		productManager=new ProductManagerImpl();
		productManager.increasePrice(priceIncrease);
		
	}
	
	@Test
	public void testWithEmptyList(){
		productManager=new ProductManagerImpl();
		productManager.setProducts(new ArrayList<Product>());
		productManager.increasePrice(priceIncrease);
		
	}
	
	@Test
	public void testWithList(){
		productManager.increasePrice(10);
		List<Product>products=productManager.getProducts();
		
		for (Product product:products){
			double increase=product.getPrice();
			if(product.getDescription()=="Chair"){
				Assert.assertEquals(22.55, product.getPrice(),0);
			}else if(product.getDescription()=="Table"){
				Assert.assertEquals(115.61, product.getPrice(),0);
			}
		}
	}
}

/ 7	. 

/ ch4

/ (mock) product manager impl (service layer) wordt spring bean , en wordt injected in de web controller	,
/ ze test de web controller met de mock product manager (in ch3 hadden ze een test van de product manager en in de test werd deze een list van products gegeven	, maar eig overbodig, als ze in ch3 de product manager al een spring bean hadden gemaakt	, 


/ proj. spring-mvc-step-by-step2	,

/ we zien dat de ProductManager voorkomt in de test op de controller	, moeten we deze niet mock?
/ TODO

/ we copy HelloController(Test) naar InventoryController(Test)	,

/ we hadden	, 
$ vi HelloController.java

		String now=(new Date()).toString();
		ModelAndView modelAndView=new ModelAndView("hello","now",now);

$ vi HelloControllerTest.java

		HelloController helloController=new HelloController();
		ModelAndView modelAndView=helloController.handleRequest(null, null);
		Map<String, Object>model=modelAndView.getModel();
		Object value=model.get("now");

/ Nu	,


/ Lees	,
http://stackoverflow.com/questions/262367/type-safety-unchecked-cast

/ we maken voorlopig een test, met een product manager, die nog geen products heeft	,

$ vi InventoryController.java

public class InventoryController implements Controller {
	
	private ProductManager productManager;

	public ModelAndView handleRequest(HttpServletRequest arg0,HttpServletResponse arg1) throws Exception {
		Date now=new Date();
		List<Product>products=productManager.getProducts();
		Map<String,Object>map=new HashMap<String, Object>();
		map.put("now", now);
		map.put("products", products);
		
		ModelAndView modelAndView=new ModelAndView("inventory","map",map);
		return modelAndView;
	}

	public ProductManager getProductManager() {
		return productManager;
	}

	public void setProductManager(ProductManager productManager) {
		this.productManager = productManager;
	}

}

$ vi InventoryController.java

public class InventoryControllerTest {
	
	private Controller inventoryController;
	private ProductManager productManager;

	@Before
	public void setup(){
		inventoryController=new InventoryController();
		productManager=new ProductManagerImpl();
		((InventoryController)inventoryController).setProductManager(productManager);
	}
	
	@Test
	public void testInventory() throws Exception{
		ModelAndView modelAndView=inventoryController.handleRequest(null, null);
		Assert.assertEquals("inventory", modelAndView.getViewName());
		Map<String, Object>model=modelAndView.getModel();
		Map<?,?>map=(Map<?,?>)model.get("map");
		Assert.assertNotNull(model);
		Object now=map.get("now");
		Assert.assertThat(now,IsInstanceOf.instanceOf(Date.class) );
		Object products=map.get("products");
		Assert.assertNull(products);
	}

}

/ 7	.

/ Lees	,
http://programmers.stackexchange.com/questions/151527/how-to-unit-test-a-jsp-file

http://stackoverflow.com/questions/7260426/choosing-a-test-framework-for-a-jsf-based-webapp

/ Selenium

/ Toggle breakpoint op 1ste line in table-view	,
/ step met blauwe pijl naar beneden	,
/ de current line die hij zal gaan doen in geel	,
/ record in HTML, format naar JUnit	, en save	,
/ we save ze naar ~/Devel/Test/Selenium	,

/ ga in src/test/java op de package staan, en import	,

/ Voor XPath	, lees	,
http://www.w3schools.com/xpath/
https://developer.mozilla.org/en-US/docs/Web/XPath/Functions/contains

/ 7	. 

/ form	,

/ proj. spring-mvn-step-by-step2	,

/ ch4	,

/ DEBUG TOMCAT APP	,

[eric@almond apache-tomcat-7.0.62]$ bin/catalina.sh jpda run

/ en maak in eclipse debug config	, remote debug op 8000	,
project: spring-mvc-step-by-step2
host: localhost
port: 8000

$ vi /WEB-INF/web.xml

 <jsp-config>
  	<taglib>
  		<taglib-uri>/spring</taglib-uri>
  		<taglib-location>/WEB-INF/tld/spring-form.tld</taglib-location>
  	</taglib>
  </jsp-config>

$ /WEB-INF/jsp/priceincrease.jsp

<head>
  <title><fmt:message key="title"/></title>
</head>

<h1><fmt:message key="priceincrease.heading"/></h1>

<form:form method="post" commandName="priceIncrease">
	<form:input path="percentage"/>
	<input type="submit" value="Execute">
</form:form>

<a href="<c:url value="hello.htm"/>">Home</a>

$ vi src/main/java/my.own.spring-mvc-step-by-step.PriceIncrease.java

public class PriceIncrease {
	private int percentage;
// getter/setter	,

$ vi /WEB-INF/inventoryapp-servlet.xml

       <bean name="/priceincrease.htm" class="my.own.spring_mvc_step_by_step.PriceIncreaseFormController">
       		<property name="sessionForm" value="true"></property>
       		<property name="commandName" value="priceIncrease"></property>
       		<property name="commandClass" value="my.own.spring_mvc_step_by_step.PriceIncrease"></property>
       		<property name="formView" value="priceincrease"></property>
       		<property name="successView" value="inventory.htm"></property>
       		<property name="productManager" ref="productManager"></property>
       </bean>

$ vi src/main/java/my.own.spring-mvc-step-by-step.PriceIncreaseFormController.java

	private ProductManager productManager;
/ + getter/setter
	
	@Override
	protected ModelAndView onSubmit(Object command) throws Exception {
		int percentage=((PriceIncrease)command).getPercentage();
		productManager.increasePrice(percentage);
		String url=getSuccessView();
		RedirectView redirectView=new RedirectView(url);
		return new ModelAndView(redirectView);
	}		
		
/ url=="inventory.htm"

	@Override
	protected Object formBackingObject(HttpServletRequest request)
			throws Exception {
		PriceIncrease priceIncrease=new PriceIncrease();
		priceIncrease.setPercentage(20);
		return priceIncrease;
		
	}

$ vi src/main/resources/messages.properties

priceincrease.heading=Price Increase :: SpringApp
error.not-specified=Percentage not specified.
error.to-low=You have to specify a percentage higher that {0}%.
error.to-high=You have to specify a percentage lower that {0}%.
required=Required.
typemismatch=Invalid data.
typemismatch.percentage=That is not a number.

$ vi /WEB-INF/jsp/inventory.jsp
	...
   <br>
    <a href="<c:url value="priceincrease.htm"/>">Increase Prices</a>
    <br>







/ 7	. 

/ proj. spring-mvn-step-by-step2	,

/ Lees	,
http://docs.spring.io/docs/Spring-MVC-step-by-step/part5.html

/ ch5

/ create product dao	,
/ test 
/ de test class AbstractTransactionalDataSourceSpringContextTests is er in spring 4 niet meer	, hij rollbacks wat hij deed op de bestaande database	,

/ Ga naar	,
https://jdbc.postgresql.org/download.html
/ click 'Maven repository'	,
http://mvnrepository.com/artifact/org.postgresql
/ of naar search.maven.org	,

$ vi pom.xml

  	<postgresql.version>9.4-1201-jdbc41</postgresql.version>

	<dependency>
	    <groupId>org.postgresql</groupId>
	    <artifactId>postgresql</artifactId>
	    <version>${postgresql.version}</version>
	</dependency>

/ Intermezzo

/ unit test postgres	,

/ Lees over een embedded postgresql	,
http://stackoverflow.com/questions/14314026/embedded-postgresql
https://github.com/yandex-qatools/postgresql-embedded

I suggest that you check out two other databases: 
1. SQLite 
2. Apache Derby 

/ Einde Intermezzo

/ Intermezzo

/ Debug stmts postgres driver	,

loglevel = int
    Set the amount of logging information printed to the DriverManager's current value for LogStream or LogWriter. It currently supports values of org.postgresql.Driver.DEBUG (2) and org.postgresql.Driver.INFO (1). INFO will log very little information while DEBUG will produce significant detail. This property is only really useful if you are a developer or are having problems with the driver. 

/ Einde Intermezzo

/ Heeft mvn zoiets als een ant task <sql/>	?
/ See sql-maven-plugin	,
/ Lees	,
http://www.mojohaus.org/
/ click plugins	,
http://www.mojohaus.org/plugins.html
/ we zien	,
http://www.mojohaus.org/sql-maven-plugin/
/ TODO

/ we doen met de hand	,

[eric@almond repository]$ PGPASSWORD=foo psql -U foo -h localhost

CREATE TABLE products (
  id INTEGER NOT NULL PRIMARY KEY,
  description varchar(255),
  price decimal(15,2)
);
CREATE INDEX products_description ON products(description);

INSERT INTO products (id, description, price) values(1, 'Lamp', 5.78);
INSERT INTO products (id, description, price) values(2, 'Table', 75.29);
INSERT INTO products (id, description, price) values(3, 'Chair', 22.81);

foo=> \d products
             Table "public.products"
   Column    |          Type          | Modifiers 
-------------+------------------------+-----------
 id          | integer                | not null
 description | character varying(255) | 
 price       | numeric(15,2)          | 
Indexes:
    "products_pkey" PRIMARY KEY, btree (id)
    "products_description" btree (description)

/ 7	. 

 we zien	,
    java.lang.Object
        org.springframework.dao.support.DaoSupport
            org.springframework.jdbc.core.support.JdbcDaoSupport
                org.springframework.jdbc.core.simple.SimpleJdbcDaoSupport

/ we lezen in,	
/ dat we deze NIET moeten use	,
    java.lang.Object
        org.springframework.dao.support.DaoSupport
            org.springframework.orm.hibernate4.support.HibernateDaoSupport
NOTE: Hibernate access code can also be coded in plain Hibernate style. Hence, for newly started projects, consider adopting the standard Hibernate style of coding data access objects instead, based on SessionFactory.getCurrentSession(). This HibernateTemplate primarily exists as a migration helper for Hibernate 3 based data access code, to benefit from bug fixes in Hibernate 4.x.

/ 7	. 

/ geef in google	, 
maven simplejdbcdaotemplate
/ we moeten	,
	<dependency>
		<groupId>org.springframework</groupId>
		<artifactId>spring-jdbc</artifactId>
		<version>${spring.version}</version>
	</dependency>

/ 7	.

/ Al onze types zijn deprecated	,

	private class ProductMapper  implements ParameterizedRowMapper<Product>{

/ parametrized slaat op dat we <Product> geven	,
/ maar we moeten RowMapper use	,
/ TODO

/ MapSqlParameterSource slaat op dat we in de query description=:description ipv description=?	,
/ Deze is NIET deprecated, en is bedoeld voor een NamedParameterJdbcTemplate

/ , NamedParameterJdbcTemplate, of JdbcTemplate komt ipv SimpleJdbcTemplate	,

/ 7	. 

/ we gaan tests maken	,

$ vi pom.xml

	<dependency>
		<groupId>org.springframework</groupId>
		<artifactId>spring-test</artifactId>
		<version>${spring.version}</version>
	</dependency>

/ lees	,
http://docs.spring.io/spring/docs/current/spring-framework-reference/html/testing.html#integration-testing-support-jdbc
/ lees	,
http://blog.trifork.com/2009/07/01/testing-the-database-layer/
/ lees	,
http://java.dzone.com/articles/moving-springs-xml-annotations

 7	. 

/ Kijk in api van spring 3.2.7	,
http://docs.spring.io/spring/docs/3.2.7.RELEASE/javadoc-api/index.html?overview-summary.html

/ In 3.2.7.RELEASE zijn al de oude class types er nog , en zijn deprecated	, 
/ we use daarom 3.2.7 ipv. 2.5.6.SEC03	,


/ 13	. 

$ vi AbstractTransactionalDataSourceSpringContextTests...

@Deprecated
public abstract class AbstractTransactionalDataSourceSpringContextTests
extends AbstractTransactionalSpringContextTests
Deprecated. 
as of Spring 3.0, in favor of using the listener-based test context framework (AbstractTransactionalJUnit4SpringContextTests)

Subclass of AbstractTransactionalSpringContextTests that adds some convenience functionality for JDBC access. Expects a DataSource bean to be defined in the Spring application context.

This class exposes a JdbcTemplate and provides an easy way to delete from the database in a new transaction.


java.lang.Object
  extended by org.springframework.dao.support.DaoSupport
      extended by org.springframework.jdbc.core.support.JdbcDaoSupport
          extended by org.springframework.jdbc.core.simple.SimpleJdbcDaoSupport

$ vi JdbcDaoSupport...
Requires a DataSource to be set, providing a JdbcTemplate based on it to subclasses through the getJdbcTemplate() method. 

/ Dit doen we ook	,




/ 7	.

/ we maken src/test/java/ProductDaoTest.java	, 
we rename tot ...Test.java	, anders wordt niet meegenomen	, maar misschien komt dat omdat we met JUnit4 werken	,
/ TODO
/ Ook is het zo, dat de test methods names met test moeten beginnen, anders wordt hij niet called	, dat is WH JUnit3	, 

/ we zien dat de ProductDaoTest OK	, 
/ WH doet AbstractTransactionalDataSourceSpringContextTests een transaction rollback	,
/ want in de db zien we niets van de prijsverandering	, maar hij is wel gedaan:
$ vi ProductDaoTest.java

	public void testSaveProduct(){
		List<Product>products=productDao.getProductList();
		for(Product product:products){
			product.setPrice(200.12);
			productDao.saveProduct(product);
		}
		List<Product>products2=productDao.getProductList();
		for(Product product:products){
			Assert.assertEquals(200.12, product.getPrice(),0);
		}

	}
/ OK	, 

/ in src/test/java	,

$ vi ProductDaoTest.java

package my.own.spring_mvc_step_by_step;

import static org.junit.Assert.*;

import java.util.List;

import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;
import org.springframework.test.AbstractTransactionalDataSourceSpringContextTests;
import org.springframework.test.context.junit4.AbstractTransactionalJUnit4SpringContextTests;

public class ProductDaoTest extends
//		AbstractTransactionalJUnit4SpringContextTests {
	AbstractTransactionalDataSourceSpringContextTests{
	
	private ProductDao productDao;

	public ProductDao getProductDao() {
		return productDao;
	}

	public void setProductDao(ProductDao productDao) {
		this.productDao = productDao;
	}

	
	@Override
	protected String[] getConfigLocations() {
		return new String[]{"classpath:postgres_ds.xml"};
	}
	
	@Override
	protected void onSetUpInTransaction() throws Exception {
		super.deleteFromTables(new String[]{"products"});
		super.executeSqlScript("classpath:load_data.sql", true);
	}
	
	public void testGetProductList(){
		List<Product>products=productDao.getProductList();
		Assert.assertEquals(3, products.size());
	}
	
	public void testSaveProduct(){
		List<Product>products=productDao.getProductList();
		for(Product product:products){
			product.setPrice(200.12);
			productDao.saveProduct(product);
		}
		List<Product>products2=productDao.getProductList();
		for(Product product:products){
			Assert.assertEquals(200.13, product.getPrice(),0);
		}

	}
}

/ 13	, 

/ we MOETEN	,
		super.deleteFromTables(new String[]{"products"});
/ voor	,
		super.executeSqlScript("classpath:load_data.sql", true);
/ anders ERRs in de tests	 , 
/ TODO
Tests in error: 
  testSaveProduct(my.own.spring_mvc_step_by_step.ProductDaoTest): StatementCallback; uncategorized SQLException for SQL [select*from products]; SQL state [25P02]; error code [0]; ERROR: current transaction is aborted, commands ignored until end of transaction block; nested exception is org.postgresql.util.PSQLException: ERROR: current transaction is aborted, commands ignored until end of transaction block
  testGetProductList(my.own.spring_mvc_step_by_step.ProductDaoTest): ... // dezelfde	, 
/ TODO

/ 13	. 

/ in src/main/resources
$ vi postgres_ds.xml

<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:schemaLocation="http://www.springframework.org/schema/beans
       http://www.springframework.org/schema/beans/spring-beans-2.5.xsd">
 	
 	<bean id="dataSource" class="org.springframework.jdbc.datasource.DriverManagerDataSource">
 		<property name="driverClassName" value="org.postgresql.Driver"></property>
 		<property name="url" value="jdbc:postgresql://localhost/foo"></property>
 		<property name="username" value="foo"></property>
 		<property name="password" value="foo"></property>
 	</bean>
 	
 	<bean id="productDao" class="my.own.spring_mvc_step_by_step.ProductDaoImpl">
 		<property name="dataSource" ref="dataSource"></property>
 	</bean>
 	
 	<bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager">
 		<property name="dataSource" ref="dataSource"></property>
 	</bean>
       
</beans>

/ 13	. 

/ Met de volgende spring context config file hebben we met de dao-tests niets te maken,  
/ maar juist om de web controller te test	,  je inj een mock product manager (mock service laag)	,
/ onze postgres_dao.xml is ook een spring context config file	?
/ TODO
 
/ in src/main/webapp/WEB-INF/	,
$ vi inventoryapp-servlet.xml

<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:schemaLocation="http://www.springframework.org/schema/beans
       http://www.springframework.org/schema/beans/spring-beans-2.5.xsd">
       
       <bean name="/inventory.htm" class="my.own.spring_mvc_step_by_step.InventoryController">  
       		<property name="productManager" ref="productManager"></property>    
       </bean>
       
       <bean id="viewResolver" class="org.springframework.web.servlet.view.InternalResourceViewResolver">
       		<property name="viewClass" value="org.springframework.web.servlet.view.JstlView"></property>
       		<property name="prefix" value="/WEB-INF/jsp/"></property>
       		<property name="suffix" value=".jsp"></property>
       </bean>
       
       <bean id="messageSource" class="org.springframework.context.support.ResourceBundleMessageSource">
       		<property name="basename" value="messages"></property>
       </bean>
       
       <bean id="productManager" class="my.own.spring_mvc_step_by_step.ProductManagerImpl">
       		<property name="products">
       			<list>
       				<ref bean="product1"></ref>
       				<ref bean="product2"></ref>
       				<ref bean="product3"></ref>
       			</list>
       		</property>	
       </bean>
       
       <bean id="product1" class="my.own.spring_mvc_step_by_step.Product">
       		<property name="description" value="Lamp"></property>
       		<property name="price" value="5.75"></property>
       </bean>
        <bean id="product2" class="my.own.spring_mvc_step_by_step.Product">
       		<property name="description" value="Table"></property>
       		<property name="price" value="75.25"></property>
       </bean>
        <bean id="product3" class="my.own.spring_mvc_step_by_step.Product">
       		<property name="description" value="Chair"></property>
       		<property name="price" value="22.79"></property>
       </bean>
       
</beans>

/ 13	. 

/ Als we unit test	, dan is target/classes/WEB-INF/ er NIET	,
/ dus postgres_ds.xml moet in src/main/resources	, of beter in src/test/resources	,  want we doen hier alleen dao test	,
/ Hetzelfde voor load_data.sql: eig. in src/test/resources	,

./target/test-classes/my/own/spring_mvc_step_by_step/ProductDaoTest.class
./target/test-classes/postgres_ds.xml
./target/classes/my/own/spring_mvc_step_by_step/ProductDaoImpl.class
./target/classes/load_data.sql

/ 13	. 

/ we kunnen ook de test na deploy, een integration-test	, 
/ (we dachten postgres_ds.xml naast inventoryapp-servlet.xml te set	, omdat het ook een spring context file is	, maar dan wordt hij NIET gevonden met	,
	@Override
	protected String[] getConfigLocations() {
		return new String[]{"classpath:/WEB-INF/postgres_ds.xml"};
	}
/ Hij is er wel	,
target/spring-mvc-step-by-step2/WEB-INF/postgres_ds.xml
/ TODO

/ Misschien kunnen we de maven-surefile-plugin config	, 
/ TODO


$ vi pom.xml

		<plugin>
			<groupId>org.apache.maven.plugins</groupId>
			<artifactId>maven-surefire-plugin</artifactId>
			<version>${surefire.plugin.version}</version>
			<configuration>
				<skip>true</skip>
			</configuration>
			<executions>
				<execution>
					<phase>integration-test</phase>
					<goals>
						<goal>test</goal>
					</goals>
					<configuration>
						<skip>false</skip>
					</configuration>
				</execution>
			</executions>
		</plugin>

$ mvn tomcat7:undeploy clean tomcat7:deploy integration-test

/ de unit test runs na deploy 	,

/ 13	. 

/ we maken een test	,
	public void testSystemProperties(){
		String classpath=System.getProperty("java.class.path");
		System.out.println("CLASSPATH: "+classpath);
	}
/ we zien	,

CLASSPATH:
 /home/eric/Devel/Eclipse/sts-bundle/workspace/spring-mvc-step-by-step2/target/test-classes:
/home/eric/Devel/Eclipse/sts-bundle/workspace/spring-mvc-step-by-step2/target/classes:
/home/eric/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:
...


/ 7. 

/ we copy het project spring-mvc-step-by-step2 naar spring-mvc-step-by-step3	, 

/ ch6	,

http://docs.spring.io/docs/Spring-MVC-step-by-step/part6.html

/ de product manager impl (service laag) is nu nog een mock obj, want de list van products zijn def in de 
/ nu echt maken met een product dao	,

/ we veranderen	,

/ van setProducts was het arg List<Product> 	, nu ProductDao	,
/ de list kwam uit de spring context (see inventoryapp-servlet.xml)	,

$ vi ProductManager.java

public interface ProductManager {
	void increasePrice(int percentage);
	List<Product>getProducts();
	void setProductDao(ProductDao productDao);
}

public class ProductManagerImpl implements ProductManager {

//	private List<Product>products;
	private ProductDao productDao;
	
	public void increasePrice(int percentage) {
//
		List<Product>products=getProducts();
		if(products!=null){
			for (Product product:products){
				product.setPrice(product.getPrice()*(100+percentage)/100);
				productDao.saveProduct(product);
				
			}
		}

	}

	public List<Product> getProducts() {
//		return products;
		return productDao.getProductList();
	}

//	public void setProducts(List<Product> products) {
//		this.products=products;
//	}
	public void setProductDao(ProductDao productDao){
		this.productDao=productDao;
	}
}

/ De ProductManagerTest is nu ERR	, want daar staat nog List<Product>	,

$ vi ProductDao.java

public interface ProductDao {
	List<Product>getProductList();
	void saveProduct(Product product);
	
}

/ Intermezzo

/ we zien dat NU nog de product manager een list insert krijgt	, maar de .setProductList() in de product manager is verdwenen, dus deze config doet NIETS meer	, 

$ vi inventoryapp-servlet.xml

      <bean name="/inventory.htm" class="my.own.spring_mvc_step_by_step.InventoryController">  
       		<property name="productManager" ref="productManager"></property>    
       </bean>
      <bean id="productManager" class="my.own.spring_mvc_step_by_step.ProductManagerImpl">
       		<property name="products">
       			<list>
       				<ref bean="product1"></ref>
       				<ref bean="product2"></ref>
       				<ref bean="product3"></ref>
       			</list>
       		</property>	
       </bean>
       
       <bean id="product1" class="my.own.spring_mvc_step_by_step.Product">
       		<property name="description" value="Lamp"></property>
       		<property name="price" value="5.75"></property>
       </bean>
        <bean id="product2" class="my.own.spring_mvc_step_by_step.Product">
       		<property name="description" value="Table"></property>
       		<property name="price" value="75.25"></property>
       </bean>
        <bean id="product3" class="my.own.spring_mvc_step_by_step.Product">
       		<property name="description" value="Chair"></property>
       		<property name="price" value="22.79"></property>
       </bean>

/ Dit is de product dao impl,	 maar we mock hem in de test, dus voor de test speelt deze geen rol	,
/ hij is onveranderd uit ch5	,

$ vi ProductDaoImpl.java

public class ProductDaoImpl extends SimpleJdbcDaoSupport implements ProductDao {

	public List<Product> getProductList() {
		SimpleJdbcTemplate simpleJdbcTemplate=getSimpleJdbcTemplate();
		String query="select*from products";
		ParameterizedRowMapper<Product>parameterizedRowMapper=new ProductMapper();
		List<Product>products=simpleJdbcTemplate.query(query,parameterizedRowMapper);
		return products;		
	}

	public void saveProduct(Product product) {
		String query="update products set description=:description,price=:price where id=:id";
		
		SimpleJdbcTemplate  simpleJdbcTemplate=getSimpleJdbcTemplate();
		MapSqlParameterSource mapSqlParameterSource=new MapSqlParameterSource();
		mapSqlParameterSource.addValue("description", product.getDescription()).addValue("price", product.getPrice()).addValue("id", product.getId());
		simpleJdbcTemplate.update(query, mapSqlParameterSource);
		
	}
	
	private class ProductMapper  implements ParameterizedRowMapper<Product>{

		public Product mapRow(ResultSet rs, int rowNum) throws SQLException {
			Product product=new Product();
			product.setId(rs.getInt("id"));
			product.setDescription(rs.getString("description"));
			product.setPrice(rs.getDouble("price"));
			return product;
			
		}
		
	}
}

/ Einde Intermezzo

$ vi MockProductDaoImpl.java

public class MockProductDaoImpl implements ProductDao {
	
	private List<Product>products;

	public List<Product> getProductList() {
		return products;
	}

	public void saveProduct(Product product) {
		// TODO Auto-generated method stub
		
	}

	public void setProductList(List<Product> products) {
		this.products=products;
		
	}

/ .setProductList is typisch een MockProductDaoImpl method	, de ProductDaoImpl heeft deze niet	,

/////////////////////////////////////////////////////////////////////
/ De mock product dao impl krijgt een list van products	, 
/ dat hadden we al gezien in ch3/4, toen we zeiden  dat de toen product manager impl een mock was	, 
/ Nu  hebben we een product manager, met een mock product dao	, maar het komt op hetzelfde neer: er is een list van products,	 

$ vi InventoryControllerTest.java

public class InventoryControllerTest {
	
	private Controller inventoryController;
	private ProductManager productManager;

	@Before
	public void setup(){
		inventoryController=new InventoryController();
		productManager=new ProductManagerImpl();
		ProductDao mock=new MockProductDaoImpl();
		((MockProductDaoImpl)mock).setProductList(new ArrayList<Product>());
		productManager.setProductDao(mock);
		((InventoryController)inventoryController).setProductManager(productManager);
	}
	
	@Test
	public void testInventory() throws Exception{
		ModelAndView modelAndView=inventoryController.handleRequest(null, null);
		Assert.assertEquals("inventory", modelAndView.getViewName());
		Map<String, Object>model=modelAndView.getModel();
		Map<?,?>map=(Map<?,?>)model.get("map");
		Assert.assertNotNull(model);
		Object now=map.get("now");
		Assert.assertThat(now,IsInstanceOf.instanceOf(Date.class) );
		Object products=map.get("products");
		Assert.assertTrue(((List<Product>)products).isEmpty());
	}

	private class MyIsInstanceof extends ArgumentMatcher<Object>{

		@Override
		public boolean matches(Object argument) {
			return argument instanceof String;
		}
		
	}

/ In inventoryapp-servlet.xml krijgt de product manager een list van products inj	, maar de setter is er niet	, dus gebeurt er niets	, en we set in de test de dao in de product manager,  een mock dao	,

$ vi ProductManagerTest.java

public class ProductManagerTest {

	private ProductManager productManager;
	private List<Product>products;
	
	private final static int productCount=2;
	private final static double chairPrice=20.50;
	private final static String chairDescription="Chair";
	private final static double tablePrice=105.10;
	private final static String tableDescription="Table";
	private final static int priceIncrease=10;
	
	
	@Before
	public void setUp() throws Exception {
		
		productManager=new ProductManagerImpl();
		products=new ArrayList<Product>();
		
		// stub a list of products
		Product product=new Product();;
		product.setDescription(chairDescription);
		product.setPrice(chairPrice);
		products.add(product);
		
		product=new Product();
		product.setDescription(tableDescription);
		product.setPrice(tablePrice);
		products.add(product);
		
		ProductDao mock=new MockProductDaoImpl();
		((MockProductDaoImpl)mock).setProductList(products);
		productManager.setProductDao(mock);
	}

	@Test
	public void testWithNullList(){
		productManager=new ProductManagerImpl();
		ProductDao mock=new MockProductDaoImpl();
		((MockProductDaoImpl)mock).setProductList(null);
		productManager.setProductDao(mock);
		Assert.assertNull(productManager.getProducts());
	}
	@Test
	public void testIncreasePriceWithNullList(){
		productManager=new ProductManagerImpl();
		ProductDao mock=new MockProductDaoImpl();
		((MockProductDaoImpl)mock).setProductList(null);
		productManager.setProductDao(mock);
		productManager.increasePrice(priceIncrease);
		
	}
	
	@Test
	public void testWithEmptyList(){
		productManager=new ProductManagerImpl();
		ProductDao mock=new MockProductDaoImpl();
		((MockProductDaoImpl)mock).setProductList(new ArrayList<Product>());
		productManager.setProductDao(mock);
		productManager.increasePrice(priceIncrease);
		
	}
	
	@Test
	public void testWithList(){
		productManager.increasePrice(10);
		List<Product>products=productManager.getProducts();
		
		for (Product product:products){
			double increase=product.getPrice();
			if(product.getDescription()=="Chair"){
				Assert.assertEquals(22.55, product.getPrice(),0);
			}else if(product.getDescription()=="Table"){
				Assert.assertEquals(115.61, product.getPrice(),0);
			}
		}
	}
	

}

$ vi ProductDaoTest.java
/ Kunnen we nu overslaan: hebben we al gedaan	, tests de echte product dao	, 
/ de product manager hierboven kreeg een mock dao inj	, want we willen de manager (service ) test, niet de dao	,

/ 7	.

/ ch6

/ SPRING WEBAPP WITH XML

/ final spring-mvc-step-by-step3 is OK , 

/ 13	. 

/ als we spring-mvc-step-by-step3 deploy	, dan ERR	, want 


$ vi inventory.jsp
<c:forEach items="${map.products }" var="product">

$ vi inventoryapp-servlet.xml

      <bean id="productManager" class="my.own.spring_mvc_step_by_step.ProductManagerImpl">
       		<property name="products">
       			<list>
       				<ref bean="product1"></ref>
       				<ref bean="product2"></ref>
       				<ref bean="product3"></ref>
       			</list>
       		</property>	
       </bean>
       
       <bean id="product1" class="my.own.spring_mvc_step_by_step.Product">
       		<property name="description" value="Lamp"></property>
       		<property name="price" value="5.75"></property>
       </bean>
 		...
/ NB. Deze product list gaan we RM uit XML	, 

/ en in productManager moet dan een setter staan, maar die is er niet meer	,

/ De mock product manager impl heeft een .setProductList method	, die we steeds call in de tests	, waardoor productManager.getProducts() OK	,
/ TODO

$ vi ProductManagerImpl.java 

public class ProductManagerImpl implements ProductManager {

//	private List<Product>products;
	private ProductDao productDao;
	
	public List<Product> getProducts() {
//		return products;
		return productDao.getProductList();
	}

//	public void setProducts(List<Product> products) {
//		this.products=products;
//	}	
	public void setProductDao(ProductDao productDao){
		this.productDao=productDao;
	}
}

/ dus we moeten de inventoryapp-servlet.xml aanpassen	,
/ we kunnen ook de product manager en product dao config in aparte spring config files doen	, 

$ vi /WEB-INF/web.xml

 <listener>
  	<listener-class>org.springframework.web.context.ContextLoaderListener</listener-class>
  </listener>

 <context-param>
  	<param-name>contextConfigLocation</param-name>
  	<param-value>
  		/WEB-INF/inventoryapp-service.xml
  		/WEB-INF/inventoryapp-dao.xml
  	</param-value>
  </context-param>

$ vi /WEB-INF/inventoryapp-servlet.xml

/ RM
    <bean id="productManager" class="my.own.spring_mvc_step_by_step.ProductManagerImpl">
       		<property name="products">
       			<list>
       				<ref bean="product1"></ref>
       				<ref bean="product2"></ref>
       				<ref bean="product3"></ref>
       			</list>
       		</property>	
       </bean>
       
       <bean id="product1" class="my.own.spring_mvc_step_by_step.Product">
       		<property name="description" value="Lamp"></property>
       		<property name="price" value="5.75"></property>
       </bean>
		...
/ Einde RM

$ vi inventoryapp-service.xml

       <bean id="productManager" class="my.own.spring_mvc_step_by_step.ProductManagerImpl">
      		<property name="productDao" ref="productDao"></property>
       </bean>

$ vi inventoryapp-dao.xml

 <bean id="productDao" class="my.own.spring_mvc_step_by_step.ProductDaoImpl">
  	<property name="dataSource" ref="dataSource"></property>
  </bean>

/ NB 
/ we hadden in src/main/resources de datasource al config,	 maar dat was voor de product dao test	,
	@Override
	protected String[] getConfigLocations() {
		return new String[]{"classpath:postgres_ds.xml"};
	}

 <bean id="dataSource" class="org.springframework.jdbc.datasource.DriverManagerDataSource" >
   		<property name="driverClassName" value="org.postgresql.Driver"></property>
 		<property name="url" value="jdbc:postgresql://localhost/foo"></property>
 		<property name="username" value="foo"></property>
 		<property name="password" value="foo"></property>
  
  </bean>

/ Einde SPRING MVC XML

/ SPRING 

/ 7	.

/ Lees	,

http://docs.spring.io/spring-security/site/docs/3.2.x/guides/hellomvc.html
http://docs.spring.io/spring/docs/current/spring-framework-reference/html/mvc.html
http://docs.spring.io/spring-webflow/docs/2.4.0.RELEASE/reference/html/

http://docs.spring.io/spring-boot/docs/current/reference/html/getting-started-first-application.html

/ Einde SPRING

/ SPRING MVC ANNOTATIONS

/ Lees	,
docs.spring.io
/ Kies
Spring Framework
/ Kies Ch 17	,

/ Lees par 5.9	 Annotations	,


/ TODO


/ Einde SPRING MVC ANNOTATIONS

/ SPRING BOOT NOTES 

/ Lees	,

/ we kiezen bij Guides	, helemaal onderin	,
Building REST services with Spring

/ we zien een video	,
https://www.youtube.com/watch?v=p8AdyMlpmPk / spring boot video van de spring boot site	,
/ en bij suggestions	, 
https://www.youtube.com/watch?v=oG2rotiGr90 / rest
https://www.youtube.com/watch?v=47xNBNd-LLI / 6 min spring-boot
https://www.youtube.com/watch?v=zbeMDM-zDNI / rest

https://www.youtube.com/watch?v=jIae_pcG-9M /data
https://www.youtube.com/watch?v=Q6esdIY66rw  /data

http://www.mkyong.com/webservices/jax-rs/json-example-with-jersey-jackson/

https://www.youtube.com/watch?v=sbPSjI4tt10 / boot

https://github.com/nebhale/spring-one-2013 / src code rest video	,

/ Lees over spring test	,
http://www.petrikainulainen.net/spring-mvc-test-tutorial/

/ jackson is java -> json	,

/ video	josh	,
/ 30:17	mvc controller vs. data-rest controller

/ Einde SPRING BOOT NOTES

/ SPRING BOOT VIDEO DOCS

/ 7	. 

https://www.youtube.com/watch?v=47xNBNd-LLI

/ click 'New Spring Starter project
Name: spring-boot-first
/ TODO
group: org.my.own
artifact: spring-boot-first
Package: demo
/ De Java package waar bijv SpringBootApplication in is	,
/ check Actuator, Web	,
Next	,
Base url: http://start.spring.io/starter.zip
Full url:http://start.spring.io/starter.zip?name=spring-boot-first&groupId=org.my.own&artifactId=spring-boot-first&version=0.0.1-SNAPSHOT&description=Demo+project+for+Spring+Boot&packageName=demo&type=maven-project&packaging=jar&javaVersion=1.8&language=java&bootVersion=1.2.4.RELEASE&dependencies=actuator&dependencies=web

/ we hadden per ongeluk 
Type: Gradle Project,
/ we zien	,
Can not use Gradle: STS Gradle Tooling is not installed. You can install it from the STS Dashboard.
/ TODO

/ we zien in de effective pom	, rest	,

    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-data-rest</artifactId>
        <version>1.2.4.RELEASE</version>
      </dependency>

     <dependency>
        <groupId>org.springframework.data</groupId>
        <artifactId>spring-data-rest-webmvc</artifactId>
        <version>2.2.2.RELEASE</version>
      </dependency>

     <dependency>
        <groupId>org.springframework.data</groupId>
        <artifactId>spring-data-rest-core</artifactId>
        <version>2.2.2.RELEASE</version>
      </dependency>


/ right click project 'spring-boot-first'	, 
Run As -> Spring Boot App
/ Geef in chrome	, 
localhost:8080
/we zien	,
Whitelabel Error Page

This application has no explicit mapping for /error, so you are seeing this as a fallback.
Mon Jun 29 20:52:02 CEST 2015
There was an unexpected error (type=Not Found, status=404).
No message available

$ vi src/main/java/demo/SpringBootFirstApplication.java

@SpringBootApplication
public class SpringBootFirstApplication {

    public static void main(String[] args) {
        SpringApplication.run(SpringBootFirstApplication.class, args);
    }
}

$ vi src/main/java/demo/Greeting.java

@Controller
public class Greeting {
	
	@RequestMapping("/greet")
	@ResponseBody
	public String greet(){
		return "Foo Bar";
	}
}

/ click in eclipse Relaunching spring boot first - ...
/ Geef localhost:8080/greet	,

/ Geef	,
http://localhost:8080/env
http://localhost:8080/health
http://localhost:8080/beans
...

/ 7	.

/ kijk	,
https://www.youtube.com/watch?v=p8AdyMlpmPk / spring boot video van de spring boot site	,

File , new spring starter proj
group: org.my.own
artifact: spring-boot-second
/ check Web	,

/ Lees	,
http://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-testing.html

/ De deps in onze POM is nu	,

	<dependencies>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-web</artifactId>
		</dependency>
		
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-test</artifactId>
			<scope>test</scope>
		</dependency>
	</dependencies>
	
/ Edit 	,

@Entity
class Booking{
}

/ we geven Enter achter @Entity	, en we krijgen keuze	,
How do you want to add Entity from javax.persistence to your classpath?
javax.persistence
spring.boot.starter.data.jpa
/ we kiezen spring.boot.starter.data.jpa	, net als in de video	, en eclipse gaat 'Building workspace'	, duurt even	, en alles wordt OK	,

/ we zien er een dep bij in de POM	,
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-data-jpa</artifactId>
		</dependency>
	</dependencies>

/ als we javax.persistence kiezen	, dan zien we in de POM	,
		<dependency>
			<groupId>org.eclipse.persistence</groupId>
			<artifactId>javax.persistence</artifactId>
			<version>2.0.0</version>
		</dependency>

/ TODO (Afmaken)


/ Einde SPRING BOOT VIDEO DOCS

/ SPRING BOOT VIDEO JOSH

/ 7	.

/ rest 

/ eerst met mvc-rest	, daarna data-rest	,

/ 28:41	, mvc-rest	,

/ add maven dep spring-boot-starter-web	,
/ dan	,

@RestController
class ReservationRestController 
	@RequestMapping("/reservations')
	Collection<Reservation>reservations(){
		return this.reservationRepository.findAll();
	}

	@Autowired
	private ReservationRepository reservationRepository;

/ 30:53, data-rest

/ add maven dep spring-boot-starter-data-rest	,
/ dan	,

@RepositoryRestResource
interface ReservationRepository extends JpaRepository<Reservation,Long>{
	Collection<Reservation>findByReservationName(@Param("rn")String rn);


/ Einde SPRING BOOT VIDEO JOSH

/ SPRING TEST

/ geef in firefox
wicket spring
wicket spring webapplicationcontext

/ Lees	,
http://blog.comsysto.com/2010/06/04/test-driven-development-with-apache-wicket-and-spring-framework/

/ Einde SPRING TEST

/ SPRING REST

/ See video	,
https://www.youtube.com/watch?v=oG2rotiGr90


/ Einde SPRING REST

/ SPRING DATA 

/ video	,
https://www.youtube.com/watch?v=jIae_pcG-9M
https://www.youtube.com/watch?v=Q6esdIY66rw

/ ga naar getting started guides	,
https://spring.io/guides#gs
/ we zien	,
https://spring.io/guides/gs/accessing-data-jpa/

/ 7	. 

/ in STS	,

/ doe	,
file , new , import spring getting started content
/ kies	,
Accessing Data JPA
/ we zien 2 proj.
gs-accessing-data-jpa-initial
/ TODO
gs-accessing-data-jpa-complete
/ Run As -> Spring boot	, 
/ OK

/ we kunnen ook 
Debug as -> Spring boot application	, 

/ We zien in run dat 	,
   @Autowired
    CustomerRepository repository;
/ een proxy is naar een org.springframework.data.jpa.repository.support.SimpleJpaRepository@43da0955	,

/ we lezen onderaan	,
http://spring.io/guides/gs/accessing-data-jpa/
/ de link naar	,
http://spring.io/guides/gs/accessing-data-rest/

/ op 
http://spring.io/guides/gs/accessing-data-rest/
/ we zien	,  hypermedia-based
http://spring.io/guides/gs/rest-hateoas/

/ lees	,
http://spring.io/guides/gs/consuming-rest-jquery/
/ we zien CORS	,
http://spring.io/guides/gs/rest-service-cors/

/ Lees	,
http://spring.io/guides/gs/rest-service/

/ Lees	,
http://spring.io/guides/gs/consuming-rest/

/ Lees	,
Building REST services with Spring
http://spring.io/guides/tutorials/bookmarks/

/ Lees	,
http://spring.io/guides/gs/accessing-data-jpa/
http://spring.io/guides/gs/accessing-data-rest/

/ 7	. 

/ we zijn in gs-accessing-data-rest-complete	,

/ In de effective pom zien we al de deps in 
<dependencyManagement>
	<dependencies>
		<dependency>
			..
		</ dependency>
	</dependencies>
</dependencyManagement>

/ maar dit zijn NIET de feitenlijke deps	, 
/ die zien we onder
<dependencies>
	<dependency>
			..
	</ dependency>
</dependencies>

/ Einde SPRING DATA

/ SPRING DATA JPA

/ Lees	,
http://spring.io/guides/gs/accessing-data-jpa/

/ we doen op de command line,	
$ mvn clean install

[eric@almond gs-accessing-data-jpa-complete]$ jar tvf  target/gs-accessing-data-jpa-0.1.0.jar 
/ we zien GEEN embedded tomcat	,

[eric@almond gs-accessing-data-jpa-complete]$ java -jar target/gs-accessing-data-jpa-0.1.0.jar 
Customers found with findAll():
-------------------------------
Customer[id=1, firstName='Jack', lastName='Bauer']
Customer[id=2, firstName='Chloe', lastName='O'Brian']
Customer[id=3, firstName='Kim', lastName='Bauer']
Customer[id=4, firstName='David', lastName='Palmer']
Customer[id=5, firstName='Michelle', lastName='Dessler']

Customer found with findOne(1L):
--------------------------------
Customer[id=1, firstName='Jack', lastName='Bauer']

Customer found with findByLastName('Bauer'):
--------------------------------------------
Customer[id=1, firstName='Jack', lastName='Bauer']
Customer[id=3, firstName='Kim', lastName='Bauer']

/ en returns	, 
/ bij accessing-data-rest returns hij niet, alleen als we ctrl-c geven	, dat komt door tomcat TODO







/ Einde SPRING DATA JPA

/ CAMEL

/ 7	.

*	FileEndpoint.createConsumer(Processor) line: 62	
	FileEndpoint.createConsumer(Processor) line: 36	
	EventDrivenConsumerRoute.addServices(List<Service>) line: 65	
	EventDrivenConsumerRoute(DefaultRoute).onStartingServices(List<Service>) line: 85	
->	RouteService.warmUp() line: 158	
	DefaultCamelContext.doWarmUpRoutes(Map<Integer,DefaultRouteStartupOrder>, boolean) line: 3090	
	DefaultCamelContext.safelyStartRouteServices(boolean, boolean, boolean, boolean, Collection<RouteService>) line: 3020	
	DefaultCamelContext.doStartOrResumeRoutes(Map<String,RouteService>, boolean, boolean, boolean, boolean) line: 2797	
	DefaultCamelContext.doStartCamel() line: 2653	
	DefaultCamelContext.access$000(DefaultCamelContext) line: 167	
	DefaultCamelContext$2.call() line: 2467	
	DefaultCamelContext$2.call() line: 2463	
	DefaultCamelContext.doWithDefinedClassLoader(Callable<T>) line: 2486	
	DefaultCamelContext.doStart() line: 2463	
	DefaultCamelContext(ServiceSupport).start() line: 61	
	DefaultCamelContext.start() line: 2432	
	FileCopierWithCamel.main(String[]) line: 38	

->
    public synchronized void warmUp() throws Exception {
			...
           for (Route route : routes) {
				...
                // callback that we are staring these services
                route.onStartingServices(services);

route	EventDrivenConsumerRoute  (id=1545)	
EventDrivenConsumerRoute[Endpoint[file://data/inbox?noop=true] -> Channel[sendTo(Endpoint[file://data/outbox])]]

*
    public FileConsumer createConsumer(Processor processor) throws Exception {
processor	CamelInternalProcessor  (id=1538)	
Channel[sendTo(Endpoint[file://data/outbox])]
/s
        return new FileConsumer(this, processor, operations);

/ 7. 

/ we geven continue	,

->	FileEndpoint.createProducer() line: 106	
	FileEndpoint.createProducer() line: 36	
	ProducerCache.doGetProducer(Endpoint, boolean) line: 402	
	ProducerCache.acquireProducer(Endpoint) line: 123	
	SendProcessor.doStart() line: 219	
	SendProcessor(ServiceSupport).start() line: 61	
	ServiceHelper.startService(Service) line: 74	
	ServiceHelper.startService(Object) line: 59	
	ServiceHelper.startServices(Collection<?>) line: 103	
	ServiceHelper.startServices(Object...) line: 89	
	InstrumentationProcessor(DelegateAsyncProcessor).doStart() line: 79	
	InstrumentationProcessor(ServiceSupport).start() line: 61	
	ServiceHelper.startService(Service) line: 74	
	ServiceHelper.startService(Object) line: 59	
	ServiceHelper.startServices(Collection<?>) line: 103	
	ServiceHelper.startServices(Object...) line: 89	
	DefaultErrorHandler(RedeliveryErrorHandler).doStart() line: 1272	
	DefaultErrorHandler(ChildServiceSupport).start(boolean) line: 44	
	DefaultErrorHandler(ChildServiceSupport).start() line: 31	
	ServiceHelper.startService(Service) line: 74	
	ServiceHelper.startService(Object) line: 59	
	ServiceHelper.startServices(Collection<?>) line: 103	
	ServiceHelper.startServices(Object...) line: 89	
	DefaultChannel.doStart() line: 155	
	DefaultChannel(ServiceSupport).start() line: 61	
	ServiceHelper.startService(Service) line: 74	
	ServiceHelper.startService(Object) line: 59	
	ServiceHelper.startServices(Collection<?>) line: 103	
	ServiceHelper.startServices(Object...) line: 89	
	CamelInternalProcessor(DelegateAsyncProcessor).doStart() line: 79	
	CamelInternalProcessor(ServiceSupport).start() line: 61	
	ServiceHelper.startService(Service) line: 74	
	RouteService.startChildService(Route, List<Service>) line: 340	
	RouteService.warmUp() line: 182	
	DefaultCamelContext.doWarmUpRoutes(Map<Integer,DefaultRouteStartupOrder>, boolean) line: 3090	
	DefaultCamelContext.safelyStartRouteServices(boolean, boolean, boolean, boolean, Collection<RouteService>) line: 3020	
	DefaultCamelContext.doStartOrResumeRoutes(Map<String,RouteService>, boolean, boolean, boolean, boolean) line: 2797	
	DefaultCamelContext.doStartCamel() line: 2653	
	DefaultCamelContext.access$000(DefaultCamelContext) line: 167	
	DefaultCamelContext$2.call() line: 2467	
	DefaultCamelContext$2.call() line: 2463	
	DefaultCamelContext.doWithDefinedClassLoader(Callable<T>) line: 2486	
	DefaultCamelContext.doStart() line: 2463	
	DefaultCamelContext(ServiceSupport).start() line: 61	
	DefaultCamelContext.start() line: 2432	
	FileCopierWithCamel.main(String[]) line: 38	

->
    public GenericFileProducer<File> createProducer() throws Exception {
		...
        return new GenericFileProducer<File>(this, operations);

/ 7	. 

/ we geven continue	,

->	DefaultCamelContext.safelyStartRouteServices(boolean, boolean, boolean, boolean, Collection<RouteService>) line: 3029	
	DefaultCamelContext.doStartOrResumeRoutes(Map<String,RouteService>, boolean, boolean, boolean, boolean) line: 2797	
	DefaultCamelContext.doStartCamel() line: 2653	
	DefaultCamelContext.access$000(DefaultCamelContext) line: 167	
	DefaultCamelContext$2.call() line: 2467	
	DefaultCamelContext$2.call() line: 2463	
	DefaultCamelContext.doWithDefinedClassLoader(Callable<T>) line: 2486	
	DefaultCamelContext.doStart() line: 2463	
	DefaultCamelContext(ServiceSupport).start() line: 61	
	DefaultCamelContext.start() line: 2432	
	FileCopierWithCamel.main(String[]) line: 38	

->
       if (startConsumer) {
            } else {
                // and now start the routes
                // and check for clash with multiple consumers of the same endpoints which is not allowed
                doStartRouteConsumers(inputs, addingRoutes);

/ 7	. 

/ we geven continue	,

Daemon Thread [Camel (camel-1) thread #0 - file://data/inbox] (Suspended (entry into method poll in GenericFileConsumer))	
->	FileConsumer(GenericFileConsumer<T>).poll() line: 93	
	FileConsumer(ScheduledPollConsumer).doRun() line: 174	
	FileConsumer(ScheduledPollConsumer).run() line: 101	
	Executors$RunnableAdapter<T>.call() line: 511	
	ScheduledThreadPoolExecutor$ScheduledFutureTask<V>(FutureTask<V>).runAndReset() line: 308	
	ScheduledThreadPoolExecutor$ScheduledFutureTask<V>.access$301(ScheduledThreadPoolExecutor$ScheduledFutureTask) line: 180	
	ScheduledThreadPoolExecutor$ScheduledFutureTask<V>.run() line: 294	
	RejectableScheduledThreadPoolExecutor(ThreadPoolExecutor).runWorker(ThreadPoolExecutor$Worker) line: 1142	
	ThreadPoolExecutor$Worker.run() line: 617	
	Thread.run() line: 745	

->
   protected int poll() throws Exception {
            limitHit = !pollDirectory(name, files, 0);
/s
        LinkedList<Exchange> exchanges = new LinkedList<Exchange>();
        for (GenericFile<T> file : files) {
            Exchange exchange = endpoint.createExchange(file);

/ 7	. 

/ we geven cont	, 

Daemon Thread [Camel (camel-1) thread #0 - file://data/inbox] (Suspended (entry into method process in GenericFileProducer))	
->	GenericFileProducer<T>.process(Exchange) line: 61	
	AsyncProcessorConverterHelper$ProcessorToAsyncProcessorBridge.process(Exchange, AsyncCallback) line: 61	
	SendProcessor.process(Exchange, AsyncCallback) line: 129	
	InstrumentationProcessor.process(Exchange, AsyncCallback) line: 77	
	DefaultErrorHandler(RedeliveryErrorHandler).process(Exchange, AsyncCallback) line: 448	
	DefaultChannel(CamelInternalProcessor).process(Exchange, AsyncCallback) line: 191	
	CamelInternalProcessor.process(Exchange, AsyncCallback) line: 191	
	FileConsumer(GenericFileConsumer<T>).processExchange(Exchange) line: 435	
	FileConsumer(GenericFileConsumer<T>).processBatch(Queue<Object>) line: 211	
	FileConsumer(GenericFileConsumer<T>).poll() line: 175	
	FileConsumer(ScheduledPollConsumer).doRun() line: 174	
	FileConsumer(ScheduledPollConsumer).run() line: 101	
	Executors$RunnableAdapter<T>.call() line: 511	
	ScheduledThreadPoolExecutor$ScheduledFutureTask<V>(FutureTask<V>).runAndReset() line: 308	
	ScheduledThreadPoolExecutor$ScheduledFutureTask<V>.access$301(ScheduledThreadPoolExecutor$ScheduledFutureTask) line: 180	
	ScheduledThreadPoolExecutor$ScheduledFutureTask<V>.run() line: 294	
	RejectableScheduledThreadPoolExecutor(ThreadPoolExecutor).runWorker(ThreadPoolExecutor$Worker) line: 1142	
	ThreadPoolExecutor$Worker.run() line: 617	
	Thread.run() line: 745	

->
   public void process(Exchange exchange) throws Exception {
        // store any existing file header which we want to keep and propagate
        final String existing = exchange.getIn().getHeader(Exchange.FILE_NAME, String.class);
		...
       try {
            processExchange(exchange, target);
/s
GenericFileProducer<T>.processExchange(Exchange, String) line: 165	
			...
           // write/upload the file
            writeFile(exchange, tempTarget != null ? tempTarget : target);





/ Einde CAMEL

/ SPRING BOOT CAMEL

/ 7	.

/ geef in google	,
camel 2.15.2 jaxbdataformat
/ Lees	,
http://stackoverflow.com/questions/31048389/no-adapter-for-endpoint-exception-apache-camel-with-spring-boot-spring-ws


/ Einde SPRING BOOT CAMEL 

/ WS

/ geef in google	,
java ws

/ Einde WS


/ SPRING DATA REST 

/ Lees	,
http://spring.io/guides/gs/accessing-data-rest/



/ 7	. 

/ we zijn in gs-accessing-data-rest-initial	,

/ right click proj	. Run as -> spring boot
/ ERR

/ we moeten eerst	, 
$ vi src/main/java/hello/Application.java

@SpringBootApplication
public class Application {

	public static void main(String[] args) {
		SpringApplication.run(Application.class, args);
	}
}

/ Dan OK	,

/ 7	. 

[eric@almond gs-accessing-data-rest-complete]$ java -jar target/gs-accessing-data-rest-0.1.0.jar 

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v1.2.4.RELEASE)

2015-07-05 13:49:36.219  INFO 14988 --- [           main] hello.Application                        : Starting Application v0.1.0 on almond.nuts.org with PID 14988 (/home/eric/Devel/Eclipse/sts-bundle/workspace/gs-accessing-data-rest-complete/target/gs-accessing-data-rest-0.1.0.jar started by eric in /home/eric/Devel/Eclipse/sts-bundle/workspace/gs-accessing-data-rest-complete)
2015-07-05 13:49:36.287  INFO 14988 --- [           main] ationConfigEmbeddedWebApplicationContext : Refreshing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5102df69: startup date [Sun Jul 05 13:49:36 CEST 2015]; root of context hierarchy
2015-07-05 13:49:37.922  INFO 14988 --- [           main] o.s.b.f.s.DefaultListableBeanFactory     : Overriding bean definition for bean 'beanNameViewResolver': replacing [Root bean: class [null]; scope=; abstract=false; lazyInit=false; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=org.springframework.boot.autoconfigure.web.ErrorMvcAutoConfiguration$WhitelabelErrorViewConfiguration; factoryMethodName=beanNameViewResolver; initMethodName=null; destroyMethodName=(inferred); defined in class path resource [org/springframework/boot/autoconfigure/web/ErrorMvcAutoConfiguration$WhitelabelErrorViewConfiguration.class]] with [Root bean: class [null]; scope=; abstract=false; lazyInit=false; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=org.springframework.boot.autoconfigure.web.WebMvcAutoConfiguration$WebMvcAutoConfigurationAdapter; factoryMethodName=beanNameViewResolver; initMethodName=null; destroyMethodName=(inferred); defined in class path resource [org/springframework/boot/autoconfigure/web/WebMvcAutoConfiguration$WebMvcAutoConfigurationAdapter.class]]
2015-07-05 13:49:39.643  INFO 14988 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [class org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$c0c234e1] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2015-07-05 13:49:39.704  INFO 14988 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'transactionAttributeSource' of type [class org.springframework.transaction.annotation.AnnotationTransactionAttributeSource] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2015-07-05 13:49:39.718  INFO 14988 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'transactionInterceptor' of type [class org.springframework.transaction.interceptor.TransactionInterceptor] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2015-07-05 13:49:39.731  INFO 14988 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.config.internalTransactionAdvisor' of type [class org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2015-07-05 13:49:40.735  INFO 14988 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat initialized with port(s): 8080 (http)
2015-07-05 13:49:41.361  INFO 14988 --- [           main] o.apache.catalina.core.StandardService   : Starting service Tomcat
2015-07-05 13:49:41.363  INFO 14988 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.0.23
2015-07-05 13:49:41.615  INFO 14988 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2015-07-05 13:49:41.616  INFO 14988 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 5332 ms
2015-07-05 13:49:44.146  INFO 14988 --- [ost-startStop-1] j.LocalContainerEntityManagerFactoryBean : Building JPA container EntityManagerFactory for persistence unit 'default'
2015-07-05 13:49:44.198  INFO 14988 --- [ost-startStop-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [
	name: default
	...]
2015-07-05 13:49:44.454  INFO 14988 --- [ost-startStop-1] org.hibernate.Version                    : HHH000412: Hibernate Core {4.3.10.Final}
2015-07-05 13:49:44.468  INFO 14988 --- [ost-startStop-1] org.hibernate.cfg.Environment            : HHH000206: hibernate.properties not found
2015-07-05 13:49:44.472  INFO 14988 --- [ost-startStop-1] org.hibernate.cfg.Environment            : HHH000021: Bytecode provider name : javassist
2015-07-05 13:49:45.037  INFO 14988 --- [ost-startStop-1] o.hibernate.annotations.common.Version   : HCANN000001: Hibernate Commons Annotations {4.0.5.Final}
2015-07-05 13:49:45.157  INFO 14988 --- [ost-startStop-1] org.hibernate.dialect.Dialect            : HHH000400: Using dialect: org.hibernate.dialect.H2Dialect
2015-07-05 13:49:45.321  INFO 14988 --- [ost-startStop-1] o.h.h.i.ast.ASTQueryTranslatorFactory    : HHH000397: Using ASTQueryTranslatorFactory
2015-07-05 13:49:45.791  INFO 14988 --- [ost-startStop-1] org.hibernate.tool.hbm2ddl.SchemaExport  : HHH000227: Running hbm2ddl schema export
2015-07-05 13:49:45.808  INFO 14988 --- [ost-startStop-1] org.hibernate.tool.hbm2ddl.SchemaExport  : HHH000230: Schema export complete
2015-07-05 13:49:46.736  INFO 14988 --- [ost-startStop-1] o.s.b.c.e.ServletRegistrationBean        : Mapping servlet: 'dispatcherServlet' to [/]
2015-07-05 13:49:46.751  INFO 14988 --- [ost-startStop-1] o.s.b.c.embedded.FilterRegistrationBean  : Mapping filter: 'characterEncodingFilter' to: [/*]
2015-07-05 13:49:46.752  INFO 14988 --- [ost-startStop-1] o.s.b.c.embedded.FilterRegistrationBean  : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]
2015-07-05 13:49:46.960  INFO 14988 --- [           main] o.s.b.f.config.PropertiesFactoryBean     : Loading properties file from class path resource [rest-default-messages.properties]
2015-07-05 13:49:47.341  INFO 14988 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5102df69: startup date [Sun Jul 05 13:49:36 CEST 2015]; root of context hierarchy
2015-07-05 13:49:47.424  INFO 14988 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error],methods=[],params=[],headers=[],consumes=[],produces=[text/html],custom=[]}" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest)
2015-07-05 13:49:47.424  INFO 14988 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error],methods=[],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest)
2015-07-05 13:49:47.482  INFO 14988 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
2015-07-05 13:49:47.482  INFO 14988 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
2015-07-05 13:49:47.646  INFO 14988 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
2015-07-05 13:49:48.132  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerAdapter   : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5102df69: startup date [Sun Jul 05 13:49:36 CEST 2015]; root of context hierarchy
2015-07-05 13:49:48.158  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}/{propertyId}],methods=[DELETE],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.deletePropertyReferenceId(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,java.lang.String,java.lang.String) throws java.lang.Exception
2015-07-05 13:49:48.159  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}/{propertyId}],methods=[GET],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.followPropertyReference(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,java.lang.String,java.lang.String,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws java.lang.Exception
2015-07-05 13:49:48.159  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}],methods=[GET],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.followPropertyReference(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,java.lang.String,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws java.lang.Exception
2015-07-05 13:49:48.159  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}],methods=[DELETE],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<? extends org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.deletePropertyReference(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,java.lang.String) throws java.lang.Exception
2015-07-05 13:49:48.160  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}],methods=[GET],params=[],headers=[],consumes=[],produces=[application/x-spring-data-compact+json || text/uri-list],custom=[]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.followPropertyReferenceCompact(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,java.lang.String,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws java.lang.Exception
2015-07-05 13:49:48.160  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}],methods=[PATCH || PUT],params=[],headers=[],consumes=[application/json || application/x-spring-data-compact+json || text/uri-list],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<? extends org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.createPropertyReference(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.http.HttpMethod,org.springframework.hateoas.Resources<java.lang.Object>,java.io.Serializable,java.lang.String) throws java.lang.Exception
2015-07-05 13:49:48.161  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/schema],methods=[GET],params=[],headers=[],consumes=[],produces=[application/schema+json],custom=[]}" onto public org.springframework.http.HttpEntity<org.springframework.data.rest.webmvc.json.JsonSchema> org.springframework.data.rest.webmvc.RepositorySchemaController.schema(org.springframework.data.rest.webmvc.RootResourceInformation)
2015-07-05 13:49:48.167  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[PUT],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<? extends org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryEntityController.putItemResource(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.PersistentEntityResource,java.io.Serializable,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws org.springframework.web.HttpRequestMethodNotSupportedException
2015-07-05 13:49:48.168  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[PATCH],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryEntityController.patchItemResource(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.PersistentEntityResource,java.io.Serializable,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws org.springframework.web.HttpRequestMethodNotSupportedException,org.springframework.data.rest.webmvc.ResourceNotFoundException
2015-07-05 13:49:48.168  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[DELETE],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryEntityController.deleteItemResource(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable) throws org.springframework.data.rest.webmvc.ResourceNotFoundException,org.springframework.web.HttpRequestMethodNotSupportedException
2015-07-05 13:49:48.168  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}],methods=[OPTIONS],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryEntityController.optionsForCollectionResource(org.springframework.data.rest.webmvc.RootResourceInformation)
2015-07-05 13:49:48.169  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}],methods=[HEAD],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryEntityController.headCollectionResource(org.springframework.data.rest.webmvc.RootResourceInformation) throws org.springframework.web.HttpRequestMethodNotSupportedException
2015-07-05 13:49:48.169  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}],methods=[GET],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.hateoas.Resources<?> org.springframework.data.rest.webmvc.RepositoryEntityController.getCollectionResource(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.support.DefaultedPageable,org.springframework.data.domain.Sort,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws org.springframework.data.rest.webmvc.ResourceNotFoundException,org.springframework.web.HttpRequestMethodNotSupportedException
2015-07-05 13:49:48.169  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}],methods=[GET],params=[],headers=[],consumes=[],produces=[application/x-spring-data-compact+json || text/uri-list],custom=[]}" onto public org.springframework.hateoas.Resources<?> org.springframework.data.rest.webmvc.RepositoryEntityController.getCollectionResourceCompact(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.support.DefaultedPageable,org.springframework.data.domain.Sort,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws org.springframework.data.rest.webmvc.ResourceNotFoundException,org.springframework.web.HttpRequestMethodNotSupportedException
2015-07-05 13:49:48.169  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}],methods=[POST],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryEntityController.postCollectionResource(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.PersistentEntityResource,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws org.springframework.web.HttpRequestMethodNotSupportedException
2015-07-05 13:49:48.170  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[OPTIONS],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryEntityController.optionsForItemResource(org.springframework.data.rest.webmvc.RootResourceInformation)
2015-07-05 13:49:48.170  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[HEAD],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryEntityController.headForItemResource(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable) throws org.springframework.web.HttpRequestMethodNotSupportedException
2015-07-05 13:49:48.170  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[GET],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.Resource<?>> org.springframework.data.rest.webmvc.RepositoryEntityController.getItemResource(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws org.springframework.web.HttpRequestMethodNotSupportedException
2015-07-05 13:49:48.172  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/],methods=[OPTIONS],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.HttpEntity<?> org.springframework.data.rest.webmvc.RepositoryController.optionsForRepositories()
2015-07-05 13:49:48.172  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/],methods=[HEAD],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryController.headForRepositories()
2015-07-05 13:49:48.172  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/],methods=[GET],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.HttpEntity<org.springframework.data.rest.webmvc.RepositoryLinksResource> org.springframework.data.rest.webmvc.RepositoryController.listRepositories()
2015-07-05 13:49:48.174  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search],methods=[OPTIONS],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.HttpEntity<?> org.springframework.data.rest.webmvc.RepositorySearchController.optionsForSearches(org.springframework.data.rest.webmvc.RootResourceInformation)
2015-07-05 13:49:48.175  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search],methods=[HEAD],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.HttpEntity<?> org.springframework.data.rest.webmvc.RepositorySearchController.headForSearches(org.springframework.data.rest.webmvc.RootResourceInformation)
2015-07-05 13:49:48.175  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search],methods=[GET],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.hateoas.ResourceSupport org.springframework.data.rest.webmvc.RepositorySearchController.listSearches(org.springframework.data.rest.webmvc.RootResourceInformation)
2015-07-05 13:49:48.175  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search/{search}],methods=[GET],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<java.lang.Object> org.springframework.data.rest.webmvc.RepositorySearchController.executeSearch(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.web.context.request.WebRequest,java.lang.String,org.springframework.data.rest.webmvc.support.DefaultedPageable,org.springframework.data.domain.Sort,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler)
2015-07-05 13:49:48.175  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search/{search}],methods=[GET],params=[],headers=[],consumes=[],produces=[application/x-spring-data-compact+json],custom=[]}" onto public org.springframework.hateoas.ResourceSupport org.springframework.data.rest.webmvc.RepositorySearchController.executeSearchCompact(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.web.context.request.WebRequest,java.lang.String,java.lang.String,org.springframework.data.rest.webmvc.support.DefaultedPageable,org.springframework.data.domain.Sort,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler)
2015-07-05 13:49:48.176  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search/{search}],methods=[OPTIONS],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<java.lang.Object> org.springframework.data.rest.webmvc.RepositorySearchController.optionsForSearch(org.springframework.data.rest.webmvc.RootResourceInformation,java.lang.String)
2015-07-05 13:49:48.176  INFO 14988 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search/{search}],methods=[HEAD],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto public org.springframework.http.ResponseEntity<java.lang.Object> org.springframework.data.rest.webmvc.RepositorySearchController.headForSearch(org.springframework.data.rest.webmvc.RootResourceInformation,java.lang.String)
2015-07-05 13:49:48.184  INFO 14988 --- [           main] o.s.d.r.w.BaseUriAwareHandlerMapping     : Mapped "{[/alps || /alps/{repository}],methods=[OPTIONS],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto org.springframework.http.HttpEntity<?> org.springframework.data.rest.webmvc.alps.AlpsController.alpsOptions()
2015-07-05 13:49:48.184  INFO 14988 --- [           main] o.s.d.r.w.BaseUriAwareHandlerMapping     : Mapped "{[/alps],methods=[GET],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto org.springframework.http.HttpEntity<org.springframework.hateoas.alps.Alps> org.springframework.data.rest.webmvc.alps.AlpsController.alps()
2015-07-05 13:49:48.184  INFO 14988 --- [           main] o.s.d.r.w.BaseUriAwareHandlerMapping     : Mapped "{[/alps/{repository}],methods=[GET],params=[],headers=[],consumes=[],produces=[],custom=[]}" onto org.springframework.http.HttpEntity<org.springframework.data.rest.webmvc.RootResourceInformation> org.springframework.data.rest.webmvc.alps.AlpsController.descriptor(org.springframework.data.rest.webmvc.RootResourceInformation)
2015-07-05 13:49:48.353  INFO 14988 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup
2015-07-05 13:49:48.551  INFO 14988 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)
2015-07-05 13:49:48.554  INFO 14988 --- [           main] hello.Application                        : Started Application in 12.826 seconds (JVM running for 13.639)

$ ctrl+c
/ stop deze appl	,

/ we zien in chrome	 (voordat we gestopt hebben natuurlijk)	,
http://localhost:8080/
{
  "_links" : {
    "people" : {
      "href" : "http://localhost:8080/people{?page,size,sort}",
      "templated" : true
    },
    "profile" : {
      "href" : "http://localhost:8080/alps"
    }
  }
}

/ In Firefox hebben we JSONView addon installed	,

/ 7	.




	

/ Einde SPRING DATA 

/ SPRING REST

/ Lees tutorial	,
http://spring.io/guides/tutorials/bookmarks/

/ we maken een spring-boot appl	, spring-boot-tutorial-bookmarks	,



/ Einde SPRING REST

/ JAVA LAMBDAS

// Lees,
http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/Lambda-QuickStart/index.html
https://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html
 

/ Einde JAVA LAMBDAS

/ STS

/ lees	,
https://manueljordan.wordpress.com/2011/12/12/creating-a-spring-web-mvc-project-with-springsource-tool-suite/

https://spring.io/guides/gs/sts-cloud-foundry-deployment/

/ ga naar getting started guides	,
https://spring.io/guides#gs
/ we zien	,
https://spring.io/guides/gs/sts/

/ Lees	,
http://stackoverflow.com/questions/27211048/using-spring-boot-web-application-with-pivotal-tc-server

/ we kunnen een maven proj 
  <packaging>war</packaging>
/ naar de tc server slepen	, 
/ wel ERR
/ maar een spring boot NIET	, doe dan 	,
Run as -> Spring boot 


 






/ Einde STS

/ POSTGRES 

/ 7	 .

[eric@almond workspace]$ sudo less /var/lib/pgsql/9.4/data/pg_hba.conf
# "local" is for Unix domain socket connections only
local   all             all                                     peer

/ peer=CURRENT login 

[eric@almond repository]$ psql
/ of	,
[eric@almond repository]$ psql -U eric
eric=> \q

[eric@almond repository]$ sudo su - postgres -c psql
/ of	,
[eric@almond repository]$ sudo su - postgres -c "psql -U postgres"
postgres=# 

/ en inderdaad	,
[eric@almond repository]$ sudo su - postgres -c "psql -U eric"
psql: FATAL:  Peer authentication failed for user "eric"

/ Lees	,
http://stackoverflow.com/questions/18664074/getting-error-peer-authentication-failed-for-user-postgres-when-trying-to-ge
ALTER USER postgres with password 'your-pass';
/ Kan ook	,
/ TODO

/ we hebben postgres-9.4 install	,
/ we zien pg_ctl niet in /usr/bin	,

[eric@almond repository]$  ls -l /usr/bin/pg
pg_basebackup  pg_dump        pg_dumpall     pgrep          pg_restore

[eric@almond repository]$ vi /etc/alternatives/pgsql-
pgsql-clusterdb         pgsql-droplang          pgsql-pg_dumpman
pgsql-clusterdbman      pgsql-droplangman       pgsql-pg_restore
pgsql-createdb          pgsql-dropuser          pgsql-pg_restoreman
pgsql-createdbman       pgsql-dropuserman       pgsql-psql
pgsql-createlang        pgsql-ld-conf           pgsql-psqlman
pgsql-createlangman     pgsql-pg_basebackup     pgsql-reindexdb
pgsql-createuser        pgsql-pg_basebackupman  pgsql-reindexdbman
pgsql-createuserman     pgsql-pg_dump           pgsql-vacuumdb
pgsql-dropdb            pgsql-pg_dumpall        pgsql-vacuumdbman
pgsql-dropdbman         pgsql-pg_dumpallman     

[eric@almond repository]$ sudo su - postgres -c "/usr/pgsql-9.4/bin/pg_ctl reload"
server signaled
/ OK
/ TODO

/ 7	.

[eric@almond repository]$ PGPASSWORD=foo psql -U foo -h localhost
foo=> \q

/ 7	. 

$ vi pg_hba.conf
host    all             all             127.0.0.1/32            md5
host    all             all             192.168.123.182/24      md5

 /de 1ste is voor	,
[eric@almond repository]$ psql -U foo -h localhost
/ de 2de voor	,
[eric@almond repository]$ psql -U foo -h almond 



/ Einde POSTGRES 


/ JSTL JSP-API

/ 7	.
/ jstl is tag lib , nodig voor	<c:out/>,
<%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core"%>
<%@ taglib prefix="fmt" uri="http://java.sun.com/jsp/jstl/fmt"%>

/ see	,
https://tomcat.apache.org/tomcat-4.1-doc/servletapi/
/ dit zijn packages
javax.servlet.*
javax.servlet.http.*
javax.servlet.jsp.*
javax.servlet.jsp.tagext.*

/ Dit is voor custom tags program	,
/ TODO 

/ 7	.

/ we zien in in proj. spring-mvc-step-by-step2	,  

  <dependency>
	    <groupId>javax.servlet</groupId>
	    <artifactId>jstl</artifactId>
	    <version>${jstl.version}</version>
	</dependency>

	<dependency>
    <groupId>javax.servlet</groupId>
    <artifactId>jsp-api</artifactId>
    <version>2.0</version>
	</dependency>

/ we zien dat de 2de dep een dep is van de 1ste	, voor eclipse moeten we dit , anders SOMS ERR	,
<%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core"%>
/ SOMS is jstl GENOEG	, 
/ TODO

/ 7	.
/ wat zijn 
jstl.jar, standard.jar 
servlet.jsp-api.jar	, jsp-api.jar	?
servlet-api.jar, 


/ Voor als je custom tags wilt create	,
javax.servlet.jsp:jsp-api
/ of	,
javax.servlet.jsp:javax.servlet.jsp-api

[eric@almond repository]$ jar tvf ~/Downloads/jsp-api-2.2.1-b03-sources.jar 
  3956 Mon Jan 10 14:14:10 CET 2011 javax/servlet/jsp/el/Expression.java
  4362 Mon Jan 10 14:14:10 CET 2011 javax/servlet/jsp/HttpJspPage.java
 17944 Mon Jan 10 14:14:10 CET 2011 javax/servlet/jsp/JspWriter.java
 11719 Mon Jan 10 14:14:10 CET 2011 javax/servlet/jsp/tagext/VariableInfo.java
 11518 Mon Jan 10 14:14:10 CET 2011 javax/servlet/jsp/tagext/Tag.java

[eric@almond repository]$ less javax/servlet/jsp/javax.servlet.jsp-api/2.3.2-b01/javax.servlet.jsp-api-2.3.2-b01.pom
   <dependencies>
        <dependency>
            <groupId>javax.servlet</groupId>
            <artifactId>javax.servlet-api</artifactId>
            <version>3.0.1</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>javax.el</groupId>
            <artifactId>javax.el-api</artifactId>
            <version>[3.0.0,)</version>
            <scope>provided</scope>
        </dependency>
   </dependencies>
/ TODO




/ standard.jar zit in jstl.jar	,

[eric@almond repository]$ jar tvf javax/servlet/jstl/1.2/jstl-1.2.jar
  1672 Thu Jul 20 08:44:38 CEST 2006 javax/servlet/jsp/jstl/core/ConditionalTagSupport.class
  2547 Thu Jul 20 08:44:38 CEST 2006 javax/servlet/jsp/jstl/fmt/LocaleSupport.class
   327 Thu Jul 20 08:44:38 CEST 2006 javax/servlet/jsp/jstl/sql/Result.class
  4135 Thu Jul 20 08:44:38 CEST 2006 javax/servlet/jsp/jstl/tlv/ScriptFreeTLV.class
 3445 Thu Jul 20 08:44:38 CEST 2006 org/apache/taglibs/standard/extra/spath/ParseException.class
  4835 Thu Jul 20 08:44:38 CEST 2006 org/apache/taglibs/standard/functions/Functions.class
  1189 Thu Jul 20 08:44:40 CEST 2006 org/apache/taglibs/standard/lang/jstl/ArithmeticOperator.class
  1349 Thu Jul 20 08:44:40 CEST 2006 org/apache/taglibs/standard/tag/common/core/CatchTag.class
  1651 Thu Jul 20 08:44:40 CEST 2006 org/apache/taglibs/standard/tag/common/fmt/ParamSupport.class
  2835 Thu Jul 20 08:44:40 CEST 2006 org/apache/taglibs/standard/tag/common/sql/SetDataSourceTagSupport.class
  6870 Thu Jul 20 08:44:42 CEST 2006 org/apache/taglibs/standard/tag/common/xml/ParseSupport.class
  2590 Thu Jul 20 08:44:42 CEST 2006 org/apache/taglibs/standard/tag/el/core/ForEachTag.class
  1524 Thu Jul 20 08:44:42 CEST 2006 org/apache/taglibs/standard/tag/el/fmt/BundleTag.class
  1388 Thu Jul 20 08:44:42 CEST 2006 org/apache/taglibs/standard/tag/el/sql/TransactionTag.class
   785 Thu Jul 20 08:44:42 CEST 2006 org/apache/taglibs/standard/tag/rt/core/OutTag.class
  1090 Thu Jul 20 08:44:42 CEST 2006 org/apache/taglibs/standard/tag/rt/sql/SetDataSourceTag.class
   803 Thu Jul 20 08:44:42 CEST 2006 org/apache/taglibs/standard/tei/ForEachTEI.class
  8395 Thu Jul 20 08:44:42 CEST 2006 org/apache/taglibs/standard/tlv/JstlBaseTLV.class
 10763 Thu Jul 20 08:44:42 CEST 2006 META-INF/c-1_0-rt.tld
 11310 Thu Jul 20 08:44:42 CEST 2006 META-INF/c-1_0.tld
 16263 Thu Jul 20 08:44:42 CEST 2006 META-INF/c.tld
 11409 Thu Jul 20 08:44:42 CEST 2006 META-INF/fmt-1_0-rt.tld
 12580 Thu Jul 20 08:44:42 CEST 2006 META-INF/fmt-1_0.tld
 19595 Thu Jul 20 08:44:42 CEST 2006 META-INF/fmt.tld
  7298 Thu Jul 20 08:44:42 CEST 2006 META-INF/fn.tld
  1169 Thu Jul 20 08:44:42 CEST 2006 META-INF/permittedTaglibs.tld
  1722 Thu Jul 20 08:44:42 CEST 2006 META-INF/scriptfree.tld
  5372 Thu Jul 20 08:44:42 CEST 2006 META-INF/sql-1_0-rt.tld
  6127 Thu Jul 20 08:44:42 CEST 2006 META-INF/sql-1_0.tld
  8446 Thu Jul 20 08:44:42 CEST 2006 META-INF/sql.tld
  7200 Thu Jul 20 08:44:42 CEST 2006 META-INF/x-1_0-rt.tld
  7673 Thu Jul 20 08:44:42 CEST 2006 META-INF/x-1_0.tld
 12290 Thu Jul 20 08:44:42 CEST 2006 META-INF/x.tld

/ Welke class in jstl doet <c:out/> bijv?
/ TODO

$ less  javax/servlet/jstl/1.2/jstl-1.2.pom
<project>
  <modelVersion>4.0.0</modelVersion>
  <groupId>javax.servlet</groupId>
  <artifactId>jstl</artifactId>
  <version>1.2</version>
  <dependencies>
    <dependency>
      <groupId>javax.servlet</groupId>
      <artifactId>jsp-api</artifactId>
      <version>2.0</version>
      <scope>provided</scope>
    </dependency>
  </dependencies>
</project>
/ WH ziet Eclipse deze dep niet	,
/ TODO

/ Dit zijn servlets	,
[eric@almond repository]$ jar tvf javax/servlet/servlet-api/2.5/servlet-api-2.5.jar
   473 Wed May 10 14:20:30 CEST 2006 javax/servlet/Servlet.class
  3579 Wed May 10 14:20:32 CEST 2006 javax/servlet/http/HttpServletRequestWrapper.class
 16078 Wed May 10 14:20:32 CEST 2006 javax/servlet/resources/XMLSchema.dtd
  6360 Wed May 10 14:20:32 CEST 2006 javax/servlet/resources/datatypes.dtd
 45896 Wed May 10 14:20:32 CEST 2006 javax/servlet/resources/j2ee_1_4.xsd
 10402 Wed May 10 14:20:32 CEST 2006 javax/servlet/resources/j2ee_web_services_client_1_1.xsd
 63023 Wed May 10 14:20:32 CEST 2006 javax/servlet/resources/javaee_5.xsd
 18354 Wed May 10 14:20:32 CEST 2006 javax/servlet/resources/javaee_web_services_client_1_2.xsd
  9536 Wed May 10 14:20:32 CEST 2006 javax/servlet/resources/jsp_2_0.xsd
 10781 Wed May 10 14:20:32 CEST 2006 javax/servlet/resources/jsp_2_1.xsd
 18708 Wed May 10 14:20:32 CEST 2006 javax/servlet/resources/web-app_2_2.dtd
 32726 Wed May 10 14:20:32 CEST 2006 javax/servlet/resources/web-app_2_3.dtd
 36344 Wed May 10 14:20:32 CEST 2006 javax/servlet/resources/web-app_2_4.xsd
 37722 Wed May 10 14:20:32 CEST 2006 javax/servlet/resources/web-app_2_5.xsd
  2922 Wed May 10 14:20:32 CEST 2006 javax/servlet/resources/xml.xsd






/ Lees	,
http://www.tutorialspoint.com/jsp/jsp_custom_tags.htm



/ Einde JSTL JSP-API

/ SELENIUM

/ 7	.

/ Selenium IDE

/ Lees
http://docs.seleniumhq.org/download/maven.jsp

$ vi pom.xml

	<dependency>
	        <groupId>org.seleniumhq.selenium</groupId>
	        <artifactId>selenium-java</artifactId>
	        <version>${selenium.version}</version>
    </dependency>  
    
  </dependencies>

/ Ga naar	,
http://docs.seleniumhq.org/download/

Selenium IDE
Selenium IDE is a Firefox plugin which records and plays back user interactions with the browser. Use this to either create simple scripts or assist in exploratory testing. It can also export Remote Control or WebDriver scripts, though they tend to be somewhat brittle and should be overhauled into some sort of Page Object-y structure for any kind of resiliency.

Download latest released version 2.9.0 released on 09/Mar/2015 or view the Release Notes and then install some plugins.

Download previous version 2.8.0 released on 29/Sep/2014.

/ click op 2.9.0	, install ze allemaal in firefox	,

/ 7	.

/ click op icon rechts	, we record	, 
/ click options, options...
/ check enable experimental  features					: JUnit4 format 
/ click options, format, Java/ JUnit4/ Webdriver

/ save ,en neem op in als JUnit test	,

/ 13	. 

 /we gaan naar angular.js	, wait als title er is	, en gaan naar localhost:8080/... en wachten tot de title er is	, maar het gaan naar http://localhost:8080/spring-mvc-step-by-step/hello.jsp heeft hij niet record	,

  @Before
  public void setUp() throws Exception {
    driver = new FirefoxDriver();
/ We zien de browser	,
    baseUrl = "https://angularjs.org/";
    driver.manage().timeouts().implicitlyWait(30, TimeUnit.SECONDS);
/ we step hier overheen	, 
/ doet niets zo te zien	,
/ TODO
  }

  @Test
  public void testSelenium() throws Exception {
    driver.get(baseUrl + "/");
    for (int second = 0;; second++) {
    	if (second >= 60) fail("timeout");
    	try { if ("AngularJS — Superheroic JavaScript MVW Framework".equals(driver.getTitle())) break; } catch (Exception e) {}
/JA, dus uit de for loop	,
    	Thread.sleep(1000);
    }

    for (int second = 0;; second++) {
    	if (second >= 60) fail("timeout");
    	try { if ("Hello :: Spring application".equals(driver.getTitle())) break; } catch (Exception e) {}
/ NEE	, 
/ hij is niet naar http://localhost:8080/spring-mvc-step-by-step/hello.jsp	, 
    	Thread.sleep(1000);
    }

/ 7	. 

There was an unexpected error. Msg: TypeError: insertedCommand.line is undefined
Url: chrome://selenium-ide/content/sourceView.js, line: 34, column: 7
SourceView.prototype.rowInserted@chrome://selenium-ide/content/sourceView.js:34:8
Editor.prototype.addCommand@chrome://selenium-ide/content/editor.js:858:5
Editor.prototype.appendWaitForPageToLoad@chrome://selenium-ide/content/editor.js:885:7
Editor.prototype.onUnloadDocument/<@chrome://selenium-ide/content/editor.js:662:5




/ Einde SELENIUM


/ TOMCAT MAVEN

$ vi conf/tomcat-users.xml

  <role rolename="manager-gui"/>
  <role rolename="manager-script"/>
  <user username="tomcat" password="s3cret" roles="manager-gui,manager-script"/>
</tomcat-users>

$ vi ~/.m2/settings.xml

<?xml version="1.0" encoding="UTF-8"?>
<settings>
    <servers>
        <server>
            <id>tomcat</id>
            <username>tomcat</username>
            <password>s3cret</password>
        </server>
    </servers>
</settings>

$ vi pom.xml

	    <plugin>
		  <groupId>org.apache.tomcat.maven</groupId>
		  <artifactId>tomcat7-maven-plugin</artifactId>
		  <version>${tomcat7.plugin.version}</version>
		  <configuration>
		 <server>tomcat</server>
		  </configuration>
		</plugin>

/ Doe 	,
mvn tomcat7:undeploy clean tomcat7:deploy	
/ OK

/ Einde TOMCAT MAVEN

/ STS

/ Einde STS

/ POSTGRES CLUSTER

/ bekijk	,
https://www.youtube.com/watch?v=mbXPbLjiYTI

https://www.youtube.com/watch?v=vsMI5RhfU3M

/ Lees	,
https://wiki.postgresql.org/wiki/PL/Proxy

http://www.postgres-xl.org/overview/

http://plproxy.projects.pgfoundry.org/doc/tutorial.html



/ Einde  POSTGRES CLUSTER
