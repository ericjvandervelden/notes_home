/ See FINAL GD NEWTON SELF SCIKIT LEARN
/ See 3 CLASSES	, SCIKIT LEARN NEWTON
/ See LAATSTE VERSIE 3 CLASSES

/ 7	. 

def init():
	a1=iris['data'][:,3:]
	A1=np.c_[np.ones((150,1)),a1]
	y=(iris['target']==2).astype(int).reshape(-1,1)
	return a1,A1,y

def permu(A1,y,N):
  permu=np.random.permutation(150)
  A1=A1[permu[:N]]
  y=y[permu[:N]]
  return A1,y 

def logreg_gd_reg(w):
  z2=A1.dot(w)
  a2=expit(z2)
  delta2=a2-y
  w=w*(1-alpha*lmbda/len(a1))-(alpha/len(a1))*A1.T.dot(delta2)
  return w

def round(init,w):
	if init:
		w=np.array([[0],[0]])
  for i in range(0,100000):
    w=logreg_gd_reg(w)
  return w

def total(N,_alpha,_lmbda):
	global a1,A1,y,alpha,lmbda
	a1,A1,y=init()
	alpha=_alpha
	lmbda=_lmbda
	A1,y=permu(A1,y,N)
	w=round(True,None)
	return w

w=total(150,1,0)
w=round(False,w)
...

w=total(150,1,1)
w=round(False,w)
...

w=total(75,1,0)
w=round(False,w)
...


w=total(75,1,0)
w=round(False,w)
...

In [6720]: w=total(75,1,0)
In [6721]: w
Out[6721]: 
array([[-19.58827344],
       [ 12.07202365]])

In [6722]: w=total(75,1,1)
In [6723]: w
Out[6723]: 
array([[-3.03177716],
       [ 2.13542635]])

/ 7	 

def C(w):
  z2=A1.dot(w)
  a2=expit(z2)
  c2=y*-np.log(a2)+(1-y)*-np.log(1-a2)
  return np.ones((1,len(A1))).dot(c2)

In [6757]: w=total(150,1,1)
In [6758]: w
Out[6758]: 
array([[-4.22103221],
       [ 2.61733229]])
In [6759]: C(w)
Out[6759]: array([[39.14823815]])

In [6760]: w=total(150,1,0)
In [6763]: w
Out[6763]: 
array([[-21.12563996],
       [ 12.94750716]])
In [6761]: C(w)
Out[6761]: array([[16.71040414]])

/ 7	. 

def Cgrid(h,N):
  # c center, shape (2,1)
  # h grootte steps
  # N aantal steps
  hblk=np.linspace(w[0,0]-h,w[0,0]+h,N)
  hor=np.tile(hblk,N)
  vblk=np.linspace(w[1,0]-h,w[1,0]+h,N)
  ver=np.repeat(vblk,N)
  W=np.array([hor,ver])
  return C(W).reshape(N,-1)

In [6760]: w=total(150,1,0)
In [6787]: Cgrid(.1,3)
Out[6787]: 
array([[16.87473249, 16.77348118, 16.72000491],
       [16.73432467, 16.71040414, 16.73450667],
       [16.71984576, 16.77291837, 16.87451379]])

In [6801]: w=total(150,1,1)
In [6802]: Cgrid(.1,3)
Out[6802]: 
array([[39.69895832, 39.68827917, 39.88890701],
       [38.83038841, 39.14823815, 39.67494106],
       [38.51933109, 39.15571137, 39.99744445]])

/ 7	. 

/ Newton-Raphson  ,
/ Bishop  ,

#from sklearn import datasets
#iris=datasets.load_iris()
def total(K,N,_lmbda):
  global x,X,y,lmbda
	x=iris['data'][:,3:]
	X=np.c_[np.ones(len(x)),x]
	y=(iris['target']==2).astype(int).reshape(-1,1)
  lmbda=_lmbda
  X,y=permu(X,y,N)
  w=np.array([[0],[0]])
  for i in range(0,K):
    w=logreg_newton_regul(w)
  return w

w=logreg_newton_regul(w)

# als a.shape=(4,1), is a[:,0].shape=(4,)	, dus is weer een array	, 
# np.diag verwacht een np.array	,
def logreg_newton_regul(w):
  z=X.dot(w)
  a=expit(z)
  D=np.diag((a*(1-a))[:,0]) 
  H=X.T.dot(D).dot(X)+lmbda*np.diag(np.ones(len(w)))
	term_indep_regul=X.T.dot(D.dot(z)-(a-y))
  w=np.linalg.inv(H).dot(term_indep_regul)
	return w

def permu(X,y,n):
  permu=np.random.permutation(150)
  X=X[permu[:n]]
  y=y[permu[:n]]
  return X,y


In [6995]: w=total(10,150,0)
In [6996]: w
Out[6996]: 
array([[-21.12563996],
       [ 12.94750716]])

In [6997]: w=total(10,150,1)
In [6998]: w
Out[6998]: 
array([[-4.22103221],
       [ 2.61733229]])

In [7006]: w=total(10,75,0)

In [7007]: w
Out[7007]: 
array([[-19.27290711],
       [ 11.89854691]])
/ een andere total(10,75,0) geeft andere uitkomst	,

In [7008]: w=total(10,75,1)

In [7009]: w
Out[7009]: 
array([[-3.17860162],
       [ 1.95655075]])
/ een andere total(10,75,1) geeft andere uitkomst	,


------------------

/ 7	. 

/ Dit kan gewoon	,

In [7319]: b1=np.array([[1],[2],[3],[4]])
In [7320]: b1
Out[7320]: 
array([[1],
       [2],
       [3],
       [4]])
In [7321]: 1-b1
Out[7321]: 
array([[ 0],
       [-1],
       [-2],
       [-3]])

In [7325]: np.log(b1)
Out[7325]: 
array([[0.        ],
       [0.69314718],
       [1.09861229],
       [1.38629436]])

/ 13	. 

In [7336]: b1=np.arange(1,5)
In [7338]: b1=b1.reshape(len(b1),1)
In [7339]: b1
Out[7339]: 
array([[1],
       [2],
       [3],
       [4]])

/ 13	. 

In [7351]: theta=np.random.rand(2,1)
In [7352]: theta
Out[7352]: 
array([[0.77893654],
       [0.27937527]])

/ 13	. 

In [7372]: X=iris['data'][:,[2,3]]         
In [7372]: X=np.c_[np.ones((len(X),1)),X]
/ alle waarnemingen, alle features + 1 

In [7378]: theta=np.random.rand(3,1)
In [7379]: theta
Out[7379]: 
array([[0.28211265],
       [0.88633801],
       [0.06248916]])

In [7392]: y=(iris['target']==2).reshape(-1,1).astype(int)

for i in range(...):

In [7409]: z=X.dot(theta)
In [7428]: a=expit(z)                                     
In [7427]: J=y.T.dot(-np.log(a))+(1-y).T.dot(-np.log(1-a))

In [7429]: J
Out[7429]: array([[297.24332029]])

In [7430]: (1-y).T.dot(-np.log(1-a))
Out[7430]: array([[296.97379305]])
In [7431]: y.T.dot(-np.log(a))
Out[7431]: array([[0.26952724]])

/ In J zien we dat de cost klein is als, 
y.T.dot(-np.log(a): y=1 en a=1
(1-y).T.dot(-np.log(1-a)): y=0 en a=0

y.T.dot(-np.log(a): als y=1 en a=0	, is de cost groot	, 
(1-y).T.dot(-np.log(1-a)): als y=0 en a=1, is de cost groot	, 

/ hoge cost betekent: a en y kloppen niet	,

In [7447]: np.c_[a,y]
array([[0.82280723, 0.        ],	/ hoge cost	,
       [0.82280723, 0.        ],	/ ...
       [0.80951316, 0.        ],
       [0.8353622 , 0.        ],
       [0.82280723, 0.        ],
...
       [0.98485169, 0.        ],
       [0.9530178 , 0.        ],
       [0.98196665, 0.        ],		/ hoge cost	,
       [0.99684738, 1.        ],		/ lage cost	,
       [0.99276214, 1.        ],		/ ...
       [0.99646934, 1.        ],

/ 13	. 

/ FINAL GD NEWTON SELF SCIKIT LEARN

/ petal width	,

/ 1313	. 

/ gd


X=iris['data'][:,[3]]         
X=np.c_[np.ones((len(X),1)),X]
theta=np.array([0,0]).reshape(-1,1)
y=(iris['target']==2).reshape(-1,1).astype(int)
alpha=.1 
for i in range(0,10000):
	z=X.dot(theta)
	a=expit(z)
	grad=X.T.dot(a-y)
	theta=theta-alpha*grad

/ 1313	. 

/ newton	,

X=iris['data'][:,[3]]         
X=np.c_[np.ones((len(X),1)),X]
theta=np.array([0,0]).reshape(-1,1)
y=(iris['target']==2).reshape(-1,1).astype(int)
for i in range(0,10):
	z=X.dot(theta)
	a=expit(z)
	grad=X.T.dot(a-y)
	D=np.diag((a*(1-a)).ravel())
	H=X.T.dot(D).dot(X)
	theta=theta-np.linalg.inv(H).dot(grad)

In [7594]: theta
Out[7594]: 
array([[-21.12563996],
       [ 12.94750716]])

/ 13	. 

/ petal length, width

/ 1313	. 

/ gd	,

X=iris['data'][:,[2,3]]         
X=np.c_[np.ones((len(X),1)),X]
theta=np.array([0,0,0]).reshape(-1,1)
y=(iris['target']==2).reshape(-1,1).astype(int)
alpha=.1 
for i in range(0,100000):
	z=X.dot(theta)
	a=expit(z)
	theta=theta-alpha*X.T.dot(a-y)
/ niet stabiel	,
/ TODO

/ 1313	. 

/ newton	,

X=iris['data'][:,[2,3]]         
X=np.c_[np.ones((len(X),1)),X]
theta=np.array([0,0,0]).reshape(-1,1)
y=(iris['target']==2).reshape(-1,1).astype(int)
for i in range(0,10):
	z=X.dot(theta)
	a=expit(z)
	grad=X.T.dot(a-y)
	D=np.diag((a*(1-a)).ravel())
	H=X.T.dot(D).dot(X)
	theta=theta-np.linalg.inv(H).dot(grad)

In [7602]: theta
Out[7602]: 
array([[-45.27234377],
       [  5.75453232],
       [ 10.44669989]])
/ stable	,


/ 1313	. 

/ gd, regl	,

X=iris['data'][:,[2,3]]         
X=np.c_[np.ones((len(X),1)),X]
theta=np.array([0,0,0]).reshape(-1,1)
y=(iris['target']==2).reshape(-1,1).astype(int)
alpha=.1 
lmbda=1
for i in range(0,100000):
	z=X.dot(theta)
	a=expit(z)
	grad=X.T.dot(a-y)+lmbda*theta
	theta=theta-alpha*grad

In [7660]: theta
Out[7660]: 
array([[-18.87363258],
       [ 13.18067091],
       [ 16.50468954]])

/ met lbmda=.1 is niet stabiel	, 
/ TODO

/ 1313	. 

/ newton, regl	,

X=iris['data'][:,[2,3]]         
X=np.c_[np.ones((len(X),1)),X]
theta=np.array([0,0,0]).reshape(-1,1)
y=(iris['target']==2).reshape(-1,1).astype(int)
lmbda=1
for i in range(0,10):
	z=X.dot(theta)
	a=expit(z)
	grad=X.T.dot(a-y)+lmbda*theta
	D=np.diag((a*(1-a)).ravel())
	H=X.T.dot(D).dot(X)+lmbda*np.diag([1,1,1])
	theta=theta-np.linalg.inv(H).dot(grad)

In [7638]: theta
Out[7638]: 
array([[-4.27700077],
       [ 0.04030967],
       [ 2.53398464]])

/ als lmbda=.1	, 
In [7644]: theta
Out[7644]: 
array([[-11.50605939],
       [  0.74893214],
       [  4.79308489]])

/ als lmbda=.01
In [7651]: theta
Out[7651]: 
array([[-24.86777444],
       [  2.70155942],
       [  7.10675456]])

/ 1313		. 

/ sk	,

In [7676]: y=(iris['target']==2).astype(np.int)
In [7678]: log_reg = LogisticRegression(solver="liblinear", C=10**10)
In [7679]: log_reg.fit(X,y)
In [7680]: log_reg.coef_
Out[7680]: array([[ 5.7528683 , 10.44455633]])
In [7682]: log_reg.intercept_
Out[7682]: array([-45.26062435])
/= eigen Newton boven	, zonder regularisatie	,

/ 1313	. 

/ sk, regl	,

In [7683]: log_reg = LogisticRegression(solver="liblinear")
In [7684]: log_reg.fit(X,y)
In [7686]: log_reg.coef_
Out[7686]: array([[0.04030798, 2.5339354 ]])
In [7687]: log_reg.intercept_
Out[7687]: array([-4.27691117])
/= eigen Newton boven, met regularisatie , lambda=1
/ default C=1	,

In [7688]: log_reg = LogisticRegression(solver="liblinear",C=10)
In [7689]: log_reg.fit(X,y)
In [7690]: log_reg.intercept_
Out[7690]: array([-11.50605884])
In [7691]: log_reg.coef_
Out[7691]: array([[0.74893219, 4.79308438]])
/= eigen Newton boven, met regulatisatie, lmbda=.1

/ 13	. 

/ print


X=iris['data'][:,[3]]
Xext=np.c_[np.ones((len(X),1)),X]

In [7958]: plt.figure()
In [7959]: plt.plot(X[(y==0).ravel()],y[(y==0).ravel()],'gs')
In [7960]: plt.plot(X[(y==1).ravel()],y[(y==1).ravel()],'b^')

In [7936]: I=np.linspace(min(X.ravel()),max(X.ravel()),101)
In [7936]: Iext=np.c_[np.ones(101),I]
In [7939]: v=expit(Iext.dot(theta))
In [7968]: plt.plot(I,v)

x=iris['data'][:,[3]]
X=np.c_[np.ones((len(x),1)),x]

plt.figure()
plt.plot(x[(y==0).ravel()],y[(y==0).ravel()],'gs')
plt.plot(x[(y==1).ravel()],y[(y==1).ravel()],'b^')

i=np.linspace(min(x.ravel()),max(x.ravel()),101)
I=np.c_[np.ones(101),i]
v=expit(I.dot(theta))
plt.plot(i,v)


/ 13	. 

/ newton & print

[eric@almond my]$ pwd
/home/eric/Devel/python/my
[eric@almond my]$ cat newton.py 

import numpy as np
from IPython.display import display

from sklearn import datasets
iris=datasets.load_iris()

x=iris['data'][:,[3]]
X=np.c_[np.ones((len(x),1)),x]
theta=np.array([0,0]).reshape(-1,1)
y=(iris['target']==2).reshape(-1,1).astype(int)

from scipy.special import expit

for i in range(0,10):
  z=X.dot(theta)
  a=expit(z)
  grad=X.T.dot(a-y)
  D=np.diag((a*(1-a)).ravel())
  H=X.T.dot(D).dot(X)
  theta=theta-np.linalg.inv(H).dot(grad)

display(theta)

import matplotlib.pyplot as plt
import seaborn
seaborn.set()

plt.figure()
plt.plot(x[(y==0).ravel()],y[(y==0).ravel()],'gs')
plt.plot(x[(y==1).ravel()],y[(y==1).ravel()],'b^')

i=np.linspace(min(x.ravel()),max(x.ravel()),101)
I=np.c_[np.ones(101),i]
v=expit(I.dot(theta))
plt.plot(i,v)

plt.show()

/ in ipython deden we 
%matplotlib
/ in het script doen we dat NIET	,

/ hierdoor zien we een assenstelsel:
seaborn.set() 

/ we moeten, in ipython hoeft dat NIET	,
plt.show()

/ call	,
$ ipython newton.py

/ 13	. 

/ print 2 features	,

[eric@almond my]$ pwd
/home/eric/Devel/python/my
[eric@almond my]$ cat newton2.py 

import numpy as np
from IPython.display import display

from sklearn import datasets
iris=datasets.load_iris()

# waarnemingen	,
x=iris['data'][:,[2,3]]
X=np.c_[np.ones((len(x),1)),x]
theta=np.array([0,0,0]).reshape(-1,1)
y=(iris['target']==2).reshape(-1,1).astype(int)

from scipy.special import expit

for i in range(0,10):
  z=X.dot(theta)
  a=expit(z)
  grad=X.T.dot(a-y)
  D=np.diag((a*(1-a)).ravel())
  H=X.T.dot(D).dot(X)
  theta=theta-np.linalg.inv(H).dot(grad)

display(theta)

import matplotlib.pyplot as plt
import seaborn
seaborn.set()

plt.figure()
plt.plot(x[(y==0).ravel(),0],x[(y==0).ravel(),1],'gs')
plt.plot(x[(y==1).ravel(),0],x[(y==1).ravel(),1],'b^')

/ Intermezzo

In [8370]: min(x[:,0])
Out[8370]: 1.0
In [8371]: max(x[:,0])
Out[8371]: 6.9
In [8372]: min(x[:,1])
Out[8372]: 0.1
In [8373]: max(x[:,1])
Out[8373]: 2.5

/ Einde Intermezzo

i=np.linspace(min(x[:,0]),max(x[:,0]),60)
j=np.linspace(min(x[:,1]),max(x[:,1]),25)
I,J=np.meshgrid(i,j)
G=np.c_[np.ones(len(I.ravel())),I.ravel(),J.ravel()]

/ Intermezzo

In [8389]: G[:5]
Out[8389]: 
array([[1. , 1. , 0.1],
       [1. , 1.1, 0.1],
       [1. , 1.2, 0.1],
       [1. , 1.3, 0.1],
       [1. , 1.4, 0.1]])
In [8395]: G[-5:]
Out[8395]: 
array([[1. , 6.5, 2.5],
       [1. , 6.6, 2.5],
       [1. , 6.7, 2.5],
       [1. , 6.8, 2.5],
       [1. , 6.9, 2.5]])

/ of gewoon	,
In [8401]: G
Out[8401]: 
array([[1. , 1. , 0.1],
       [1. , 1.1, 0.1],
       [1. , 1.2, 0.1],
       ...,
       [1. , 6.7, 2.5],
       [1. , 6.8, 2.5],
       [1. , 6.9, 2.5]])


/ Einde Intermezzo

V=expit(G.dot(theta)).reshape(I.shape)
contours=plt.contour(I,J,V,cmap=plt.cm.brg)
plt.clabel(contours,inline=True)

plt.show()






/ Einde FINAL GD NEWTON SELF SCIKIT LEARN








/ 7	. 

/ m waarnemingen, n features	,
/ X.shape=(m,n+1)	, y.shape=(m,1)

/ 7	. 

/ google,
differentiate to vectors chain rule
/ lees,
http://www.met.reading.ac.uk/~ross/Documents/Chain.pdf

/ 7	. 

/ Intermezzo

/ conclusie	, 

/ 13	. 

/ we hebben een array van (150,2)	,
/ deze kun je select op een boolean (150,)	, hij pakt dan die entries waar het boolean array True is	,
/ omdat het array (150,2) is , kun je ook select op een boolean (150,2)	, hij pakt dan die entries waar het boolean array True is	, en dit wil je niet	,
/ boolean array (150,1) geeft ERR op dimension 1	, die moet 2 zijn en is 1	,

https://www.leukerecepten.nl/recepten/wrap-met-falafel/

bs=np.random.rand(150)<.5
bs2=np.concatenate((bs.reshape(-1,2),bs.reshape(-1,2)),axis=0)

Xextsel=Xext[:10]
bs2sel=bs2[:10]

In [7913]: Xextsel
Out[7913]: 
array([[1. , 0.2],
       [1. , 0.2],
       [1. , 0.2],
       [1. , 0.2],
       [1. , 0.2],
       [1. , 0.4],
       [1. , 0.3],
       [1. , 0.2],
       [1. , 0.2],
       [1. , 0.1]])

In [7914]: bs2sel
Out[7914]: 
array([[ True,  True],
       [False, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [False, False],
       [False,  True],
       [ True,  True],
       [ True, False]])

In [7915]: Xextsel[bs2sel]
Out[7915]: array([1. , 0.2, 1. , 1. , 1. , 1. , 0.2, 1. , 0.2, 1. ])

/ Dit willen we niet	,
/ we willen	,
In [7916]: bssel=bs[:10]
array([ True,  True, False, False,  True, False,  True, False,  True,
       False])

In [7918]: Xextsel[bssel]
Out[7918]: 
array([[1. , 0.2],
       [1. , 0.2],
       [1. , 0.2],
       [1. , 0.3],
       [1. , 0.2]])

/ 13	. 

/ y is een (150,1) array. Deze kun je dan toevallig wel: y[y==0]	, 
/ beter is y[(y==0).ravel()]
/ y[y==0] werkt toevallig wel	, want y is (150,) en y==0 ook	, en net als boven pakt hij alle entries waar True staat	, maar nu is dimensie 1 1 groot, dus gaat het goed	, 
/ dat is alleen in dit geval	, 

/ Einde conclusie

/ 13	.

X=iris['data'][:,[3]]
Xext=np.c_[np.ones((len(X),1)),X]
y=(iris['target']==2).reshape(-1,1).astype(int)

In [7824]: X[y==0]          
/ OK	,
In [7824]: X[(y==0).ravel()]
/ OK

In [7820]: Xext[y==0]
IndexError: boolean index did not match indexed array along dimension 1; dimension is 2 but corresponding boolean dimension is 1
/ ERR	,
In [7820]: Xext[(y==0).ravel()]
/ OK	,

/ maak een random boolean array van 150 lang	,
bs=np.random.rand(150)<.5

In [7833]: (y==0).shape
Out[7833]: (150, 1)
In [7834]: (y==0).ravel().shape
Out[7834]: (150,)

In [7831]: (bs==True).shape
Out[7831]: (150,)
In [7832]: Xext[bs==True]                
/ OK	,
In [7832]: Xext[(bs==True).reshape(-1,1)]
IndexError: boolean index did not match indexed array along dimension 1; dimension is 2 but corresponding boolean dimension is 1
/ ERR,	
/ net als Xext[y==0]	

In [7782]: len(bs[bs==True])
Out[7782]: 78
/ zoveel zijn er True	,

In [7784]: len(X[bs==True])
Out[7784]: 78

In [7786]: len(Xext[bs==True])
Out[7786]: 78

In [7790]: Xext[y==0]
IndexError: boolean index did not match indexed array along dimension 1; dimension is 2 but corresponding boolean dimension is 1
In [7790]: Xext[(y==0).ravel()]
/ OK	,

In [7794]: Xext[[bs==True]]
/ OK	,
In [7792]: Xext[[[bs==True]]]
IndexError: boolean index did not match indexed array along dimension 0; dimension is 150 but corresponding boolean dimension is 1

/ 1313	. 

/ tmp	,

----> 1 Xext[np.array([True,False])]
IndexError: boolean index did not match indexed array along dimension 0; dimension is 150 but corresponding boolean dimension is 2

----> 1 Xext[np.array([True,False,True])]
IndexError: boolean index did not match indexed array along dimension 0; dimension is 150 but corresponding boolean dimension is 3

In [7866]: np.array([True,False,True]).shape
Out[7866]: (3,)







/ 7 .

/ install pydev	,
https://www.pydev.org/manual_101_install.html







/ Einde Intermezzo

/ 7	

In [10061]: m=np.array([1,2,3,4]).reshape(-1,2)
array([[1, 2],
       [3, 4]])

In [10063]: x=np.array([2,3]).reshape(-1,1)
array([[2],
       [3]])

In [10077]: x.T.dot(m)
Out[10077]: array([[11, 16]])

In [10078]: m.dot(x)
array([[ 8],
       [18]])

/ 1313	. 

In [10088]: m
array([[1, 2],
       [3, 4]])

In [10089]: x
array([[2],
       [3]])


/ elem gewijs verm zolang het past	,

/ omdat x vector, verm ieder elem van x met rij in m	,
In [10079]: x*m
/=
In [10079]: m*x
/=
In [10084]: np.diag(x.ravel()).dot(m)
Out[10079]: 
array([[ 2,  4],
       [ 9, 12]])

/ verm ieder elem van x met kolom van m	,
In [10081]: x.T*m
/=
In [10085]: m*x.T
/=
In [10086]: m.dot(np.diag(x.ravel()))
array([[ 2,  6],
       [ 6, 12]])

/ 7	 

/ np.eye(3) maakt een diagonaal matrix met 1 op hoofddiagonaal	,
/ maar we kunnen hem ook zo maken: elem=int(i==j) 
/ dit is NIET OK	,
In [10138]: a=np.array([0,1,2]).reshape(-1,1)
In [10139]: a==a
Out[10139]: 
array([[ True],
       [ True],
       [ True]])

/ 7	

/ maak van 
n [10180]: m
Out[10180]: 
array([[1, 2],
       [3, 4]])

(1 0	.dot (1 0	(2 0
 0 3)				0 3) 0 4)
(2 0
 0 4)

/ lees	,
https://cmdlinetips.com/2018/04/how-to-concatenate-arrays-in-numpy/

In [10191]: n=np.concatenate((np.diag(m[:,0]),np.diag(m[:,1])))
/ of	,
In [10192]: n=np.vstack((np.diag(m[:,0]),np.diag(m[:,1])))
Out[10192]: 
array([[1, 0],
       [0, 3],
       [2, 0],
       [0, 4]])
/ let op extra ()

/ of	,
In [10213]: n=np.r_[np.diag(m[:,0]),np.diag(m[:,1])]
       ...: 
       ...: 
Out[10213]: 
array([[1, 0],
       [0, 3],
       [2, 0],
       [0, 4]])


In [10196]: n.dot(n.T)
Out[10196]: 
array([[ 1,  0,  2,  0],
       [ 0,  9,  0, 12],
       [ 2,  0,  4,  0],
       [ 0, 12,  0, 16]])

/ 13	. 


In [10220]: s=np.diag(np.r_[m[:,[0]],m[:,[1]]].ravel())
Out[10220]: 
array([[1, 0, 0, 0],
       [0, 3, 0, 0],
       [0, 0, 2, 0],
       [0, 0, 0, 4]])

In [10222]: s-n.dot(n.T)
Out[10222]: 
array([[  0,   0,  -2,   0],
       [  0,  -6,   0, -12],
       [ -2,   0,  -2,   0],
       [  0, -12,   0, -12]])


/ 	7

/ bij np.r_ moet je []	, dus np.r_[m,n]
/ bij np.block moet je ook ()	, dus np.block([[m,n],[o,p]])

np.block([[m,n],[o,p]])

/ 7	. 

/ let p () in:

In [10283]: t[(t==0)|(t==2)]
Out[10283]: 
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])









/ BISHOP (209)

/ 3 classes	, 2 features	,

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((len(X),1)),X]
t=iris['target']
T=np.c_[t==0,t==1,t==2].astype(int)
mu0=np.zeros(3).reshape(-1,1)
mu1=np.zeros(3).reshape(-1,1)
mu2=np.zeros(3).reshape(-1,1)
mu=np.r_[mu0,mu1,mu2]

#for i in range(0,10):
mu0=mu[:3].reshape(-1,1)
mu1=mu[3:6].reshape(-1,1)
mu2=mu[6:9].reshape(-1,1)
M=np.c_[np.exp(X.dot(mu0)),np.exp(X.dot(mu1)),np.exp(X.dot(mu2))]
S=(M[:,0]+M[:,1]+M[:,2]).reshape(-1,1)
Y=M/S # (150,3) Y[n,k]=p(k|xn)
J=np.ones(len(X)).reshape(1,-1).dot(T*Y).dot(np.ones(len(T[0])))
# TODO print bij verschillende mu0,mu1,mu2 J	, en maak contourlijnen	,
GradJ=X.T.dot(Y-T)  # (3,3) 
HessianJ= np.block([
	[X.T.dot(np.diag(Y[:,0])).dot(np.diag(1-Y[:,0])).dot(X),
		X.T.dot(np.diag(Y[:,0])).dot(np.diag(-Y[:,1])).dot(X),
		X.T.dot(np.diag(Y[:,0])).dot(np.diag(-Y[:,2])).dot(X)],
	[X.T.dot(np.diag(Y[:,1])).dot(np.diag(-Y[:,0])).dot(X),
		X.T.dot(np.diag(Y[:,1])).dot(np.diag(1-Y[:,1])).dot(X),
		X.T.dot(np.diag(Y[:,1])).dot(np.diag(-Y[:,2])).dot(X)],
	[X.T.dot(np.diag(Y[:,2])).dot(np.diag(-Y[:,0])).dot(X),
		X.T.dot(np.diag(Y[:,2])).dot(np.diag(-Y[:,1])).dot(X),
		X.T.dot(np.diag(Y[:,2])).dot(np.diag(1-Y[:,2])).dot(X)],
])
mu=mu-np.linalg.inv(HessianJ).dot(GradJ)


/ 2 classes	, 2 features,	

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((len(X),1)),X]
t=iris['target']
T=np.c_[t==1,(t==0)|(t==2)].astype(int)
mu1=np.zeros(3).reshape(-1,1)
mu0=np.zeros(3).reshape(-1,1)
mu=np.r_[mu1,mu0]

#for i in range(0,10):
mu1=mu[:3].reshape(-1,1)
mu0=mu[3:6].reshape(-1,1)
M=np.c_[np.exp(X.dot(mu1)),np.exp(X.dot(mu0))]
S=(M[:,0]+M[:,1]).reshape(-1,1)
Y=M/S # (150,3) Y[n,k]=p(k|xn)
J=np.ones(len(X)).reshape(1,-1).dot(T*Y).dot(np.ones(len(T[0])))
# TODO print bij verschillende mu0,mu1,mu2 J	, en maak contourlijnen	,
GradJ=X.T.dot(Y-T)  # (3,3) 
HessianJ= np.block([
	[X.T.dot(np.diag(Y[:,0])).dot(np.diag(1-Y[:,0])).dot(X),
		X.T.dot(np.diag(Y[:,0])).dot(np.diag(-Y[:,1])).dot(X),],
	[X.T.dot(np.diag(Y[:,1])).dot(np.diag(-Y[:,0])).dot(X),
		X.T.dot(np.diag(Y[:,1])).dot(np.diag(1-Y[:,1])).dot(X),],
])
mu=mu-np.linalg.inv(HessianJ).dot(GradJ)


/ LAATSTE VERSIE 3 CLASSES

/ 3 classes	, 2 features	,

/ 7	.

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((len(X),1)),X]
t=iris['target']
T=np.c_[t==0,t==1,t==2].astype(int)
psi=np.zeros(3).reshape(-1,1)
theta=np.zeros(3).reshape(-1,1)
psitheta=np.r_[psi,theta]

for i in range(0,10):
	psi=psitheta[:3].reshape(-1,1)
	theta=psitheta[3:6].reshape(-1,1)
	M=np.c_[np.exp(-X.dot(psi)),np.ones(len(X)).reshape(-1,1),np.exp(-X.dot(theta))]
	S=(M[:,0]+M[:,1]+M[:,2]).reshape(-1,1)
	Y=M/S # (150,3) Y[n,k]=p(k|xn)
	#J=np.ones(len(X)).reshape(1,-1).dot(T*-np.log(Y)).dot(np.ones(len(T[0])))
	# TODO print bij verschillende psi,theta J	, en maak contourlijnen	,
	GradJ=X.T.dot((T-Y)[:,[0,2]])  
	gradJ=np.r_[GradJ[:,[0]],GradJ[:,[1]]]
	HessianJ= np.block([
		[X.T.dot(np.diag(Y[:,0])).dot(np.diag(1-Y[:,0])).dot(X),
			X.T.dot(np.diag(-Y[:,0])).dot(np.diag(Y[:,2])).dot(X)],
		[X.T.dot(-np.diag(-Y[:,2])).dot(np.diag(Y[:,0])).dot(X),
			X.T.dot(np.diag(Y[:,2])).dot(np.diag(1-Y[:,2])).dot(X)],
	])
	psitheta=psitheta-np.linalg.inv(HessianJ).dot(gradJ)

psi=psitheta[:3].reshape(-1,1)
theta=psitheta[3:6].reshape(-1,1)
M=np.c_[np.exp(-X.dot(psi)),np.ones(len(X)).reshape(-1,1),np.exp(-X.dot(theta))]
S=(M[:,0]+M[:,1]+M[:,2]).reshape(-1,1)
Y=M/S # (150,3) Y[n,k]=p(k|xn)
#J=np.ones(len(X)).reshape(1,-1).dot(T*-np.log(Y)).dot(np.ones(len(T[0])))
# TODO print bij verschillende psi,theta J	, en maak contourlijnen	,
GradJ=X.T.dot((T-Y)[:,[0,2]])  
gradJ=np.r_[GradJ[:,[0]],GradJ[:,[1]]]
HessianJ= np.block([
	[X.T.dot(np.diag(Y[:,0])).dot(np.diag(1-Y[:,0])).dot(X),
		X.T.dot(np.diag(-Y[:,0])).dot(np.diag(Y[:,2])).dot(X)],
	[X.T.dot(np.diag(-Y[:,2])).dot(np.diag(Y[:,0])).dot(X),
		X.T.dot(np.diag(Y[:,2])).dot(np.diag(1-Y[:,2])).dot(X)],
])
psitheta=psitheta-np.linalg.inv(HessianJ).dot(gradJ)

/ 7	. 

X=iris['data'][:,[2,3]][[0,50,100]]
X=np.c_[np.ones((X.shape[0],1)),X]
t=iris['target'][[0,50,100]].reshape(-1,1)
T=np.c_[t==0,t==1,t==2].astype(int)

s0=np.linspace(-1,1,3)
s1=np.linspace(-1,1,3)
s2=np.linspace(-1,1,3)
t0=np.linspace(-1,1,3)
t1=np.linspace(-1,1,3)
t2=np.linspace(-1,1,3)
S0,S1,S2,T0,T1,T2=np.meshgrid(s0,s1,s2,t0,t1,t2)
G=np.c_[S0.ravel(),S1.ravel(),S2.ravel(),T0.ravel(),T1.ravel(),T2.ravel()]

psi=np.c_[s0.ravel(),s1.ravel(),s2.ravel()][0:3]
theta=np.c_[t0.ravel(),t1.ravel(),t2.ravel()][0:3]
M=np.c_[np.exp(-X.dot(psi.T)),np.ones(X.shape[0]),np.exp(-X.dot(theta.T))]
S=M.dot(np.ones(M.shape[1]).reshape(-1,1))
# TODO bij .dot moeten we .reshape doen op np.ones	, bij np.c_ niet 
Y=M/S # (150,3) Y[n,k]=p(k|xn)
J=np.ones(len(X)).reshape(1,-1).dot(T*-np.log(Y)).dot(np.ones(len(T[0])))
Res=np.c_[psi,theta,J]

/ controle,	
In [10677]: Y.dot(np.ones(Y.shape[1]).reshape(-1,1))
Out[10677]: 
array([[1.],
       [1.],
       [1.]])

/ 7	. 

s0,s1,t0,t1= np.meshgrid(
        np.linspace(-1,1,3),
        np.linspace(-1,1,3),
        np.linspace(-1,1,3),
        np.linspace(-1,1,3),
)
s0,s1= np.meshgrid(
        np.linspace(0,1,2),
        np.linspace(0,1,2),
)

/ 7	. 

/ OK	,

i=np.linspace(0,1,2)
j=np.linspace(0,1,2)
I,J=np.meshgrid(i,j)
G=np.c_[J.ravel(),I.ravel()]

In [11718]: i
Out[11718]: array([0., 1.])

In [11719]: j
Out[11719]: array([0., 1.])

In [11720]: I
Out[11720]: 
array([[0., 1.],
       [0., 1.]])

In [11721]: J
Out[11721]: 
array([[0., 0.],
       [1., 1.]])

In [11722]: G
Out[11722]: 
array([[0., 0.],
       [1., 0.],
       [0., 1.],
       [1., 1.]])


/ 7	. 

/ we zien dit fout gaan	,

s0,s1= np.meshgrid(
	np.linspace(0,1,2),
	np.linspace(0,1,2),
)
In [11702]: s0
Out[11702]: 
array([[0., 1.],
       [0., 1.]])

In [11703]: s1
Out[11703]: 
array([[0., 0.],
       [1., 1.]])

/ Dit is ERR	,
In [11706]: np.array([s0.ravel(),s1.ravel()])
Out[11706]: 
array([[0., 1., 0., 1.],
       [0., 0., 1., 1.]])

In [11707]: np.array([s0.ravel(),s1.ravel()]).reshape(-1,2)
Out[11707]: 
array([[0., 1.],
       [0., 1.],
       [0., 0.],
       [1., 1.]])

/ 7	. 

In [11743]: print??
Signature: print()
Docstring: <no docstring>
Source:   
def print():

  plt.figure()
  plt.plot(x[y==0],y[y==0],'bs')
  plt.plot(x[y==1],y[y==1],'g^')

  intval=np.linspace(min(x),max(x),11)
  Intval=np.c_[np.ones(len(intval)),intval]
  vals=expit(Intval.dot(theta))
  plt.plot(intval,vals)
File:      ~/Devel/python/geron/handson-ml/<ipython-input-7151-224684f4a9b6>
Type:      function
/ TODO

In [11744]: del print

In [11746]: for i,j in G:
       ...:     print(i,j)
       ...:     
0.0 0.0
0.0 1.0
1.0 0.0
1.0 1.0

/ of	,

In [11747]: a=np.zeros((4,2))
In [11747]: k=0              
In [11747]: for i,j in G:    
       ...:     a[k]=(i,j)
       ...:     k=k+1
       ...:     
In [11751]: a
Out[11751]: 
array([[0., 0.],
       [0., 1.],
       [1., 0.],
       [1., 1.]])

/ 7	.

s0=np.linspace(-1,1,3)
s1=np.linspace(-1,1,3)
s2=np.linspace(-1,1,3)
t0=np.linspace(-1,1,3)
t1=np.linspace(-1,1,3)
t2=np.linspace(-1,1,3)
S0,S1,S2,T0,T1,T2=np.meshgrid(s0,s1,s2,t0,t1,t2,indexing='ij')
G=np.c_[S0.ravel(),S1.ravel(),S2.ravel(),T0.ravel(),T1.ravel(),T2.ravel()]

for i in np.arange(G.shape[0]):
	psi=G[i][:3]
	theta=G[i][-3:]
	print(psi,theta)

[-1. -1. -1.] [-1. -1. -1.]
[-1. -1. -1.] [-1. -1.  0.]
[-1. -1. -1.] [-1. -1.  1.]
[-1. -1. -1.] [-1.  0. -1.]
[-1. -1. -1.] [-1.  0.  0.]
[-1. -1. -1.] [-1.  0.  1.]
[-1. -1. -1.] [-1.  1. -1.]
[-1. -1. -1.] [-1.  1.  0.]
[-1. -1. -1.] [-1.  1.  1.]
[-1. -1. -1.] [ 0. -1. -1.]
[-1. -1. -1.] [ 0. -1.  0.]
[-1. -1. -1.] [ 0. -1.  1.]
[-1. -1. -1.] [ 0.  0. -1.]
[-1. -1. -1.] [0. 0. 0.]
[-1. -1. -1.] [0. 0. 1.]
[-1. -1. -1.] [ 0.  1. -1.]
[-1. -1. -1.] [0. 1. 0.]
[-1. -1. -1.] [0. 1. 1.]
[-1. -1. -1.] [ 1. -1. -1.]
[-1. -1. -1.] [ 1. -1.  0.]
[-1. -1. -1.] [ 1. -1.  1.]
[-1. -1. -1.] [ 1.  0. -1.]
[-1. -1. -1.] [1. 0. 0.]
[-1. -1. -1.] [1. 0. 1.]
[-1. -1. -1.] [ 1.  1. -1.]
[-1. -1. -1.] [1. 1. 0.]
[-1. -1. -1.] [1. 1. 1.]
[-1. -1.  0.] [-1. -1. -1.]
[-1. -1.  0.] [-1. -1.  0.]
[-1. -1.  0.] [-1. -1.  1.]
[-1. -1.  0.] [-1.  0. -1.]
[-1. -1.  0.] [-1.  0.  0.]
[-1. -1.  0.] [-1.  0.  1.]
[-1. -1.  0.] [-1.  1. -1.]
[-1. -1.  0.] [-1.  1.  0.]
...


/ 7	. 

/ 1313	. 

s0=np.linspace(0,1,2)
s1=np.linspace(0,1,2)
t0=np.linspace(0,1,2)
t1=np.linspace(0,1,2)
S0,S1,T0,T1=np.meshgrid(s0,s1,t0,t1,indexing='ij')
G=np.c_[S0.ravel(),S1.ravel(),T0.ravel(),T1.ravel()]

In [11775]: G
Out[11775]: 
array([[0., 0., 0., 0.],
       [0., 0., 0., 1.],
       [0., 0., 1., 0.],
       [0., 0., 1., 1.],
       [0., 1., 0., 0.],
       [0., 1., 0., 1.],
       [0., 1., 1., 0.],
       [0., 1., 1., 1.],
       [1., 0., 0., 0.],
       [1., 0., 0., 1.],
       [1., 0., 1., 0.],
       [1., 0., 1., 1.],
       [1., 1., 0., 0.],
       [1., 1., 0., 1.],
       [1., 1., 1., 0.],
       [1., 1., 1., 1.]])

for i in np.arange(G.shape[0]):
	psi=G[i][:2]
	theta=G[i][-2:]
	print(psi,theta)

[0. 0.] [0. 0.]
[0. 0.] [0. 1.]
[0. 0.] [1. 0.]
[0. 0.] [1. 1.]
[1. 0.] [0. 0.]
[1. 0.] [0. 1.]
[1. 0.] [1. 0.]
[1. 0.] [1. 1.]
[0. 1.] [0. 0.]
[0. 1.] [0. 1.]
[0. 1.] [1. 0.]
[0. 1.] [1. 1.]
[1. 1.] [0. 0.]
[1. 1.] [0. 1.]
[1. 1.] [1. 0.]
[1. 1.] [1. 1.]


/ 1313	. 

In [11779]: s0=np.linspace(0,1,2)
       ...: s1=np.linspace(0,1,2)
       ...: t0=np.linspace(0,1,2)
       ...: t1=np.linspace(0,1,2)
       ...: S0,S1,T0,T1=np.meshgrid(s0,s1,t0,t1,indexing='xy')
       ...: G=np.c_[S0.ravel(),S1.ravel(),T0.ravel(),T1.ravel()]


In [11778]: G
Out[11778]: 
array([[0., 0., 0., 0.],
       [0., 0., 0., 1.],
       [0., 0., 1., 0.],
       [0., 0., 1., 1.],
       [1., 0., 0., 0.],
       [1., 0., 0., 1.],
       [1., 0., 1., 0.],
       [1., 0., 1., 1.],
       [0., 1., 0., 0.],
       [0., 1., 0., 1.],
       [0., 1., 1., 0.],
       [0., 1., 1., 1.],
       [1., 1., 0., 0.],
       [1., 1., 0., 1.],
       [1., 1., 1., 0.],
       [1., 1., 1., 1.]])

/ 1313	. 






/ Einde LAATSTE VERSIE 3 CLASSES

/ Einde BISHOP (209)

/ BISHOP (209) 

/ multiclass log reg

/ 13	. 

In [8454]: a=np.array([1,2,3,4])
In [8455]: A=a.reshape(-1,1)
In [8456]: a.shape
Out[8456]: (4,)
In [8457]: A.shape
Out[8457]: (4, 1)
In [8462]: a
Out[8462]: array([1, 2, 3, 4])
In [8463]: A
Out[8463]: 
array([[1],
       [2],
       [3],
       [4]])

In [8458]: b=np.array([4,3,2,1])
In [8459]: B=b.reshape(-1,1)
In [8460]: b
Out[8460]: array([4, 3, 2, 1])
In [8461]: B
Out[8461]: 
array([[4],
       [3],
       [2],
       [1]])


In [8464]: A+B
Out[8464]: 
array([[5],
       [5],
       [5],
       [5]])

In [8465]: A/(A+B)
Out[8465]: 
array([[0.2],
       [0.4],
       [0.6],
       [0.8]])

In [8466]: a/(a+b)
Out[8466]: array([0.2, 0.4, 0.6, 0.8])

/ 13	. 

X=iris['data'][:,[3]]
X=np.c_[np.ones((len(X),1)),X]
y=(iris['target']==2).reshape(-1,1).astype(int)

theta1=np.array([1,0])
Theta1=theta1.reshape(-1,1)
theta2=np.array([0,1])
Theta2=theta2.reshape(-1,1)
Theta=np.r_[Theta1,Theta2]

In [8481]: X.dot(theta1).shape
Out[8481]: (150,)
In [8482]: X.shape
Out[8482]: (150, 2)

X.dot(Theta1)/(X.dot(Theta1)+X.dot(Theta2))
/ shape=(150,1)

/ y is vector met 1 en 0	, 1 betekent: iris['target']=2	,
/ moeten we omzetten naar 1-of-K coding	,
T=np.c_[1-y,y]

/ of	,
# T=np.c_[y,1-y]

/ print	,
In [8501]: T[:5]
/ of	,
In [8501]: T[-5:]

V1=np.exp(X.dot(Theta1))/(np.exp(X.dot(Theta1))+np.exp(X.dot(Theta2)))
V2=np.exp(X.dot(Theta2))/(np.exp(X.dot(Theta1))+np.exp(X.dot(Theta2)))
grad1J=X.T.dot(V1-T[:,[0]])                                     
grad2J=X.T.dot(V2-T[:,[1]])                                     
gradJ=np.r_[grad1J,grad2J]

In [8585]: V1.shape
Out[8585]: (150, 1)
In [8586]: V2.shape
Out[8586]: (150, 1)
In [8587]: grad1J.shape
Out[8587]: (2, 1)
In [8588]: grad2J.shape
Out[8588]: (2, 1)

/  verschil Theta1 en theta1	,
In [8537]: r=X.dot(theta1)/(X.dot(theta1)+X.dot(theta2))-T[:,0]
/ r.shape=(150,)

H11=X.T.dot(np.diag((V1*(1-V1)).ravel())).dot(X)
H12=X.T.dot(np.diag(-(V1*V2).ravel())).dot(X)
H21=X.T.dot(np.diag(-(V2*V1).ravel())).dot(X)
H22=X.T.dot(np.diag((V2*(1-V2)).ravel())).dot(X)

H=np.block([[H11,H12],[H21,H22]])

Theta=Theta-np.linalg.inv(H).dot(gradJ)

/ 13	. 

/ alles bij elkaar, precies hetzelfde als de 13 hierboven	,


/ init	,

X=iris['data'][:,[3]]
X=np.c_[np.ones((len(X),1)),X]
y=(iris['target']==2).reshape(-1,1).astype(int)

theta1=np.array([1,2])
Theta1=theta1.reshape(-1,1)
theta2=np.array([0,1])
Theta2=theta2.reshape(-1,1)
Theta=np.r_[Theta1,Theta2]

/ loop	,

for i in range(0,10):
	V1=np.exp(X.dot(Theta1))/(np.exp(X.dot(Theta1))+np.exp(X.dot(Theta2)))
	V2=np.exp(X.dot(Theta2))/(np.exp(X.dot(Theta1))+np.exp(X.dot(Theta2)))
	grad1J=X.T.dot(V1-T[:,[0]])                                     
	grad2J=X.T.dot(V2-T[:,[1]])                                     
	gradJ=np.r_[grad1J,grad2J]
	
	H11=X.T.dot(np.diag((V1*(1-V1)).ravel())).dot(X)
	H12=X.T.dot(np.diag(-(V1*V2).ravel())).dot(X)
	H21=X.T.dot(np.diag(-(V2*V1).ravel())).dot(X)
	H22=X.T.dot(np.diag((V2*(1-V2)).ravel())).dot(X)
	
	H=np.block([[H11,H12],[H21,H22]])
	
	Theta=Theta-np.linalg.inv(H).dot(gradJ)

/ doe het	,

In [8712]: Theta
Out[8712]: 
array([[19.2654514 ],
       [-7.90724373],
       [-6.06130297],
       [ 3.70362186]])

In [8716]: Theta[1][0]-Theta[3][0]
Out[8716]: -11.610865592785048

In [8717]: Theta[0][0]-Theta[2][0]
Out[8717]: 25.32675436427709


/ log reg	, 2 classes met sigmoid	,
In [7594]: theta
Out[7594]: 
array([[-21.12563996],
       [ 12.94750716]])

/ 13	. 

/ newton 

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((len(X),1)),X]
#y=iris['target']
#of ,
y=np.array(iris['target']).reshape(-1,1)
T=np.c_[y==0,y==1,y==2].astype(int)
theta12=np.array([0,0,0])
Theta12=theta12.reshape(-1,1)
theta13=np.array([0,0,0])
Theta13=theta13.reshape(-1,1)
Theta=np.r_[Theta12,Theta13]
for i in range(0,10):
  p1givenX=1/(1+np.exp(-X.dot(Theta12))+np.exp(-X.dot(Theta13)))
  V12=np.exp(-X.dot(Theta12))/p1givenX
  V13=np.exp(-X.dot(Theta13))/p1givenX
  grad12J=X.T.dot(T[:,[1]]-V12)
  grad13J=X.T.dot(T[:,[2]]-V13)
  grad1213J=np.r_[grad12J,grad13J]
  H1212=X.T.dot(np.diag((V12*(1-V12)).ravel())).dot(X)
  H1312=X.T.dot(np.diag((-V12*V13).ravel())).dot(X)
  H1213=X.T.dot(np.diag((-V13*V12).ravel())).dot(X)
  H1313=X.T.dot(np.diag((V13*(1-V13)).ravel())).dot(X)
  H_=np.block([[H1212,H1312],[H1213,H1313]])
  Theta=Theta-np.linalg.inv(H_).dot(grad1213J)

/ 13	. 

/ newton 

/ HIER HIER HIER

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((len(X),1)),X]
y=iris['target']
#of ,
#y=np.array(iris['target']).reshape(-1,1)
#T=np.c_[y==0,y==1,y==2].astype(int)
theta10=np.array([0,0,0])
Theta10=theta10.reshape(-1,1)
theta12=np.array([0,0,0])
Theta12=theta12.reshape(-1,1)
Theta=np.r_[Theta10,Theta12]
lmbda=1
for i in range(0,10):
  p1gX=1/(np.exp(-X.dot(Theta10))+1+np.exp(-X.dot(Theta12)))
  V10=np.exp(-X.dot(Theta10))*p1gX
  V12=np.exp(-X.dot(Theta12))*p1gX
  grad10J=X.T.dot((y==0).reshape(-1,1).astype(int)-V10)+lmbda*Theta10
  grad12J=X.T.dot((y==2).reshape(-1,1).astype(int)-V12)+lmbda*Theta12
  grad1012J=np.r_[grad10J,grad12J]
  H1010=X.T.dot(np.diag((V10*(1-V10)).ravel())).dot(X)+lmbda*np.diag(np.ones(len(theta10)))
  H1210=X.T.dot(np.diag((-V10*V12).ravel())).dot(X)
  H1012=X.T.dot(np.diag((-V12*V10).ravel())).dot(X)
  H1212=X.T.dot(np.diag((V12*(1-V12)).ravel())).dot(X)+lmbda*np.diag(np.ones(len(theta10)))
  H_=np.block([[H1010,H1210],[H1012,H1212]])
  Theta=Theta-np.linalg.inv(H_).dot(grad1012J)

/ HIER HIER HIER


/ 13	. 

/ lees	,
http://localhost:8888/notebooks/04_training_linear_models.ipynb

/ voorbeeld 2 classes,	 2 features	,

X = iris["data"][:, (2, 3)]  # petal length, petal width
y = (iris["target"] == 2).astype(np.int)

lg=LogisticRegression(C=10**10)
lg.fit(X,y)


x0, x1 = np.meshgrid(
        np.linspace(2.9, 7, 500).reshape(-1, 1),
        np.linspace(0.8, 2.7, 200).reshape(-1, 1),
)
X_new = np.c_[x0.ravel(), x1.ravel()]

In [9166]: x0
Out[9166]: 
array([[2.9       , 2.90821643, 2.91643287, ..., 6.98356713, 6.99178357,
        7.        ],
       [2.9       , 2.90821643, 2.91643287, ..., 6.98356713, 6.99178357,
        7.        ],
       [2.9       , 2.90821643, 2.91643287, ..., 6.98356713, 6.99178357,
        7.        ],
       ...,
       [2.9       , 2.90821643, 2.91643287, ..., 6.98356713, 6.99178357,
        7.        ],
       [2.9       , 2.90821643, 2.91643287, ..., 6.98356713, 6.99178357,
        7.        ],
       [2.9       , 2.90821643, 2.91643287, ..., 6.98356713, 6.99178357,
        7.        ]])
In [9167]: x1
Out[9167]: 
array([[0.8       , 0.8       , 0.8       , ..., 0.8       , 0.8       ,
        0.8       ],
       [0.80954774, 0.80954774, 0.80954774, ..., 0.80954774, 0.80954774,
        0.80954774],
       [0.81909548, 0.81909548, 0.81909548, ..., 0.81909548, 0.81909548,
        0.81909548],
       ...,
       [2.68090452, 2.68090452, 2.68090452, ..., 2.68090452, 2.68090452,
        2.68090452],
       [2.69045226, 2.69045226, 2.69045226, ..., 2.69045226, 2.69045226,
        2.69045226],
       [2.7       , 2.7       , 2.7       , ..., 2.7       , 2.7       ,
        2.7       ]])

In [9168]: X_new
Out[9168]: 
array([[2.9       , 0.8       ],
       [2.90821643, 0.8       ],
       [2.91643287, 0.8       ],
       ...,
       [6.98356713, 2.7       ],
       [6.99178357, 2.7       ],
       [7.        , 2.7       ]])

In [9172]: X_new.shape
Out[9172]: (100000, 2)

# p(1|(x0,x1))
y_proba = lg.predict_proba(X_new)

In [9173]: y_proba[50250]
Out[9173]: array([0.17256704, 0.82743296])
In [9174]: y_proba[50250][0]+y_proba[50250][1]
Out[9174]: 1.0


In [9161]: y_predict = log_reg.predict(X_new)
In [9162]: y_predict
Out[9162]: array([0, 0, 0, ..., 1, 1, 1])
In [9163]: y_predict[50250]
Out[9163]: 1

# p(1|(x0,x1)) in rooster vorm	, zoals x0 en x1 dat zijn	,
zz1 = y_proba[:, 1].reshape(x0.shape)

# de uitkomst 0 of 1 in (x0,x1) al na gelang welke groter is: 
zz = y_predict.reshape(x0.shape)

/ 13	. 

/ lees	,
http://localhost:8888/notebooks/04_training_linear_models.ipynb

/ 3 classes, 2 features	,



X=iris['data'][:,[2,3]]
y = iris["target"]
lg=LogisticRegression(C=10**10)
lg.fit(X,y)

/ grotere grid	als in vorig voorbeeld	,

x0, x1 = np.meshgrid(
        np.linspace(0,8, 500).reshape(-1, 1),
        np.linspace(0,3.5, 200).reshape(-1, 1),
)
X_new = np.c_[x0.ravel(), x1.ravel()]
y_proba = lg.predict_proba(X_new)
y_predict = lg.predict(X_new)

(.29<=y_proba[:,0]) & (y_proba[:,0]<=.31) & (.29<=y_proba[:,1]) & (y_proba[:,1]<=.31) & (.29<=y_proba[:,2]) & (y_proba[:,2]<=.31)         
/ OK	,
/ maar te strict , er zijn geen punten in X_new die hieraan voldoen	,
/ daarom doen we	,
np.where((.25<=y_proba[:,0]) & (y_proba[:,0]<=.35) & (.25<=y_proba[:,1]) & (y_proba[:,1]<=.35) & (.25<=y_proba[:,2]) & (y_proba[:,2]<=.35))
Out[9253]: (array([80560, 81058]),)

/ er zijn dus 2 punten in X_new	, met index 80560, 81058
In [9258]: X_new[80560]
Out[9258]: array([0.96192385, 2.83165829])
In [9259]: X_new[81058]
Out[9259]: array([0.92985972, 2.84924623])

In [9254]: y_proba[80560]
Out[9254]: array([0.32889255, 0.33255792, 0.33854952])
In [9260]: y_predict[80560]
Out[9260]: 2
In [9255]: y_proba[80560][0]+y_proba[80560][1]+y_proba[80560][2]
Out[9255]: 1.0

In [9256]: y_proba[81058]
Out[9256]: array([0.34949538, 0.30554656, 0.34495806])
In [9261]: y_predict[81058]
Out[9261]: 0
In [9257]: y_proba[81058][0]+y_proba[81058][1]+y_proba[81058][2]
Out[9257]: 1.0

/ 1313	. 

/ hij laat 2 contours zien: 

zz1 = y_proba[:, 1].reshape(x0.shape)
zz = y_predict.reshape(x0.shape)

from matplotlib.colors import ListedColormap
custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])

plt.contourf(x0, x1, zz, cmap=custom_cmap)
contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)
plt.clabel(contour, inline=1, fontsize=12)

/ de 3 classes toont hij met 3 kleuren	, en de niveaus van de middelste klasse toont hij met niveaulijnen	,

/ 1313	. 

/ we gaan de grafiek ook tekenen	,

X=iris['data'][:,[2,3]]
y = iris["target"]
lg=LogisticRegression(C=10**10)
lg=LogisticRegression(multi_class="multinomial",solver="lbfgs",C=10) # C=10 !
lg=LogisticRegression(C=10)

lg.fit(X,y)

x0, x1 = np.meshgrid(
        np.linspace(0,8, 500).reshape(-1, 1),
        np.linspace(0,3.5, 200).reshape(-1, 1),
)
X_new = np.c_[x0.ravel(), x1.ravel()]
y_proba = lg.predict_proba(X_new)
y_predict = lg.predict(X_new)

prb0=y_proba[:,0].reshape(x0.shape)
prb1=y_proba[:,1].reshape(x0.shape)
prb2=y_proba[:,2].reshape(x0.shape)
est=y_predict.reshape(x0.shape)

# print training data	,
plt.figure()
plt.plot(X[y==2,0],X[y==2,1],"g^",label="Iris-Virginia")
plt.plot(X[y==1,0],X[y==1,1],"bs",label="Iris-Versicolor")
plt.plot(X[y==0,0],X[y==0,1],"yo",label="Iris-Setosa")

from matplotlib.colors import ListedColormap
custom_cmap=ListedColormap(['#fafab0','#9898ff','#a0faa0'])
plt.contourf(x0,x1,est,cmap=custom_cmap)

countour1=plt.contour(x0,x1,prb1,cmap=plt.cm.brg)
countour0=plt.contour(x0,x1,prb0,cmap=plt.cm.brg)
countour2=plt.contour(x0,x1,prb2,cmap=plt.cm.brg)

plt.clabel(countour1,inline=1,fontsize=12)

/ deze 2 zijn minder onderscheidend	, als we de nivolijnen tekenen van class 1, lopen die door de waarnemingen met classes 0 en 2 heen	, 
lg=LogisticRegression(C=10**10)
lg=LogisticRegression(C=10)

/ deze is meer onderscheidend	,
lg=LogisticRegression(multi_class="multinomial",solver="lbfgs",C=10) # C=10 !
/ maar met C=10**10 is het hele gebied totaal anders, 	we zien 3 schuine gebieden	, het is misschien zo dat het 3-vlakken punt buiten het gebied ligt, linksboven	,
/ TODO

/ we kunnen ook nivolijnen tekenen voor classes 0 en 2	, deze gebieden zijn minder scherp, dus ze lopen ook langs hun grenzen net als bij class 1, maar maken niet zo'n bocht	,

/ 13	. 

/ we gaan terug naar 2 classes, 1 feature	, 

/ 1313	. 

/ self, 

x=iris['data'][:,[3]]
X=np.c_[np.ones((len(x),1)),x]
theta=np.array([0,0]).reshape(-1,1)
y=(iris['target']==2).reshape(-1,1).astype(int)

for i in range(0,10):
  z=X.dot(theta)
  a=expit(z)
  grad=X.T.dot(a-y)
  D=np.diag((a*(1-a)).ravel())
  H=X.T.dot(D).dot(X)
  theta=theta-np.linalg.inv(H).dot(grad)

In [9370]: theta
Out[9370]: 
array([[-21.12563996],
       [ 12.94750716]])

p(1|1.5)
In [9382]: expit(np.c_[1,1.5].dot(theta))
Out[9382]: array([[0.15389418]])

p(0|1.5)
In [9383]: 1-expit(np.c_[1,1.5].dot(theta))
/ of	,
In [9384]: expit(-np.c_[1,1.5].dot(theta))
Out[9384]: array([[0.84610582]])

/ p(1|1.5)<p(0|1.5) -> pred t=0

p(1|1.6)
In [9385]: expit(np.c_[1,1.6].dot(theta))
Out[9385]: array([[0.3990012]])

p(0|1.6)
In [9386]: 1-expit(np.c_[1,1.6].dot(theta))
/ of	,
In [9387]: expit(-np.c_[1,1.6].dot(theta))
Out[9387]: array([[0.6009988]])

/ p(1|1.6)>p(0|1.6) -> pred t=1


/ 1313	. 

/ met sklearn	,

X=iris['data'][:,[3]]
y=(iris['target']==2).astype(int)
lg=LogisticRegression(C=10**10)
lg.fit(X,y)

X_=np.linspace(0.1,2.5,25).reshape(-1,1)
p1gX=lg.predict_proba(X_)
pr=lg.predict(X_.reshape(-1,1))


In [9368]: np.c_[X_,p1gX,pr]
Out[9368]: 
				x								p(0|x)					p(1|x)					pred t	,
array([[1.00000000e-01, 9.99999998e-01, 2.44100157e-09, 0.00000000e+00],
       [2.00000000e-01, 9.99999991e-01, 8.90984641e-09, 0.00000000e+00],
       [3.00000000e-01, 9.99999967e-01, 3.25216349e-08, 0.00000000e+00],
       [4.00000000e-01, 9.99999881e-01, 1.18706499e-07, 0.00000000e+00],
       [5.00000000e-01, 9.99999567e-01, 4.33287861e-07, 0.00000000e+00],
       [6.00000000e-01, 9.99998418e-01, 1.58153274e-06, 0.00000000e+00],
       [7.00000000e-01, 9.99994227e-01, 5.77269389e-06, 0.00000000e+00],
       [8.00000000e-01, 9.99978930e-01, 2.10704615e-05, 0.00000000e+00],
       [9.00000000e-01, 9.99923095e-01, 7.69045368e-05, 0.00000000e+00],
       [1.00000000e+00, 9.99719350e-01, 2.80650372e-04, 0.00000000e+00],
       [1.10000000e+00, 9.98976366e-01, 1.02363447e-03, 0.00000000e+00],
       [1.20000000e+00, 9.96273762e-01, 3.72623759e-03, 0.00000000e+00],
       [1.30000000e+00, 9.86531936e-01, 1.34680637e-02, 0.00000000e+00],
       [1.40000000e+00, 9.52534622e-01, 4.74653781e-02, 0.00000000e+00],
       [1.50000000e+00, 8.46105573e-01, 1.53894427e-01, 0.00000000e+00],
       [1.60000000e+00, 6.00998853e-01, 3.99001147e-01, 0.00000000e+00],
       [1.70000000e+00, 2.92117947e-01, 7.07882053e-01, 1.00000000e+00],
       [1.80000000e+00, 1.01572935e-01, 8.98427065e-01, 1.00000000e+00],
       [1.90000000e+00, 3.00431473e-02, 9.69956853e-01, 1.00000000e+00],
       [2.00000000e+00, 8.41435978e-03, 9.91585640e-01, 1.00000000e+00],
       [2.10000000e+00, 2.31942430e-03, 9.97680576e-01, 1.00000000e+00],
       [2.20000000e+00, 6.36516988e-04, 9.99363483e-01, 1.00000000e+00],
       [2.30000000e+00, 1.74465108e-04, 9.99825535e-01, 1.00000000e+00],
       [2.40000000e+00, 4.78036916e-05, 9.99952196e-01, 1.00000000e+00],
       [2.50000000e+00, 1.30970760e-05, 9.99986903e-01, 1.00000000e+00]])


In [9365]: p1gX[p1gX[:,0]<p1gX[:,1]]
Out[9365]: 
array([[2.92117947e-01, 7.07882053e-01],
       [1.01572935e-01, 8.98427065e-01],
       [3.00431473e-02, 9.69956853e-01],
       [8.41435978e-03, 9.91585640e-01],
...

/ HIER HIER HIER
















/ 13	. 

X=iris['data'][:,[2,3]]
lg=LogisticRegression(multi_class="multinomial",solver="lbfgs",C=10**10)
y=iris['target']
lg.fit(X,y)

In [8996]: lg.intercept_
Out[8996]: array([ 41.08973527,   2.09196084, -43.18169611]) / klasse 0,1,2

In [8997]: lg.coef_
Out[8997]: 
array([[ -8.08440785, -18.01963296],	 / klasse 0
       [  1.16488946,   3.78620864],	 / klasse 1
       [  6.91951839,  14.23342432]])	 / klasse 2


/ 13	. 

X=iris['data'][:,[2,3]]
lg=LogisticRegression(C=10**10)
y=iris['target']
lg.fit(X,y)

In [9001]: lg.intercept_
Out[9001]: array([ 24.05565958,  -2.85895422, -45.26062435]) / klasse 0,1,2

In [9002]: lg.coef_
Out[9002]: 
array([[-6.67454643, -9.82307197],		/ klasse 0
       [ 1.54644801, -3.10696268],		/ klasse 1
       [ 5.7528683 , 10.44455633]])		/ klasse 2

/ 13	. 

X=iris['data'][:,[2,3]]
lg= LogisticRegression(C=10**10)
y=(iris['target']==2).astype(np.int)
lg.fit(X,y)

In [9006]: lg.intercept_
Out[9006]: array([-45.26062435])
In [9007]: lg.coef_
Out[9007]: array([[ 5.7528683 , 10.44455633]])

/ 13	. 

X=iris['data'][:,[2,3]]
lg= LogisticRegression(C=10**10)
T=np.c_[y==2,y!=2].astype(int)
lg.fit(X,T)

In [9006]: lg.intercept_
Out[9006]: array([-45.26062435])
In [9007]: lg.coef_
Out[9007]: array([[ 5.7528683 , 10.44455633]])


/ 13	. 

/ LogisticRegression verwacht altijd een 1-dim array	, dus ovr T=[(1,0),(0,1),...] kan NIET	,

/ we hebben 2 classes	, 1 en 0	,

X=iris['data'][:,[2,3]]
y=iris['target']
T=np.c_[y==2].astype(int).ravel()
lg=LogisticRegression(multi_class="multinomial",solver="lbfgs",C=10**10)
lg.fit(X,T)

In [9066]: lg.intercept_
Out[9066]: array([-22.63617953])
In [9067]: lg.coef_
Out[9067]: array([[2.87726566, 5.22335671]])

/ 13	. 

/ als je 2 classes opgeeft,	

/ 1313	. 

/ klasse 2	,

X=iris['data'][:,[2,3]]
y=iris['target']
T=np.c_[y==2].astype(int).ravel()
lg=LogisticRegression(C=10**10)
lg.fit(X,T)

In [9074]: lg.coef_
Out[9074]: array([[ 5.7528683 , 10.44455633]])
In [9075]: lg.intercept_
Out[9075]: array([-45.26062435])

/ 1313	. 

/ klasse 1	,

X=iris['data'][:,[2,3]]
y=iris['target']
T=np.c_[y==1].astype(int).ravel()
lg=LogisticRegression(C=10**10)
lg.fit(X,T)

In [9079]: lg.coef_
Out[9079]: array([[ 1.54644801, -3.10696268]])
In [9080]: lg.intercept_
Out[9080]: array([-2.85895422])

/ 1313. 

/ klasse 0	,

X=iris['data'][:,[2,3]]
y=iris['target']
T=np.c_[y==0].astype(int).ravel()
lg=LogisticRegression(C=10**10)
lg.fit(X,T)

In [9084]: lg.coef_
Out[9084]: array([[-6.67454643, -9.82307197]])
In [9085]: lg.intercept_
Out[9085]: array([24.05565958])


//////////////////
/ 3 CLASSES	, SCIKIT LEARN NEWTON

/ 1313	. 

/ klasse 0,1,2 tegelijk	,

X=iris['data'][:,[2,3]]
lg=LogisticRegression(C=10**10)
y=iris['target']
lg.fit(X,y)

In [9001]: lg.intercept_
Out[9001]: array([ 24.05565958,  -2.85895422, -45.26062435]) / klasse 0,1,2

In [9002]: lg.coef_
Out[9002]: 
array([[-6.67454643, -9.82307197],		/ klasse 0
       [ 1.54644801, -3.10696268],		/ klasse 1
       [ 5.7528683 , 10.44455633]])		/ klasse 2

/ 13	. 

/ bij 2 classes gaat het anders als bij 3 of meer classes	,

X=iris['data'][:,[2,3]]
T=np.copy(iris['target'])
T[T==1]=0
lg=LogisticRegression(C=10**10)
lg.fit(X,T)

X=iris['data'][:,[2,3]]
T=np.copy(iris['target'])
T[T==0]=1
lg=LogisticRegression(C=10**10)
lg.fit(X,T)

In [9110]: lg.coef_
Out[9110]: array([[ 5.7528683 , 10.44455633]])
In [9111]: lg.intercept_
Out[9111]: array([-45.26062435])

/ 13	. 

/ bij 4 classes	,


X=iris['data'][:,[2,3]]
T=np.copy(iris['target'])
In [9115]: T[149]=4
lg=LogisticRegression(C=10**10)
lg.fit(X,T)

In [9119]: lg.coef_
Out[9119]: 
array([[-6.67454643, -9.82307197],
       [ 1.54644801, -3.10696268],
       [ 4.83150998,  8.55309696],
       [ 0.29721707,  0.7417456 ]])
In [9120]: lg.intercept_
Out[9120]: array([ 24.05565958,  -2.85895422, -37.93261325,  -7.48765147])


/ 13	. 

/ newton	,

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((len(X),1)),X]
theta=np.array([0,0,0]).reshape(-1,1)
y=(iris['target']==2).reshape(-1,1).astype(int)
for i in range(0,10):
  z=X.dot(theta)
  a=expit(z)
  grad=X.T.dot(a-y)
  D=np.diag((a*(1-a)).ravel())
  H=X.T.dot(D).dot(X)
  theta=theta-np.linalg.inv(H).dot(grad)

In [9123]: theta
Out[9123]: 
array([[-45.27234329],
       [  5.75453225],
       [ 10.44669981]])


/ 1313	. 

/ newton	,

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((len(X),1)),X]
theta=np.array([0,0,0]).reshape(-1,1)
y=(iris['target']==1).reshape(-1,1).astype(int)
for i in range(0,10):
  z=X.dot(theta)
  a=expit(z)
  grad=X.T.dot(a-y)
  D=np.diag((a*(1-a)).ravel())
  H=X.T.dot(D).dot(X)
  theta=theta-np.linalg.inv(H).dot(grad)

In [9125]: theta
Out[9125]: 
array([[-2.85880689],
       [ 1.54646368],
       [-3.10710628]])

/ 1313	 

X=iris['data'][:,[2,3]]
X=np.c_[np.ones((len(X),1)),X]
theta=np.array([0,0,0]).reshape(-1,1)
y=(iris['target']==0).reshape(-1,1).astype(int)
for i in range(0,10):
  z=X.dot(theta)
  a=expit(z)
  grad=X.T.dot(a-y)
  D=np.diag((a*(1-a)).ravel())
  H=X.T.dot(D).dot(X)
  theta=theta-np.linalg.inv(H).dot(grad)

In [9127]: theta
Out[9127]: 
array([[21.81940571],
       [-5.73678936],
       [-9.66992458]])

/ deze wijkt iets af van toen we de klasses 0,1,2 tegelijk deden	,

/ Einde 3 CLASSES	, SCIKIT LEARN NEWTON



/ Einde BISHOP (209) 

